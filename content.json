{"meta":{"title":"QSX1C","subtitle":"  ","description":"","author":"QSX1C","url":"https://topone233.github.io","root":"/"},"pages":[{"title":"categories","date":"2020-07-12T08:02:56.000Z","updated":"2020-07-12T08:04:10.280Z","comments":true,"path":"categories/index.html","permalink":"https://topone233.github.io/categories/","excerpt":"","text":""},{"title":"about","date":"2020-07-12T08:05:04.000Z","updated":"2020-07-12T08:05:18.584Z","comments":true,"path":"about/index.html","permalink":"https://topone233.github.io/about/","excerpt":"","text":""},{"title":"contact","date":"2020-07-12T08:05:31.000Z","updated":"2020-07-12T08:05:48.424Z","comments":true,"path":"contact/index.html","permalink":"https://topone233.github.io/contact/","excerpt":"","text":""},{"title":"friends","date":"2020-09-21T13:29:00.000Z","updated":"2020-09-21T13:29:54.435Z","comments":true,"path":"friends/index.html","permalink":"https://topone233.github.io/friends/","excerpt":"","text":""},{"title":"tags","date":"2020-07-12T08:04:26.000Z","updated":"2020-07-12T08:04:53.027Z","comments":true,"path":"tags/index.html","permalink":"https://topone233.github.io/tags/","excerpt":"","text":""}],"posts":[{"title":"Windows登陆前自检是否有NVIDIA显卡","slug":"Windows登陆前自检是否有NVIDIA显卡","date":"2023-02-05T16:00:00.000Z","updated":"2023-05-09T07:46:26.684Z","comments":true,"path":"2023/02/06/Windows登陆前自检是否有NVIDIA显卡/","link":"","permalink":"https://topone233.github.io/2023/02/06/Windows%E7%99%BB%E9%99%86%E5%89%8D%E8%87%AA%E6%A3%80%E6%98%AF%E5%90%A6%E6%9C%89NVIDIA%E6%98%BE%E5%8D%A1/","excerpt":"","text":"Windows登陆前自检是否有NVIDIA显卡我的电脑开机时识别不到独立显卡，必须要重启才能使用独立显卡。更新了驱动也解决不了。 解决思路：写一个windows任务，在开机时，用户登录前进行检测，如果没有NVIDIA显卡则立即重新启动。 1. Bat 检测是否有NVIDIA显卡，如果没有则立即重启电脑@echo off for /f \"skip=1 tokens=1\" %%a in ('wmic path Win32_VideoController get AdapterCompatibility') do ( if /i \"%%a\" == \"NVIDIA\" ( exit /b ) ) shutdown -r -t 0 2. 设置自动执行任务，调用脚本按win键，搜素任务计划程序， 窗口右侧操作 -&gt;创建基本任务，触发器选择计算机启动时。 设置好后到任务计划程序库中找到刚才创建的任务。 更改用户或组 -&gt; 高级 -&gt; 立即查找，找到你的用户名，确定。 设置不管用户是否登录都需要运行。","categories":[{"name":"Windows","slug":"windows","permalink":"https://topone233.github.io/categories/windows/"}],"tags":[{"name":"Windows 10","slug":"windows-10","permalink":"https://topone233.github.io/tags/windows-10/"},{"name":"bat","slug":"bat","permalink":"https://topone233.github.io/tags/bat/"}]},{"title":"GitHub Action自动获取力扣每日一题","slug":"GitHub Action自动获取力扣每日一题","date":"2021-01-25T13:25:04.466Z","updated":"2023-05-09T07:47:42.286Z","comments":true,"path":"2021/01/25/GitHub Action自动获取力扣每日一题/","link":"","permalink":"https://topone233.github.io/2021/01/25/GitHub%20Action%E8%87%AA%E5%8A%A8%E8%8E%B7%E5%8F%96%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/","excerpt":"","text":"项目地址：https://github.com/topone233/LeetCode 起因没别的，天太冷了，早上赖床不想起，手一滑打开了力扣APP，本想着先看下今天的每日一题（简单一小时、中等看一天、难题一秒钟），手机端不给看每日一题？？？总不能为了看眼题目就让我起床吧？ 嘿嘿，写个定时器，每天早上自动发送每日一题到邮箱。 躺着把每日一题看喽！ 具体实现代码实现部分直接看这里： https://blog.csdn.net/malloc_can/article/details/113004579 讲的很详细，不再赘述了。 唯一需要修改的一点： python 脚本中获取环境变量 import os SCKEY = os.environ[\"SCKEY\"] GitHub Action GitHub Actions 是 GitHub 的持续集成服务，于2018年10月推出。 持续集成由很多操作组成，比如抓取代码、运行测试、登录远程服务器，发布到第三方服务等等。GitHub 把这些操作就称为 actions。 很多操作在不同项目里面是类似的，完全可以共享。GitHub 注意到了这一点，想出了一个很妙的点子，允许开发者把每个操作写成独立的脚本文件，存放到代码仓库，使得其他开发者可以引用。 如果你需要某个 action，不必自己写复杂的脚本，直接引用他人写好的 action 即可，整个持续集成过程，就变成了一个 actions 的组合。这就是 GitHub Actions 最特别的地方. 上面的代码已经很简洁了，不过问题在于。。。我没有服务器啊 嘿嘿，白嫖一波GitHub Action的服务器资源。 Action的详细使用参考（三者结合着看）： ​ 基于GITHUB ACTION的定时任务，真香！ ​ GitHub Actions 教程：定时发送天气邮件 ​ 官方说明文档 Action的关键在于 .github/workflows 里的yaml文件（完整代码在开头给出的仓库中）。 # Controls when the action will run. 执行的触发条件 on: # Triggers the workflow on push or pull request events but only for the main branch push时触发 #push: # branches: [ main ] #pull_request: # branches: [ main ] # 定时任务，每天在国际标准时间23点（北京时间早上7点）运行 schedule: - cron: '0 23 * * *' # A workflow run is made up of one or more jobs that can run sequentially or in parallel jobs: # This workflow contains a single job called \"build\" build: # The type of runner that the job will run on 运行环境 runs-on: ubuntu-latest # Steps represent a sequence of tasks that will be executed as part of the job steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - uses: actions/checkout@v2 # 安装python3.7 - name: Set up Python 3.7 uses: actions/setup-python@v2 with: python-version: '3.7' - name: Install dependencies run: | python -m pip install --upgrade pip pip install requests - name: 'Send Email' # 设置两个环境变量，给python脚本使用 env: # 从仓库的setting中设置两个secret，加密邮箱用户名和密码 username: ${{ secrets.MAILUSERNAME }} pwd: ${{ secrets.MAILPASSWORD }} run: python request.py","categories":[{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/categories/%E8%B5%84%E6%BA%90/"}],"tags":[{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/tags/%E8%B5%84%E6%BA%90/"},{"name":"python","slug":"python","permalink":"https://topone233.github.io/tags/python/"},{"name":"GitHub","slug":"github","permalink":"https://topone233.github.io/tags/github/"}]},{"title":"线程安全(二)-锁升级","slug":"线程安全(二)-锁升级","date":"2020-11-05T15:01:53.448Z","updated":"2023-02-06T11:46:03.883Z","comments":true,"path":"2020/11/05/线程安全(二)-锁升级/","link":"","permalink":"https://topone233.github.io/2020/11/05/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8(%E4%BA%8C)-%E9%94%81%E5%8D%87%E7%BA%A7/","excerpt":"","text":"基础知识超线程一个ALU + 两组（Registers + PC） ALU：算术逻辑单元（Arithmetic and Logic Unit）核心。 PC：程序计数器（Program Counter）保存下一条即将执行指令的内存地址，每解释执行完一条指令，pc的值就会自动被更新为下一条指令的地址。 Register：寄存器 ContextSwitch：线程上下文切换 内核态与用户态内核态：与内核交互的核心操作，必须通过内核，如：网卡、显卡操作、拿锁（锁在内核是mutec）。 用户态：大多数应用程序在用户态。轻量级锁就在用户态。 线程阻塞的代价java的线程是映射到操作系统原生线程之上的，如果要阻塞或唤醒一个线程就需要操作系统介入，需要在用户态与核心态之间切换，这种切换会消耗大量的系统资源，因为用户态与内核态都有各自专用的内存空间，专用的寄存器等，用户态切换至内核态需要传递给许多变量、参数给内核，内核也需要保护好用户态在切换时的一些寄存器值、变量等，以便内核态调用结束后切换回用户态继续工作。 如果线程状态切换是一个高频操作时，这将会消耗很多CPU处理时间； 如果对于那些需要同步的简单的代码块，获取锁挂起操作消耗的时间比用户代码执行的时间还要长，这种同步策略显然非常糟糕的。 阻塞再唤醒会导致线程发生两次上下文切换！ synchronized会导致争用不到锁的线程进入阻塞状态，所以说它是java语言中一个重量级的同步操纵，被称为重量级锁，为了缓解上述性能问题，JVM从1.5开始，引入了轻量锁与偏向锁，默认启用了自旋锁，他们都属于乐观锁。 明确java线程切换的代价，是理解java中各种锁的优缺点的基础之一。 markword 在介绍java锁之前，先说下什么是markword，markword是java对象数据结构中的一部分，要详细了解java对象的结构可以点击这里,这里只做markword的详细介绍，因为对象的markword和java各种类型的锁密切相关； markword数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，它的最后2bit是锁状态标志位，用来标记当前对象的状态，对象的所处的状态，决定了markword存储的内容，如下表所示: 状态 标志位 存储内容 未锁定 01 对象哈希码、对象分代年龄 轻量级锁定 00 指向锁记录的指针 膨胀(重量级锁定) 10 执行重量级锁定的指针 GC标记 11 空(不需要记录信息) 可偏向 01 偏向线程ID、偏向时间戳、对象分代年龄 锁升级无锁态当我们new一个对象的时候，就是无锁态，也就是房间门敞开的，谁都可以进进出出。 偏向锁锁这种东西我们是需要去商家买锁。同样的，锁这种重量级资源也是稀缺的，数量有限，我们需要向操作系统申请。这种重量级的方案显然不应该上来就直接使用。 在竞争不是很激烈的情况下，一般线程使用过后就释放了，也就没必要申请重量级锁，而且一般再次使用的还会是它。 那么简单的方法是：偏向锁。偏向于第一个访问的线程。进来的线程在房间门上贴个标签，写上自己的名字。比如说 ：某栋大楼上挂着腾讯的牌子，别人一看就知道，“哦 这是腾讯的大楼，我要去的是隔壁的阿里”。 所谓的“贴个标签”指的是：在 markword 中用54位记录一个指向当前线程的指针，也就是标识是属于此线程的。贴的标签，标识自己的线程 id。 它通过消除资源无竞争情况下的同步原语，进一步提高了程序的运行性能。 获取偏向锁过程 1）访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01，确认为可偏向状态。 2）如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤5，否则进入步骤3。 3）如果线程ID并未指向当前线程，则通过CAS尝试修改为自己的线程ID。如果修改成功，则将Mark Word中线程ID设置为当前线程ID，然后执行5；如果竞争失败，执行4。 4）如果CAS获取偏向锁失败，则表示有竞争。当到达全局安全点（safepoint）时获得偏向锁的线程被挂起， 如果持有偏向锁线程不活跃，恢复到无锁态 如果线程依然活跃，马上执行线程的操作栈，检查对偏向锁对象的使用情况 如果仍然需要持有偏向锁，则表示有竞争，升级为轻量级锁 如果不存在使用偏向锁，恢复到无锁态 偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码。（撤销偏向锁的时候会导致stop the word） 5）执行同步代码 偏向锁特点偏向锁不可重偏向 批量偏向 批量撤销。 偏向锁由于有锁撤销的过程revoke，会消耗系统资源。所以，在锁争用特别激烈的时候，用偏向锁未必效率高。还不如直接使用轻量级锁。 偏向锁延时默认情况 偏向锁有个时延，默认是4秒。why? 因为JVM虚拟机自己有一些默认启动的线程，里面有好多sync代码，这些sync代码启动时就知道肯定会有竞争，如果使用偏向锁，就会造成偏向锁不断的进行锁撤销和锁升级的操作，效率较低。 轻量级锁轻量级锁也叫无锁、自旋锁。 在偏向锁状态下，当有另外一个或多个线程竞争（只要发生任意一个竞争）就自动升级为轻量级锁。 1）首先，撤销偏向锁状态，把门上的标签撕下来。 2）然后，每个线程的线程栈中，都生成一个 Lock Record（锁记录，放的是对Mark Word的复制 Displaced Mark Word）。 3）再然后多个线程通过CAS抢占，看哪个线程可以先把LR的指针写到轻量级锁中，轻量级锁中记录的是：指向拥有锁的，线程栈中的Lock Record 的指针。 4）如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态 5）抢占失败的线程，自旋，等待机会。 抢占使用的是自旋的方式（CAS） 线程不断的获取轻量级锁中的记录，修改为指向自己的。 然后尝试将修改后的结果写回去， 如果原来的值还是当初读到的旧值，没有被别人改变，那就把自己修改后的写进去，抢占成功。 否则就自旋，重复。 但是不能就这样一直自旋下去，因为自旋实在是太消耗CPU资源了。所以需要下一步的锁升级。 自适应自旋在JDK 1.6 之后由JVM自行控制自旋锁升级的时机。也就是自适应自旋 （Adaptive Self Spinning）。 自适应自旋锁意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。 重量级锁升级重量级锁：-&gt; 向操作系统申请资源，linux mutex , CPU从3级-0级系统调用，线程挂起，进入等待队列，等待操作系统的调度，然后再映射回用户空间 synchronizedsynchronization，其使用监视器 (monitor) 来实现。java中的每个对象都关联了一个监视器，线程可以对其进行加锁和解锁操作。在同一时间，只有一个线程可以拿到对象上的监视器锁。如果其他线程在锁被占用期间试图去获取锁，那么将会被阻塞直到成功获取到锁。同时，监视器锁可以重入，也就是说如果线程 t 拿到了锁，那么线程 t 可以在解锁之前重复获取锁；每次解锁操作会反转一次加锁产生的效果。 public class T { static volatile int i = 0; public static void n() { i++; } public static synchronized void m() {} public static void main(String[] args) { for(int j=0; j&lt;1000_000; j++) { m(); n(); } } }java -XX:+UnlockDiagonositicVMOptions -XX:+PrintAssembly T C1 Compile Level 1 (一级优化) C2 Compile Level 2 (二级优化) 找到m() n()方法的汇编码，会看到 lock comxchg …..指令 synchronized vs Lock (CAS) 在高争用 高耗时的环境下synchronized效率更高 在低争用 低耗时的环境下CAS效率更高 synchronized到重量级之后是等待队列（不消耗CPU） CAS（等待期间消耗CPU） 一切以实测为准锁优化以上介绍的锁不是我们代码中能够控制的，但是借鉴上面的思想，我们可以优化我们自己线程的加锁操作； 减少锁的时间不需要同步执行的代码，能不放在同步块里面执行就不要放在同步块内，可以让锁尽快释放； 使用读写锁ReentrantReadWriteLock 是一个读写锁，读操作加读锁，可以并发读，写操作使用写锁，只能单线程写； 消除缓存行的伪共享除了我们在代码中使用的同步锁和jvm自己内置的同步锁外，还有一种隐藏的锁就是缓存行，它也被称为性能杀手。在多核cup的处理器中，每个cup都有自己独占的一级缓存、二级缓存，甚至还有一个共享的三级缓存，为了提高性能，cpu读写数据每次都会读取数据所在的数据块，是以缓存行为最小单元读写的；32位的cpu缓存行为32字节，64位cup的缓存行为64字节，这就导致了一些问题。例如，多个不需要同步的变量因为存储在连续的32字节或64字节里面，当需要其中的一个变量时，就将它们作为一个缓存行一起加载到某个cup-1私有的缓存中（虽然只需要一个变量，但是cpu读取会以缓存行为最小单位，将其相邻的变量一起读入），被读入cpu缓存的变量相当于是对主内存变量的一个拷贝，也相当于变相的将在同一个缓存行中的几个变量加了一把锁，这个缓存行中任何一个变量发生了变化，当cup-2需要读取这个缓存行时，就需要先将cup-1中被改变了的整个缓存行更新回主存（即使其它变量没有更改），然后cup-2才能够读取，而cup-2可能需要更改这个缓存行的变量与cpu-1已经更改的缓存行中的变量是不一样的，所以这相当于给几个毫不相关的变量加了一把同步锁；为了防止伪共享，不同jdk版本实现方式是不一样的： 在jdk1.7之前会 将需要独占缓存行的变量前后添加一组long类型的变量，依靠这些无意义的数组的填充做到一个变量自己独占一个缓存行； 在jdk1.7因为jvm会将这些没有用到的变量优化掉，采用继承一个声明了好多long变量的类的方式来实现； 在jdk1.8中通过添加sun.misc.Contended注解来解决这个问题，若要使该注解有效必须在jvm中添加以下参数：-XX:-RestrictContended sun.misc.Contended注解会在变量前面添加128字节的padding将当前变量与其他变量进行隔离； 减少锁的粒度它的思想是将物理上的一个锁，拆成逻辑上的多个锁，增加并行度，从而降低锁竞争。它的思想也是用空间来换时间； java中很多数据结构都是采用这种方法提高并发操作的效率： ConcurrentHashMapjava中的ConcurrentHashMap在jdk1.8之前的版本，使用一个Segment 数组 Segment&lt; K,V >[] segments Segment继承自ReenTrantLock，所以每个Segment就是个可重入锁，每个Segment 有一个HashEntry&lt; K,V &gt;数组用来存放数据，put操作时，先确定往哪个Segment放数据，只需要锁定这个Segment，执行put，其它的Segment不会被锁定；所以数组中有多少个Segment就允许同一时刻多少个线程存放数据，这样增加了并发能力。 LongAdder（推荐）LongAdder 实现思路也类似ConcurrentHashMap，LongAdder有一个根据当前并发状况动态改变的Cell数组，Cell对象里面有一个long类型的value用来存储值;开始没有并发争用的时候或者是cells数组正在初始化的时候，会使用cas来将值累加到成员变量的base上，在并发争用的情况下，LongAdder会初始化cells数组，在Cell数组中选定一个Cell加锁，数组有多少个cell，就允许同时有多少线程进行修改，最后将数组中每个Cell中的value相加，在加上base的值，就是最终的值；cell数组还能根据当前线程争用情况进行扩容，初始长度为2，每次扩容会增长一倍，直到扩容到大于等于cpu数量就不再扩容，这也就是为什么LongAdder比cas和AtomicInteger效率要高的原因，后面两者都是volatile+cas实现的，他们的竞争维度是1，LongAdder的竞争维度为“Cell个数+1”为什么要+1？因为它还有一个base，如果竞争不到锁还会尝试将数值加到base上； LinkedBlockingQueueLinkedBlockingQueue也体现了这样的思想，在队列头入队，在队列尾出队，入队和出队使用不同的锁，相对于LinkedBlockingArray只有一个锁效率要高； 拆锁的粒度不能无限拆，最多可以将一个锁拆为当前cup数量个锁即可； 锁消除 lock eliminatepublic void add(String str1,String str2){ StringBuffer sb = new StringBuffer(); sb.append(str1).append(str2); }我们都知道 StringBuffer 是线程安全的，因为它的关键方法都是被 synchronized 修饰过的，但我们看上面这段代码，我们会发现，sb 这个引用只会在 add 方法中使用，不可能被其它线程引用（因为是局部变量，栈私有），因此 sb 是不可能共享的资源，JVM 会自动消除 StringBuffer 对象内部的锁。 锁粗化 lock coarseningpublic String test(String str){ int i = 0; StringBuffer sb = new StringBuffer(): while(i &lt; 100){ sb.append(str); i++; } return sb.toString(): }JVM 会检测到这样一连串的操作都对同一个对象加锁（while 循环内 100 次执行 append，没有锁粗化的就要进行 100 次加锁/解锁），此时 JVM 就会将加锁的范围粗化到这一连串的操作的外部（比如 while 虚幻体外），使得这一连串操作只需要加一次锁即可。 锁降级（不重要）https://www.zhihu.com/question/63859501 其实，只被VMThread访问，降级也就没啥意义了。所以可以简单认为锁降级不存在！ 参考： https://www.cnblogs.com/linghu-java/p/8944784.html http://mashibing.com 《码出高效》","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"}]},{"title":"线程安全(一)-简单使用锁","slug":"线程安全(一)-简单使用锁","date":"2020-11-01T16:00:00.000Z","updated":"2023-02-06T11:46:13.543Z","comments":true,"path":"2020/11/02/线程安全(一)-简单使用锁/","link":"","permalink":"https://topone233.github.io/2020/11/02/%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8(%E4%B8%80)-%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E9%94%81/","excerpt":"","text":"什么是线程安全 想象一下这样一个场景：你去酒店开了一间房，可是房间没有上锁，任何从你房间路过的人都可以推门而入（如果他们想的话）。在这样的房间里睡觉 安全吗？显然是不安全的。路不拾遗、夜不闭户这种理想状态只会出现在理想中。 同样的，在多线程的情况下，也存在线程安全的问题。我们先理解下下面的代码: private static int count = 0; public static void main(String[] args) { for (int i = 0; i &lt; 2; i++) { new Thread(new Runnable() { @Override public void run() { try { Thread.sleep(10); } catch (Exception e) { e.printStackTrace(); } //每个线程让count自增100次 for (int i = 0; i &lt; 100; i++) { count++; } } }).start(); } try{ Thread.sleep(2000); }catch (Exception e){ e.printStackTrace(); } System.out.println(count); } counut 的输出值是否为 200？答案是否定的。因为 count++ 这个操作不是原子性的。 先要从主存获取count值到，然后+1，最后再把结果写到主存。 因为不是原子性的，就有可能被会其他线程打断，结果不确定性，这个程序就是线程不安全的。 如何保证线程安全那么如何保证线程安全？首先说下众所周知的方法：悲观锁、乐观锁。 什么是悲观锁、乐观锁？在 java 语言里，总有一些名词看语义跟本不明白是啥玩意儿，也就总有部分面试官拿着这样的词来忽悠面试者，以此来找优越感，其实理解清楚了，这些词也就唬不住人了。 乐观锁乐观锁是一种乐观思想，即认为读多写少，遇到并发写的可能性低，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，采取在写时先读出当前版本号，然后加锁操作（比较跟上一次的版本号，如果一样则更新），如果失败则要重复读-比较-写的操作。 java中的乐观锁基本都是通过CAS操作实现的，CAS是一种更新的原子操作，比较当前值跟传入值是否一样，一样则更新，否则失败。 悲观锁 悲观锁是就是悲观思想，即认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人会修改，所以每次在读写数据的时候都会上锁，这样别人想读写这个数据就会block直到拿到锁。java中的悲观锁就是Synchronized,AQS框架下的锁则是先尝试cas乐观锁去获取锁，获取不到，才会转换为悲观锁，如RetreenLock。 CAS 机制CAS 是英文单词 Compare And Swap (Compare And Exchange) 的缩写，翻译过来就是比较并替换。 因为经常配合循环操作，直到完成为止，所以泛指一类操作 cas(v, a, b) ，变量v，期待值a, 修改值b 更新一个变量的时候，只有当变量的预期值（计算结果值V） 和主内存当中的实际值（新值 N）相同时，才会将内存地址对应的值 N修改为 V。 ABA简单说： 值A 变成 B 再变成 A。最终你只会看到A还是A，但是中间变成 B的过程你并不知道，这可能会产生不确定的影响。 解决：版本号（ AtomicStampedReference），基础类型简单值不需要版本号每次被修改都改变版本号，每次读都顺带读出版本号。 原子操作类轻量级锁，所谓原子操作类也称为无锁或者自旋锁，指的是 java.util.concurrent.atomic 包下，一系列以 Atomic 开头的包装类。例如AtomicBoolean，AtomicInteger，AtomicLong。它们分别用于Boolean，Integer，Long类型的原子性操作。 private static AtomicInteger count = new AtomicInteger(0); ... for (int i = 0; i &lt; 100; i++) { count.incrementAndGet(); }使用 AtomicInteger 之后，最终的输出结果同样可以保证是 200。并且在某些情况下，代码的性能会比 Synchronized 更好。 而 Atomic 操作的底层实现正是利用的 CAS 机制。 unsafe 底层实现AtomicInteger: public final int incrementAndGet() { for (;;) { int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; } } public final boolean compareAndSet(int expect, int update) { return unsafe.compareAndSwapInt(this, valueOffset, expect, update); }Unsafe： public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);CAS底层的的实现方法是c++写的，底层实现： inline jint Atomic::cmpxchg (jint exchange_value, volatile jint* dest, jint compare_value) { int mp = os::is_MP(); __asm__ volatile (LOCK_IF_MP(%4) &quot;cmpxchgl %1,(%3)&quot; : &quot;=a&quot; (exchange_value) : &quot;r&quot; (exchange_value), &quot;a&quot; (compare_value), &quot;r&quot; (dest), &quot;r&quot; (mp) : &quot;cc&quot;, &quot;memory&quot;); return exchange_value; } asm 表示这是汇编指令 LOCK_IF_MP：Multi Processors，如果是多个CPU，则前面加LOCK。CAS的关键就是这个LOCK。锁定一个北桥信号。 cmpxchg ：compare and exchang。（非原子性指令） 但是这种自旋（一直霸占着CPU却不做事）是很耗 cpu 资源的，所以一般情况下，都会有线程 sleep。 CAS 的缺点1.CPU 开销较大在并发量比较高的情况下，如果许多线程反复尝试更新某一个变量，却又一直更新不成功，循环往复，会给 CPU 带来很大的压力。 2. 不能保证代码块的原子性CAS 机制所保证的只是一个变量的原子性操作，而不能保证整个代码块的原子性。比如需要保证 3 个变量共同进行原子性的更新，就不得不使用 Synchronized 了。 Synchronized 同步方法：作用于方法时，锁是当前调用该方法的对象，也就是this指向的对象。 同步方法是被共享的，那么该方法的对象相当于所有的线程来说就是唯一的，保证了锁的唯一性。 作用于静态方法时，锁是该方法所在类的class实例对象，该对象可以通过“类名.class”获取。 因为Class的相关数据存储在永久代PermGen（jdk1.8则是metaspace），永久代是全局共享的，因此静态方法锁相当于类的一个全局锁。 同步代码块：作用于一个共享实例对象时，锁住的是所有以该对象为锁的代码块。 （这里需要注意：锁住的是该共享实例对象，而不是下面的代码块！锁的是房间门，而不是床！） 一开始的案例我们用Synchronized同步锁, 只需要在 count++ 的位置添加同步锁，让这个自加的过程捆绑具有原子性，其他线程不能插手这个过程。代码如下: for (int i = 0; i &lt; 100; i++) { synchronized (ThreadCas.class){ count++; }Synchronized的实现 它有多个队列，当多个线程一起访问某个对象监视器的时候，对象监视器会将这些线程存储在不同的容器中。 Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：Contention List中那些有资格成为候选资源的线程被移动到Entry List中； Wait Set：哪些调用wait方法被阻塞的线程被放置在这里； OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被称为OnDeck； Owner：当前已经获取到所资源的线程被称为Owner； !Owner：当前释放锁的线程。 JVM每次从队列的尾部取出一个数据用于锁竞争候选者（OnDeck），但是并发情况下，ContentionList会被大量的并发线程进行CAS访问，为了降低对尾部元素的竞争，JVM会将一部分线程移动到EntryList中作为候选竞争线程。Owner线程会在unlock时，将ContentionList中的部分线程迁移到EntryList中，并指定EntryList中的某个线程为OnDeck线程（一般是最先进去的那个线程）。Owner线程并不直接把锁传递给OnDeck线程，而是把锁竞争的权利交给OnDeck，OnDeck需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在JVM中，也把这种选择行为称之为“竞争切换”。 OnDeck线程获取到锁资源后会变为Owner线程，而没有得到锁资源的仍然停留在EntryList中。如果Owner线程被wait方法阻塞，则转移到WaitSet队列中，直到某个时刻通过notify或者notifyAll唤醒，会重新进去EntryList中。 处于ContentionList、EntryList、WaitSet中的线程都处于阻塞状态，该阻塞是由操作系统来完成的（Linux内核下采用pthread_mutex_lock内核函数实现的）。 Synchronized是非公平锁。 Synchronized在线程进入ContentionList时，等待的线程会先尝试自旋获取锁，如果获取不到就进入ContentionList，这明显对于已经进入队列的线程是不公平的，还有一个不公平的事情就是自旋获取锁的线程还可能直接抢占OnDeck线程的锁资源。 缺点Synchronized虽然确保了线程的安全，但是在性能上却不是最优的，Synchronized关键字会让没有得到锁资源的线程进入BLOCKED状态（阻塞或者轮询），而后在争夺到锁资源后恢复为RUNNABLE状态，这个过程中涉及到操作系统用户模式和内核模式的转换，代价比较高。 尽管 Java1.6 为Synchronized做了优化，增加了从偏向锁到轻量级锁再到重量级锁的过度，但是在最终转变为重量级锁之后，性能仍然较低。 锁释放的情况 当线程执行完了代码，释放锁 或者执行发生异常，JVM会让线程自动释放锁。 如果获取到锁的线程在等待IO或者其他原因（比如sleep）被阻塞了，但是又没有释放锁，其他线程只能等着，，，因此需要一种机制可以不让等待的线程无限期的等待。（Lock出现的原因） Locksynchronized 无法中断一个正在等候获得锁的线程，也无法通过轮询得到锁，如果不想等下去，也就没法得到锁。JDK5 增加了Lock锁。更加灵活。 Synchronized 与 Lock 的区别 实现层面不一样。synchronized是Java关键字，JVM层面实现加锁和释放锁； lock是一个接口，在代码层面实现加锁和释放锁。 是否自动释放锁。synchronized在代码执行完或出现异常时自动释放锁； lock不会自动释放锁，需要在finally代码块显式的释放锁 lock.unlock(); 是否一直等待。synchronized会导致拿不到锁的线程一直等待； lock可以设置尝试获取锁或者获取锁失败一定时间超时。lock.tryLock() 获取锁成功是否可知。synchronized无法得知； lock可以通过tryLock 返回布尔值得知。 功能复杂性。synchronized 可重入、不可中断、非公平； lock：可重入、可判断、可公平或者不公平、细分读写锁提高效率。 参考： https://www.jianshu.com/p/ae25eb3cfb5d http://mashibing.com","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"}]},{"title":"Java面试题目","slug":"Java面试题目","date":"2020-10-25T16:00:00.000Z","updated":"2023-02-06T11:47:58.497Z","comments":true,"path":"2020/10/26/Java面试题目/","link":"","permalink":"https://topone233.github.io/2020/10/26/Java%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE/","excerpt":"","text":"Java中的内存泄漏是怎么回事？请举例。 ​ 存在一些不会被GC回收，然而还占用内存的对象 equals和==的区别是什么？简单的说：equals比较值， ==比较的是地址。 equals是Object类的方法，其实内部还是调用的 “==”，使用的时候需要重写。 下述代码在多线程环境中是否存在问题？如何修改？volatile关键字是什么意思？如果删除对该段代码有何影响？ class Counter { private volatile int count = 0; public int getNext() { return ++count; } } 如果是方法里定义的，一定是线程安全的，因为每个方法栈是线程私有的。 如果是类的静态成员变量，i++则不是线程安全的，因为i++会被编译成几句字节码语句执行，而每个线程都有自己的工作内存，每个线程需要对共享变量操作的时候必须先把共享变量从主内存load到自己的工作内存，等完成对共享变量的操作时再保存到主内存。如果一个线程运算完成后还没写入到主内存，此时这个共享变量的值被另一个线程从主内存读取到了，这个时候读取的数据就是脏数据了，它会覆盖其他线程计算的值。可以通过synchronize块来提供同步。 修改方法：将count变量写在getNext()方法内部。或者通过synchronize块来提供同步。 每次修改volatile变量都会同步到主存中。volatile保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。 BlockingQueue相比普通的Queue最大的区别是什么？阻塞队列（BlockingQueue）是一个继承自 Queue的接口，被广泛使用在“生产者-消费者”问题中，其原因是 BlockingQueue 提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。 Thread.sleep() 可能抛出的 InterruptedException 代表什么？如何处理？Thread.sleep()是Thread类的一个静态方法，使当前线程休眠，进入阻塞状态（暂停执行），如果线程在睡眠状态被中断，将会抛出IterruptedException中断异常。 捕获它并在RuntimeException中将其返回，以避免为每个方法声明它。 catch (InterruptedException e) { throw new RuntimeException(&quot;Unexpected interrupt&quot;, exc); } 为什么要重写clone方法和equals方法？ 实例变量与静态变量的区别？ 静态变量用static关键字修饰，属于整个类的，也称类变量。实例变量是属于对象的。 静态变量在内存中永远只有一份，而实例变量每个对象都会有一份。 静态变量存在方法区，实例变量存在堆内存。 静态变量在第一次使用类时分配空间，实例变量创建对象时才分配空间。 静态变量还可以通过类名.变量名访问。Student.schoolName=”小学”; String s=”Hello”; s=s+”world!”;执行后，原始String内容是否改变？没有，String是被final修饰的不可变类。这段代码中，s指向一个String对象，内容是“Hello”，然后进行“+”操作，此时s指向另一个String对象，内容为“Hello World!”，原来的对象并不会被改变，只是s这个引用变量不再指向它了。所有的字符串都会存在字符串常量缓冲区中，唯一且不可改变。 顺带一提：StringBuffer、StringBuilder是可变字符串，StringBuffer线程安全，性能略低。 用最有效率的方法算出2乘以8等于多少使用位运算来实现效率最高，2乘以8相当于二进制位左移三位。所以实现方式为2&lt;&lt;3 因为将一个数左移n位，就相当于乘以了2的n次方，那么，一个数乘以8只要将其左移3位即可，而位运算cpu直接支持的，效率最高，所以，2乘以8等於几的最效率的方法是2 &lt;&lt; 3。 Checked异常和Runtime异常的区别 Runtime异常（运行时异常）：包括RuntimeaException及其所有子类。不要求程序必须对它们作出处理，比如InputMismatchException、ArithmeticException、NullPointerException等。即使没有使用try-catch或throws进行处理，仍旧可以进行编译和运行。如果运行时发生异常，会输出异常的堆栈信息并中止程序执行。 Checked异常（非运行时异常）：除了运行时异常外的其他异常类都是Checked异常。程序必须捕获或者声明抛出这种异常，否则出现编译错误，无法通过编译。处理方式包括两种：通过try-catch捕获异常，通过throws声明抛出异常从而交给上一级调用方法处理。 有一个字符串List，如下代码所示。续写代码，将words中的元素再按照字符拆分，合并成一个数组，然后字符去重，最终得到[“h”, “e”, “l”, “o”, “w”, “r”, “d”] List&lt;String&gt; words = new ArrayList&lt;&gt;(); words.add(&quot;hello&quot;); words.add(&quot;world&quot;); 如何在Controller的方法中获取http报文中的数值？ POST /example?param2=value HTTP/1.1 ... { &quot;param2&quot;: &quot;value&quot; }@PostMapping(“/example”) Public String getPost(HttpServletRequest req) { String p2 = req.getParameter(“param2”); } 使用注解（如@Component）声明Bean，如何指定Bean加载顺序？ 构造方法依赖 最简单也是最常见的，但是需要注意循环依赖的问题。 创建两个 Bean，要求 CDemo2 在 CDemo1 之前被初始化： @Component publicclass CDemo1 { private String name = \"cdemo 1\"; public CDemo1(CDemo2 cDemo2) { System.out.println(name); } } @Component publicclass CDemo2 { private String name = \"cdemo 1\"; public CDemo2() { System.out.println(name); } } 输出： cdemo 2 cdemo 1 虽然这种方式比较直观简单，但是有几个限制 需要有注入关系，如 CDemo2 通过构造方法注入到 CDemo1 中，如果需要指定两个没有注入关系的 bean 之间优先级，则不太合适（比如我希望某个 bean 在所有其他的 Bean 初始化之前执行） 循环依赖问题，如果上面的 CDemo2 的构造方法有一个 CDemo1 参数，那么循环依赖产生，应用无法启动 在构造方法中，不应有复杂耗时的逻辑，会拖慢应用的启动时间 @DependsOn(“bean_a”) 声明当前bean依赖于另一个bean，被依赖的就被被容器确保在当前bean之前被实例化 用法： 直接或者间接标注在带有@Component注解的类上面。 直接或者间接标注在带有@Bean 注解的方法上面。 有一点需要特别注意：它能控制 bean 的实例化顺序，但是 bean 的初始化操作（如构造 bean 实例之后，调用@PostConstruct注解的初始化方法）顺序则不能保证。 多个同类型的Bean，使用注解注入时如何指定？ @Qualifier(“bean_name”) 指定具体bean名称。 @Primary 标注当前为首要选择 XML映射中，SQL语句的两种参数注入方法：#和$的区别？ #{} 占位符。变量替换后自动加上单引号。能防止sql注入。无默认值 ${} 拼接符。变量替换后不会加上单引号。不能防止sql注入。默认值value XML映射中，如何使用LIKE进行模糊查询？ XML映射中，如何批量插入传入的集合（Collection）？ XML映射中，如何在插入的同时获取MySQL自增字段（AUTO_INCREMENT）的生成值？ &lt;insert id=\"insert\" parameterType=\"a\" userGeneratedKeys=\"true\" keyProperty=\"id\"> 如何使key对应的值过5秒后失效？ Expire key 5 scan和keys命令的区别？ Scan 对keys命令进行分解，减少对其他命令执行的阻塞。 Keys 一次性返回所有符合条件的key。 如何开启和执行事务？ Multi 开启事务 Exec触发事务 使用管道(Pipeline)有什么好处？ 提高客户端与服务端之间的多命令的执行效率","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"}]},{"title":"SpringBoot + WebSocket","slug":"SpringBoot + WebSocket","date":"2020-10-20T10:17:50.761Z","updated":"2023-05-09T07:50:31.277Z","comments":true,"path":"2020/10/20/SpringBoot + WebSocket/","link":"","permalink":"https://topone233.github.io/2020/10/20/SpringBoot%20+%20WebSocket/","excerpt":"","text":"在HTTP协议中，所有的请求都是由客户端发起、服务端响应，服务端无法向客户端推送消息，但是在一些需要即时通信的应用中（比如本次的web聊天室），不可避免的需要服务端向客户端推送消息。 传统的解决方案： 轮询：客户端固定的时间间隔下不停地向服务端发送请求，查看服务端是否有最新的数据。如果没有，返回一个空的JSON或者XML文档。客户端每次都要新建HTTP请求，极大的浪费服务端资源。 长轮询：服务端不会每次都立即响应客户端请求，只有在有最新数据时才会立即响应客户端请求。 弊端：TCP和HTTP都有连接超时，所谓的长轮询不能一直持续，需要定期的连接和关闭再连接。 Applet和Flash：已成为历史… WebSocket 协议 WebSocket是一种在单个TCP连接上进行全双工通信的协议，已被W3C定为标准。WebSocket 允许服务端主动向客户端推送数据，双方只需要完成一次握手就可以直接创建持久性的连接，并进行双向数据传输。 一个WebSocket请求首先使用非正常的HTTP请求以特定的模式访问一个URL（两种模式：ws或wss，对应HTTP和HTTPS）。 请求头中有一个Connection：Upgrade字段，表示客户端想要对协议进行升级。还有一个Upgrade：websocket字段，表示客户端想要将请求协议升级为WebSocket协议。 这两个字段共同告诉服务器要将连接升级为WebSocket，如果服务端同意，那么握手完成之后，文本消息或其他二进制消息就可以同时在两个方向上进行发送。此时客户端与服务端关系是对等的。 特点： 有状态，使用时需要先创建连接。 与HTTP使用相同的端口 80（ws）或者443（wss），基本不会被防火墙阻止 使用HTTP协议进行握手，自然而然的集成到浏览器和HTTP服务器，不需要额外的成本 心跳信息（ping、pong）将被反复发送，以保持WebSocket连接一直处于活跃状态 当信息启动或者到达时，服务端和客户端都可以知道 关闭时将发送一个特殊的关闭消息 支持跨域，可以避免Ajax的限制 HTTP规范要求浏览器将并发连接数限制为每个主机名两个连接，WebSocket握手完成后不存在此限制 支持扩展，用户可以扩展协议，实现部分自定义的子协议 更好的二进制支持、更好的压缩效果 应用场景： 在线股票网站 即时聊天 多人在线游戏 应用集群通信 … SpringBoot + WebSocket用SpringBoot + WebSocket实现一个简单的Web群聊。 去GitHub查看 1.创建项目、添加依赖&lt;dependency> &lt;groupId>org.springframework.boot&lt;/groupId> &lt;artifactId>spring-boot-starter-websocket&lt;/artifactId> &lt;/dependency> &lt;!-- 使用jar包的形式对所需要的前端库统一管理 --> &lt;dependency> &lt;groupId>org.webjars&lt;/groupId> &lt;artifactId>webjars-locator-core&lt;/artifactId> &lt;/dependency> &lt;dependency> &lt;groupId>org.webjars&lt;/groupId> &lt;artifactId>sockjs-client&lt;/artifactId> &lt;version>1.1.2&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.webjars&lt;/groupId> &lt;artifactId>stomp-websocket&lt;/artifactId> &lt;version>2.3.3&lt;/version> &lt;/dependency> &lt;dependency> &lt;groupId>org.webjars&lt;/groupId> &lt;artifactId>jquery&lt;/artifactId> &lt;version>3.3.1&lt;/version> &lt;/dependency> 2.WebSocketConfig STOMP是一个简单的可互操作的协议，通常被用于通过中间服务器在客户端之间进行异步消息传递。 Spring框架提供了基于WebSocket的STOMP支持。 /* * WebSocket配置类 * spring提供了基于websocket的stomp支持 * stomp是一个简单的可互操作的协议，通常被用于通过中间服务器在客户端之间进行异步消息传递。 */ @Configuration // 此注解用于开启WebSocket消息代理 @EnableWebSocketMessageBroker public class WebSocketConfig implements WebSocketMessageBrokerConfigurer{ @Override public void configureMessageBroker(MessageBrokerRegistry config) { // 设置消息代理的前缀。即如果消息的前缀是“/topic”就会将消息转发给消息代理（broker） // 再由消息代理将消息广播给当前连接的客户端。 config.enableSimpleBroker(\"/topic\"); // 配置一个或多个前缀，通过前缀过滤出需要被注解方法处理的消息。其他的将被直接交给broker处理。 config.setApplicationDestinationPrefixes(\"/app\"); } @Override public void registerStompEndpoints(StompEndpointRegistry registry) { // 定义一个前缀为“/chat”的endPoint，并开启sockjs支持。 // sockjs可以解决浏览器对websocket的兼容性问题，客户端将通过这里配置的URL来建立websocket连接 registry.addEndpoint(\"/chat\").withSockJS(); } } 3.Controller定义一个Controller来实现对消息的处理 @Controller public class GreetingController { // 注解的方法将用来接收“/app/hello”路径发送来的消息，处理后将消息转发到@SendTo定义的路径上。 @MessageMapping(\"/hello\") // 将处理过的消息交给消息代理broker，再由broker进行广播 @SendTo(\"/topic/greetings\") public Message greeting(Message message) throws Exception { return message; } } 4.聊天页面在 resources/static下创建 chat.html页面作为聊天页面 &lt;!DOCTYPE html> &lt;html lang=\"en\"> &lt;head> &lt;meta charset=\"UTF-8\"> &lt;title>群聊&lt;/title> &lt;script src=\"/webjars/jquery/jquery.min.js\">&lt;/script> &lt;script src=\"/webjars/sockjs-client/sockjs.min.js\">&lt;/script> &lt;script src=\"/webjars/stomp-websocket/stomp.min.js\">&lt;/script> &lt;script src=\"/app.js\">&lt;/script> &lt;/head> &lt;body> &lt;div> &lt;label for=\"name\">请输入用户名&lt;/label> &lt;input type=\"text\" id=\"name\" placeholder=\"用户名\"> &lt;/div> &lt;div> &lt;button id=\"connect\" type=\"button\">连接&lt;/button> &lt;button id=\"disconnect\" type=\"button\" disabled=\"disabled\">断开连接&lt;/button> &lt;/div> &lt;div id=\"chat\" style=\"display:none;\"> &lt;div> &lt;label for=\"name\">请输入聊天内容&lt;/label> &lt;input type=\"text\" id=\"content\" placeholder=\"聊天内容\"> &lt;/div> &lt;button id=\"send\" type=\"button\">发送&lt;/button> &lt;div id=\"greetings\"> &lt;div id=\"conversation\" style=\"display: none\">群聊进行中...&lt;/div> &lt;/div> &lt;/div> &lt;/body> &lt;/html> 自定义的 app.js var stompClient = null; function setConnected(connected) { $(\"#connect\").prop(\"disabled\", connected); $(\"#disconnect\").prop(\"disabled\", !connected); if (connected) { $(\"#conversation\").show(); $(\"#chat\").show(); } else { $(\"#conversation\").hide(); $(\"#chat\").hide(); } $(\"#greetings\").html(\"\"); } // 建立一个websocket连接，用户必须先输入用户名才能建立 function connect() { if (!$(\"#name\").val()) { return; } // 首先使用sockJS建立连接，然后创建一个STOMP实例发起连接请求 // 在连接成功的回调方法中，首先调用setConnected(true)方法进行页面的设置 var socket = new SockJS('/chat'); stompClient = Stomp.over(socket); stompClient.connect({}, function (frame) { setConnected(true); // 然后调用STOMP中的subscribe方法订阅服务端发送回来的消息 // 并将服务端发送来的消息展示出来（使用showGreeting方法） stompClient.subscribe('/topic/greetings', function (greeting) { showGreeting(JSON.parse(greeting.body)); }); }); } function disconnect() { if (stompClient != null) { // 调用STOMP中的disconnect方法断开一个websocket连接 stompClient.disconnect(); } setConnected(false); } function sendName() { stompClient.send(\"/app/hello\", {}, JSON.stringify({'name': $(\"#name\").val(), 'content':$(\"#content\").val()})); } function showGreeting(message) { $(\"#greetings\").append(\"&lt;div>\" +message.name+ \":\" + message.content + \"&lt;/div>\"); } $(function () { $(\"#connect\" ).click(function() { connect(); }); $(\"#disconnect\" ).click(function() { disconnect(); }); $(\"#send\" ).click(function() { sendName(); }); }); 5.运行用两个浏览器或者多用户，运行结果如下： 输入自定义的用户名，点击连接，进入聊天室。 实现两个或两个以上用户进行聊天，聊天内容房间内所有人都可以看到，实现一个简单的Web群聊。","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"SpringBoot","slug":"springboot","permalink":"https://topone233.github.io/tags/springboot/"},{"name":"WebSocket","slug":"websocket","permalink":"https://topone233.github.io/tags/websocket/"}]},{"title":"MySql 基础语句","slug":"MySql 基础语句","date":"2020-10-14T16:00:00.000Z","updated":"2023-02-06T11:48:24.679Z","comments":true,"path":"2020/10/15/MySql 基础语句/","link":"","permalink":"https://topone233.github.io/2020/10/15/MySql%20%E5%9F%BA%E7%A1%80%E8%AF%AD%E5%8F%A5/","excerpt":"","text":"1.开启关闭net start mysql net stop mysql mysql -u root -p // 准备退出mysql时一定要exit。 exit2.数据定义语言 DDLDDL（data definition language）用于定义、撤销、修改关系模式，如表、视图、索引。 // 查看当前登录用户（；是表示sql语句结束，不能少） select user()； // 数据库操作 drop database db_test; // 删除数据库db_test create database if not exists db_test; // 创建一个名为db_test的数据库 show databases; // 查看数据库列表 use db_test; // 选择db_test做为当前数据库，后续操作默认都在此数据库 mysql -uroot -proot db_test // 直接在进入mysql时指定数据库 // 数据表操作 // 创建数据表tb_a create table if not exists tb_a( id int(11) primary key auto_increment, name varchar(255), foreign key (name) references tb_b, // 外键name引用的是tb_b表的主键name on delete cascade // 表示当主表中删除了某一个主键时，基表中引用此主键的行也随之被删除 on delete restrict // 凡是被基表所引用的主键，不得被删除，默认 )engine=InnoDB auto_increment=1 charset=utf8; // 查看数据库 show create table tb_a; // 查看刚刚创建数据表的语句 show tables; //查看存在的表 desc tb_a; //查看数据表结构 select * from tb_a;alter// alter 修改表结构 // 表字段修改 alter table tb_a rename tb_newname; // 重命名tb_a alter table tb_a modify id int(12); // 修改id字段数据类型 alter table tb_a change id newID int(11); // 重命名id字段为newID，并修改数据类型。（需要旧的列名称） alter table tb_a add sex enum(&#39;男&#39;, &#39;女&#39;) after name; // 在name后增加一个字段 alter table tb_a drop sex; //删除字段sex // 约束修改 // 系统会自动在主键上建立索引，当插入元组时，会进行唯一性检查，大量插入时，会影响系统效率 // 暂时撤销主键，完成插入后再补充定义主键，可以提高效率。 alter table tb_a add primary key(name); // 定义外键后，也需要引用完整性检查，会影响系统性能，必要时可暂时撤销 alter tabel tb_a drop primary key;索引// 索引 create [unique] [cluster] index_a on tb_a(id desc, newID asc); // []为可选内容 // unique表示此索引的每一个索引值只对应唯一的数据记录 // cluster表示要建立的索引是簇集索引 // asc表示升序，desc表示降序，默认是升序。 // 索引只能删除，不能修改 drop index index_a;单引号和反引号 反引号是为了区分MySQL的保留字与普通字符而引入的符号。 引号一般用在字段的值，如果字段值是字符或字符串，则要加引号. 删除表内数据 delete from tb_a 【可选 where id=1】; delete删除表内数据，表结构不变，删除操作作为事务记录在日志中保存以便进行进行回滚操作。数据空间不释放，因为需回滚。对 table 和 view 都能操作。速度慢。 optimize table tb_a; 释放tb_a的空间 truncate table tb_a; 一次性地从表中删除所有的数据，释放存储表数据所用的数据页来删除数据，并不把单独的删除操作记录记入日志保存(只在事务日志中记录 页的释放)，因此也不能回滚，不能恢复数据，在删除的过程中不会激活与表有关的删除触发器，占用资源更加少，速度更快。数据空间会释放，这个表和索引所占用的空间会恢复到初始大小。只能操作没有关联视图的 table。不能用于参与了索引视图的表。 drop table tb_a; 删除的是整个表，包括表的结构，数据，定义。永久抹去，空间释放。对 table 和 view 都能操作。 3.查询语言 QLselect [all | distinct] &lt;目标列表达式&gt;[别名] [, &lt;目标列表达式&gt;[别名] ]... from &lt;表名或视图名&gt;[, &lt;表名或视图名&gt;]... [ where &lt;条件表达式&gt;] [ group by &lt;属性列1&gt; [having &lt;条件表达式&gt; ] ] [ order by &lt;属性列2&gt; [asc | desc] ]; select * from student; select id, name from student; select distinct name from student; //消除重复 select id, name from student where sex=&#39;女&#39;; all 表示查询结果不消除重复元组。 distinct表示消除重复元组。[]表示可选项 group by 将结果根据属性列1的值进行分组 having是分组必须满足的符加条件。必须跟在group by后面 order by 将结果根据属性列2的值进行排序，asc升序（默认），desc降序。（必须在group by的后面） where常用的查询条件 查询条件 谓词 比较 =，&lt;, &gt;, &lt;=, &gt;=, &lt;&gt;, !=, !&gt;, !&lt; 确定范围 between…and… , not between… and… 确定集合 in , not in 字符匹配 like, not like 空值 is null , is not null 多重条件 and , or // 1.查询课程号为001的课程的学生学号、姓名 select cource.id, name from student, cource where student.id = cource.id // 由于id在两个表都有出现，因此引用id时需要注明基表名 and cid = &#39;001&#39;;谓词 in// 2.查询同时选修001和002两门课程的学生学号、姓名（and） select id, name from student where id in (select x.id from cource as x, cource as y // 为了区别两个课程，引入别名x、y加以限定 where x.id = y.id and x.cid = &#39;001&#39; and y.cid = &#39;002&#39;); // 3.查询缺考的学生学号、课程号（is null） select id, cid, grade from cource where grade is null; // 空值是状态不是值，不能写成grade=null // 4.查询只有一个人选修的课程号（not in） select cid from cource sc // cource的别名sc，区别不同层次上对同一个表的查询 where cid not in (select cid from cource where id &lt;&gt; sc.id); // 聚合函数方式 select cid from cource group by cid having count(*) = 1;谓词 exists带有exists的子查询不返回任何数据，只返回布尔值 true或者false。 // 嵌套查询逐次求解、层次分明，而且执行效率也比连接查询效率高 // 5.查询没有选修001课程的学生姓名 select name from student where not exists (select * from cource where id=student.id and cid=&#39;001&#39;);谓词 like 模糊查询like 进行全部或部分字符串的匹配。%匹配0或多个字符； // 6.查询课程名为计算机开头的课程 select * from course where cname like &#39;计算机%&#39;;between… and… 范围查询// 7.查询不是1990年到2020年出生的学生姓名 select name from student where year(bdate) not between 1990 and 2020;聚集函数查询聚集函数通常能够减少比较的次数，查询效率较高。 计数 count( [distinct | all ] &lt;属性列&gt;) 统计某属性中值的个数 计算总和 sum( [distinct | all ] &lt;属性列&gt;) 计算某属性值的总和 计算平均值 avg( [distinct | all ] &lt;属性列&gt;) 计算某属性值的平均值 最大值 max( [distinct | all ] &lt;属性列&gt;) 最小值 min( [distinct | all ] &lt;属性列&gt;) // 8.查询选修了课程的学生人数,用num字段显示 select count(distinct id) as num // 一个学生可以选多门课程，但统计时不能重复计算，用distinct去重 from cource; // 9.查询每门课对应的选课人数num，结果集：cid num select cid, count(id) as num from cource group by cid; // 按课程号分组。属性列名必须在select中出现 // 10.查询选修了三门以上课程的学生学号 select id from cource group by id having count(*) &gt;= 3; // having指定选择组的条件，只有满足条件的才会出现在结果 // 11.查询计算机相关所有课程的最高分、最低分、平均分。有缺考课程不予统计。结果按cid升序 select cid, max(grade) as maxgrade, min(grade) as mingrade, avg(grade) as avggrade from cource where cid like &#39;计算机%&#39; group by cid // 按cid分组 having cid not in // 不统计有缺考的课程 (select cid from cource where grade is null) order by cid; // 结果按cid升序集合查询 并、交、 差分别对应：union、 intersect（and）、 except // 12.查询1999年出生的或者选修计算机相关课程的学生学号 (select id from student where yaer(bdate) = 1999) union (select id from cource where cid like &#39;计算机%&#39;);查询分页// 从数据表tb_a 的第4行开始，返回6行数据 select * from tb_a limit 3,6;4.数据操作语言 DMLinsertinsert into tbname [(&lt;&gt;, &lt;&gt;)] values (&lt;&gt;, &lt;&gt;...),(&lt;&gt;, &lt;&gt;...)... ; // 向student表插入一行数据 insert into student values (&#39;001&#39;, &#39;张三&#39;, &#39;男&#39;); 如果没有指定属性列则表示要插入的是一条完整的元组，且与表中定义顺序一致。 如果指定部分属性列，其余属性列取空值。 update执行修改语句时会检查是否会破坏表的完整性规则，如果破坏会给出提示，且修改操作会失败。 update table_name set &lt;属性列名&gt;=&lt;表达式&gt; [, &lt;&gt;=&lt;&gt;] [where &lt;条件&gt;]; // 将cource表所有学生成绩清零，缺考的除外 update cource set grade=0 where grade is not null;delete执行删除语句时会检查是否会破坏表的完整性规则，如果破坏会给出提示，且删除操作会失败。 delete from table_name [where &lt;条件&gt;]; // 删除学号为001的学生 delete from sutdent where id=&#39;001&#39;;","categories":[{"name":"MySql","slug":"mysql","permalink":"https://topone233.github.io/categories/mysql/"}],"tags":[{"name":"MySql","slug":"mysql","permalink":"https://topone233.github.io/tags/mysql/"}]},{"title":"分割等和子集（416）","slug":"分割等和子集（416）","date":"2020-10-10T16:00:00.000Z","updated":"2023-02-06T11:44:27.645Z","comments":true,"path":"2020/10/11/分割等和子集（416）/","link":"","permalink":"https://topone233.github.io/2020/10/11/%E5%88%86%E5%89%B2%E7%AD%89%E5%92%8C%E5%AD%90%E9%9B%86%EF%BC%88416%EF%BC%89/","excerpt":"","text":"2020-10-11 416.分割等和子集 题目 给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 示例： 输入：[1, 5, 11, 5] 输出：true 分析这道题可以换一种表述：给定一个只包含正整数的非空数组 nums[0]，判断是否可以从数组中选出一些数字，使得这些数字的和等于整个数组的元素和的一半。 因此这个问题可以转换成「0−1 背包问题」。这道题与传统的「0−1 背包问题」的区别在于，传统的「0−1 背包问题」要求选取的物品的重量之和不能超过背包的总容量，这道题则要求选取的数字的和恰好等于整个数组的元素和的一半。类似于传统的「0−1 背包问题」，可以使用动态规划求解。 在使用动态规划求解之前，首先需要进行以下判断： 根据数组的长度 n 判断数组是否可以被划分。如果 n&lt;2，则不可能将数组分割成元素和相等的两个子集，因此直接返回 false。 计算数组的元素和 sum 。如果sum 是奇数，则不可能将数组分割成元素和相等的两个子集，因此直接返回 false。 计算数组最大元素 maxNum。如果最大元素比 sum / 2 还大，直接返回false 动态规划求解创建二维数组 dp[i] [j]，表示从数组nums的 0~i 下标范围内选取若干个元素，是否存在一种选取方案使得被选取的元素的和等于 j（子集元素和，也即是背包的容量） 。例如 dp[0] [nums[0]] = true，当i=0，只有nums[0]可以被选取，因此返回true。 此时有两种情况需要考虑： 若不选nums[i]，只从 i 前选取元素来满足背包容量 j，则 dp[i] [j] = dp[i-1] [j]； 若选取nums[i]，此时从i 前选取元素来满足剩下的背包容量 j-nums[i]，则 dp[i] [j] = dp[i-1] [j - nums[i]]； 得出状态转移方程如下：dp[i] [j]=max{ dp[i-1] [j] | dp[i-1] [j-nums[i] ] } 空间优化1.降维其实还可以简化为：dp[j] = dp[j] | dp[j - nums[i]]，因为当前行每次只参考了上一行的值。 只需要 找到元素与背包容量相等（如果原始集合有一个元素等于所有元素和的一半，那么可以直接将此元素与其他元素分成两个子集）； 或者找到元素 x 满足x + nums[i] = j-nums[i] ，比如 1 + 5 = 11 - 5。 target = sum / 2 == 11；true：T；false：略 0 1 2, 3, 4 5 6 7, 8, 9 10 11 nums[0]:1 T T nums[1]:1、5 T T T T nums[2]:1、5、11 T T T T T nums[3]:1、5、11、5 T T T T T T 2.逆序观察上表，如果逆序，从target最大值11开始，一旦 target &lt; nums[i] 马上退出当前循环，可以减少很多重复计算。 代码如下： class Solution { public boolean canPartition(int[] nums) { int len = nums.length; if (len < 2) { return false; } int sum = 0, maxNum = 0; for (int num : nums) { sum += num; maxNum = Math.max(maxNum, num); } if (sum % 2 != 0) { return false; } int target = sum / 2; if (maxNum > target) { return false; } // 动态规划求解阶段 boolean[] dp = new boolean[target + 1]; dp[0] = true; // i最多到背包容量减一，否则就无法分成两个子集了 for (int i = 0; i < len; i++) { int num = nums[i]; // j由大到小递减，一旦小于num，立马跳出循环 for (int j = target; j >= num; --j) { dp[j] |= dp[j - num]; } } return dp[target]; } } 复杂度分析 时间复杂度：O(len × target)，其中 n 是数组的长度，target 是整个数组的元素和的一半。需要计算出所有的状态，每个状态在进行转移时的时间复杂度为 O(1)。 空间复杂度：O(target)，其中 target 是整个数组的元素和的一半。空间复杂度取决于dp 数组，在不进行空间优化的情况下，空间复杂度是 O(len × target)。在进行空间优化的情况下，将二维数组降到一维，空间复杂度可以降到 O(target)。 0-1背包问题背包九讲pdf 已由作者上传至github，完全公开。背包九讲 这里仅对0-1背包问题先做了解，剩下的等看完在发。 题目有N件物品和一个容量为V的背包。第i件物品的费用是c[i]，价值是w[i]。求解将哪些物品装入背包可使价值总和最大。 基本思路这是最基础的背包问题，特点是：每种物品仅有一件，可以选择放或不放。 用子问题定义状态：即 “F[i] [v]”表示前 i 件物品恰放入一个容量为v的背包可以获得的最大价值。则其状态转移方程便是： f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]+w[i]}这个方程非常重要，基本上所有跟背包相关的问题的方程都是由它衍生出来的。所以有必要将它详细解释一下：“将前i件物品放入容量为v的背包中”这个子问题，若只考虑第i件物品的策略（放或不放），那么就可以转化为一个只牵扯前i-1件物品的问题。 如果不放第i件物品：那么问题就转化为“前i-1件物品放入容量为v的背包中”，价值为”F[i-1] [v]”； 如果放第i件物品：那么问题就转化为“前i-1件物品放入剩下的容量为v-c[i]的背包中”，此时能获得的最大价值就是F[i-1] [v-c[i] ]再加上通过放入第i件物品获得的价值w[i]。 优化空间复杂度以上方法的时间和空间复杂度均为O(VN)，其中时间复杂度应该已经不能再优化了，但空间复杂度却可以优化到O。 先考虑上面讲的基本思路如何实现，肯定是有一个主循环i=1..N，每次算出来二维数组f[i] [0..V]的所有值。那么，如果只用一个数组f[0..V]，能不能保证第i次循环结束后f[v]中表示的就是我们定义的状态f[i] [v]呢？f[i] [v]是由f[i-1] [v]和f[i-1] [v-c[i]]两个子问题递推而来，能否保证在推f[i][v]时（也即在第i次主循环中推f[v]时）能够得到f[i-1] [v]和f[i-1] [v-c[i]]的值呢？事实上，这要求在每次主循环中我们以v=V..0的顺序推f[v]，这样才能保证推f[v]时f[v-c[i]]保存的是状态f[i-1] [v-c[i]]的值。伪代码如下： for i=1..N for v=V..0 f[v]=max{f[v],f[v-c[i]]+w[i]};其中的f[v]=max{f[v],f[v-c[i]]}一句恰就相当于我们的转移方程f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]}，因为现在的f[v-c[i]]就相当于原来的f[i-1] [v-c[i]]。如果将v的循环顺序从上面的逆序改成顺序的话，那么则成了f[i][v]由f[i][v-c[i]]推知，与本题意不符，但它却是另一个重要的背包问题最简捷的解决方案，故学习只用一维数组解01背包问题是十分必要的。 事实上，使用一维数组解01背包的程序在后面会被多次用到，所以这里抽象出一个处理一件01背包中的物品过程，以后的代码中直接调用不加说明。 过程ZeroOnePack，表示处理一件01背包中的物品，两个参数cost、weight分别表明这件物品的费用和价值。 procedure ZeroOnePack(cost,weight) for v=V..cost f[v]=max{f[v],f[v-cost]+weight}注意这个过程里的处理与前面给出的伪代码有所不同。前面的示例程序写成v=V..0是为了在程序中体现每个状态都按照方程求解了，避免不必要的思维复杂度。而这里既然已经抽象成看作黑箱的过程了，就可以加入优化。费用为cost的物品不会影响状态f[0..cost-1]，这是显然的。 有了这个过程以后，01背包问题的伪代码就可以这样写： for i=1..N ZeroOnePack(c[i],w[i]);初始化的细节问题我们看到的求最优解的背包问题题目中，事实上有两种不太相同的问法。有的题目要求“恰好装满背包”时的最优解，有的题目则并没有要求必须把背包装满。一种区别这两种问法的实现方法是在初始化的时候有所不同。 如果是第一种问法，要求恰好装满背包，那么在初始化时除了f[0]为0其它f[1..V]均设为-∞，这样就可以保证最终得到的f[N]是一种恰好装满背包的最优解。 如果并没有要求必须把背包装满，而是只希望价格尽量大，初始化时应该将f[0..V]全部设为0。 为什么呢？可以这样理解：初始化的f数组事实上就是在没有任何物品可以放入背包时的合法状态。如果要求背包恰好装满，那么此时只有容量为0的背包可能被价值为0的nothing“恰好装满”，其它容量的背包均没有合法的解，属于未定义的状态，它们的值就都应该是-∞了。如果背包并非必须被装满，那么任何容量的背包都有一个合法解“什么都不装”，这个解的价值为0，所以初始时状态的值也就全部为0了。 这个小技巧完全可以推广到其它类型的背包问题，后面也就不再对进行状态转移之前的初始化进行讲解。 一个常数优化前面的伪代码中有 for v=V..1，可以将这个循环的下限进行改进。 由于只需要最后f[v]的值，倒推前一个物品，其实只要知道f[v-w[n]]即可。以此类推，对以第j个背包，其实只需要知道到f[v-sum{w[j..n]}]即可，即代码中的 for i=1..N for v=V..0可以改成 for i=1..n bound=max{V-sum{w[i..n]},c[i]} for v=V..bound这对于V比较大时是有用的。 小结01背包问题是最基本的背包问题，它包含了背包问题中设计状态、方程的最基本思想，另外，别的类型的背包问题往往也可以转换成01背包问题求解。故一定要仔细体会上面基本思路的得出方法，状态转移方程的意义，以及最后怎样优化的空间复杂度。","categories":[{"name":"算法","slug":"算法","permalink":"https://topone233.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://topone233.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"动态规划","slug":"动态规划","permalink":"https://topone233.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"背包问题","slug":"背包问题","permalink":"https://topone233.github.io/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"}]},{"title":"Redis","slug":"Redis","date":"2020-09-27T16:00:00.000Z","updated":"2023-02-06T11:49:30.984Z","comments":true,"path":"2020/09/28/Redis/","link":"","permalink":"https://topone233.github.io/2020/09/28/Redis/","excerpt":"","text":"基础配置略 数据结构1.String字符串是Redis最简单的数据结构，字符串值内容是二进制安全的，这意味着程序可以把数字、文字、图片、视频等都赋给这个值。 2.List列表是由若干插入顺序排序的字符串元素组成的集合。可以理解为多个字符串组成一个集合对象，并按照链表的插入顺序排序，在读写操作时只能从其两端开始（由链表的寻址方式决定）。有序可重复。 可用于聊天记录、评论等无需调整字符串顺序，而又需要快速响应的场景。 3.Set集合由不重复且无序的字符串元素构成的一个整体。 4.Hash散列表可以存储多个键值对的映射，是无序的数据集合。键的内容必须是唯一的，键内容之间可以采用 “:” 隔离 。例如 Book:name。 键内容必须是字符串型，但值可以是字符串型也可以是数字型。 5.zset有序集合（sorted set）和Hash一样都是由键值对构成数据集合，主要区别是有序集合根据值进行自动排序，而Hash不排序；有序集合可以对值直接进行操作，而Hash通过键查找获取值。键字符串有序且必须唯一，值可以重复。 常用命令略 优点 基于内存，读写速度快 支持数据的持久化 支持master-slave模式的数据备份 丰富的数据类型 原子性操作 丰富的特性 提高管道 Redis引入管道，为了提高客户端与服务端之间的多命令的执行效率 原理Redis数据库从客户端到服务端传输命令，采用 请求-响应 的TCP通信协议。一个命令从客户端发出查询请求，往往采用阻塞方式监听socket接口，直到服务器端返回执行结果信号，一个命令的执行时间周期才结束。这个时间周期叫做往返时间（Round Trip Time，RTT) 如果一条命令的RTT延迟100ms，那么10条连续的命令就会消耗1000ms的时间。于是采用了管道技术：批量发送命令，在服务器端一起执行，最后把结果一次性发送会客户端。可以减少命令的返回次数、减少阻塞时间。 使用建议（1）管道技术会占用服务器端内存资源，一般建议一次管道最大发送命令数量限制在10000条以内 （2)Lua脚本技术无论是内存的消耗，还是速度都比管道技术占优势 分布式集群集群安装略 模拟节点故障略 加减节点略 Lua脚本应用 Lua本身是一种独立的脚本编程语言，优点是能很方便的被嵌入到其他语言中，然后被调用 为什么管道技术提高了Redis多命令的执行效率，那么我们是否可以把客户端应用系统的部分代码直接在服务器端运行，进一步提高代码的执行效率，同时可以实现在服务器端更换代码的目标？ 随着大型电子商务平台对客户操作行为研究的深入，技术人员希望在广告行为分析重定向时，可以快速替换该部分代码，而不需要去应用系统服务器上更新业务系统的执行代码。这样的替换过程，几乎不影响用户的操作使用。（在下面会讲到具体实现） 优点Redis本身没有类似关系数据库的存储过程的功能，但Redis设计者很聪明，直接把世界上最优秀的嵌入式脚本语言Lua内嵌到Redis系统之中，使Redis具备了在数据库服务器端运行逻辑运算代码的功能。有以下优势： 减少网络开销 如刚才所说，将部分特殊代码直接放到服务器端执行，减少交互产生的额外的网络开销问题，大幅度提高应用系统的响应性能。 原子性操作 Lua脚本在服务器端执行时，采用排他性行为，也就是脚本代码执行时，其他命令或脚本无法在同一个服务器端执行（除了极个别命令外）。同时命令要么都被执行，要么都被放弃，具有完整的执行原子性 服务器端快速代码替换 经常需要变化业务规则或算法的代码，可以考虑放到服务器端交给Lua脚本统一执行。因为Lua脚本第一次被执行后，将一直保存在服务器端的脚本缓存中，可以供其他客户端持续调用，效率高、占用内存少。当需要修改Lua脚本代码时，只需要更新内存中的执行脚本内容即可，无需修改业务系统的原始代码，几乎不对客户端产生影响。 使用建议 不要把执行速度慢的代码纳入Lua脚本中，否则将严重影响客户端的使用性能 确保Lua脚本编写正确，尤其是不能出现无限循环这样的低价错误 不能滥用Lua脚本，把最需要、最重要的任务交给它处理。如利用它的原子性特点处理高价值的数据修改一致性 Lua脚本代码条数不应太多，应该是非常简练的、轻量级的 优化读写分离内存配置优化 压缩存储 合理选择存储结构 分片 字符串优化 内存使用管理 数据持久化AOF持久 实时备份 RDB持久（默认） 快照持久化 缓存缓存穿透缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。每次都要去数据库再查询一遍，然后返回空，这样请求就绕过缓存直接查数据库，如果恶意请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。 解决方案 1. 最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力。 2. 也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴。 缓存击穿缓存中的一个 Key(比如一个促销商品)，在某个时间点过期的时候，恰好在这个时间点对这个 Key 有大量的并发请求过来，这些请求发现缓存过期一般都会从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮。 解决方案对缓存查询加锁，如果 KEY 不存在，就加锁，然后查 DB 入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入 DB 查询。 缓存雪崩缓存雪崩我们可以简单的理解为：由于原有缓存失效，新缓存未到期间（大面积缓存击穿）(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库 CPU 和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，会给后端系统带来很大压力。导致系统崩溃。 解决方案大多数系统设计者考虑用加锁（ 最多的解决方案）或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。还有一个简单方案就是将缓存失效时间分散开。 如何避免1：在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个 key 只允许一个线程查询数据和写缓存，其他线程等待。2：做二级缓存，A1 为原始缓存，A2 为拷贝缓存，A1 失效时，可以访问 A2，A1 缓存失效时间设置为短期，A2 设置为长期3：不同的 key，设置不同的过期时间，让缓存失效的时间点尽量均匀。 缓存预热缓存预热这个应该是一个比较常见的概念，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决方案1、直接写个缓存刷新页面，上线时手工操作下；2、数据量不大，可以在项目启动的时候自动进行加载；3、定时刷新缓存； 缓存更新除了缓存服务器自带的缓存失效策略之外（Redis 默认的有 6 中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：（1）定时去清理过期的缓存；（2）当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。两者各有优劣，第一种的缺点是维护大量缓存的 key 是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 缓存降级当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。以参考日志级别设置预案：（1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；（2）警告：有些服务在一段时间内成功率有波动（如在 95~100% 之间），可以自动降级或人工降级，并发送告警；（3）错误：比如可用率低于 90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；（4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。服务降级的目的，是为了防止 Redis 服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis 出现问题，不去数据库查询，而是直接返回默认值给用户。 缓存失效策略redis 采用的是定期删除 + 惰性删除策略。 1、定时删除: 在设置键的过期时间的同时，创建一个定时器 timer). 让定时器在键 的过期时间来临时，立即执行对键的删除操作。 2、惰性删除: 放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是 否过期，如果过期的话，就删除该键; 如果没有过期，就返回该键。 3、定期删除: 每隔一段时间程序就对数据库进行一次检查，删除里面的过期键。至 于要删除多少过期键，以及要检查多少个数据库，则由算法决定。 为什么不用定时删除策略定时删除, 用一个定时器来负责监视 key, 过期则自动删除。虽然内存及时释放，但是十分消耗 CPU 资源。在大并发请求下，CPU 要将时间应用在处理请求，而不是删除 key, 因此没有采用这一策略. 如何工作定期删除，redis 默认每个 100ms 检查，是否有过期的 key, 有过期 key 则删除。需要说明的是，redis 不是每个 100ms 将所有的 key 检查一次，而是随机抽取进行检查 (如果每隔 100ms, 全部 key 进行检查，redis 岂不是卡死)。因此，如果只采用定期删除策略，会导致很多 key 到时间没有删除。于是，惰性删除派上用场。也就是说在你获取某个 key 的时候，redis 会检查一下，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除。 如果定期删除没删除 key。然后你也没即时去请求 key，也就是说惰性删除也没生效。这样，redis 的内存会越来越高。那么就应该采用内存淘汰机制。在 redis.conf 中有一行配置maxmemory-policy volatile-lru 该配置就是配内存淘汰策略的 回收策略（淘汰策略） volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最 少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过 期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意 选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 . no-enviction（驱逐）：禁止驱逐数据注意这里的 6 种机制，volatile 和 allkeys 规定了是对已设置过期时间的数据集淘 汰数据还是从全部数据集淘汰数据，后面的 lru、ttl 以及 random 是三种不同的 淘汰策略，再加上一种 no-enviction 永不回收的策略。 使用策略规则 1、如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率 低，则使用 allkeys-lru 2、如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用 allkeys-random 应用广告访问电子广告特点 要求客户界面响应快速 如果一个带广告界面被浏览时，响应动作不流畅，甚至产生明显的广告图片托屏现象，用户印象一般不会太好 广告投放具有临时性 广告投放一般是短期行为，频繁更换。通过Redis+Java技术，进行相对独立管理比较合理 广告内容都通过代码生成，并缓存到Redis服务端 代码略 商品推荐为了快速推荐商品，需要事先把推荐商品信息通过Redis缓存到内存上。这样的预存动作一般发生在用户浏览某类商品之前的几秒钟。如根据用户的浏览喜好、检索字，生成对应的推荐商品列表。 商品推荐属于临时行为，可以设置n分钟后过期，一般保证被业务系统短时间内调用一两次后将自动删除。淘宝、京东一般也不会将同一件商品推荐多次. 购物车购物车的核心是：商品ID、商品购买数量、销售价格等信息。 行为记录用户从登录开始，就会产生很多相关的操作信息，如客户端访问连接信息：IP地址、端口号、访问时间、经纬度等；又比如：用户在网站上的点击行为记录：访问网页ID号、点击时间、鼠标的坐标点、停留时间等。 替代session session记录了一个终端用户从登录到退出的刚才，临时存储于服务端内存上，但是当同时在线用户过多时，如几十万、数百万，对服务器内存压力将很多。 Redis常驻内存、快速处理、相对低内存占用、对Key对象时限的灵活控制、分布式处理等优点，可以很好的处理session所承担的临时数据的存取任务。 分页缓存当商品数量很大时，必须分页处理，否则，用户是无法忍受一个非常长的、呆板的浏览界面。 为此，需要把商品信息分页显示，然后为其提供下翻到第二个页面的功能，这样的分页浏览功能在传统Web技术下已经实现，只不过响应速度不够快，不能让用户体验到如丝般的顺滑，因此利用Redis的高效缓存处理。 参考自：《NoSQL 数据库入门与时间（基于MongoDB、Redis）》 涉及的源代码部分：https://github.com/lxlxlx1020/nosql_bookdemo","categories":[{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/categories/nosql/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/tags/nosql/"},{"name":"Redis","slug":"redis","permalink":"https://topone233.github.io/tags/redis/"}]},{"title":"Innodb 中的事务隔离级别和锁的关系 - 美团技术团队","slug":"Innodb 中的事务隔离级别和锁的关系 - 美团技术团队","date":"2020-09-20T01:49:35.432Z","updated":"2023-02-06T11:47:26.034Z","comments":true,"path":"2020/09/20/Innodb 中的事务隔离级别和锁的关系 - 美团技术团队/","link":"","permalink":"https://topone233.github.io/2020/09/20/Innodb%20%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8C%E9%94%81%E7%9A%84%E5%85%B3%E7%B3%BB%20-%20%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F/","excerpt":"","text":"原文地址 tech.meituan.com 前言 我们都知道事务的几种性质，数据库为了维护这些性质，尤其是一致性和隔离性，一般使用加锁这种方式。同时数据库又是个高并发的应用，同一时间会有大量的并发访问，如果加锁过度，会极大的降低并发处理能力。所以对于加锁的处理，可以说就是数据库对于事务处理的精髓所在。这里通过分析 MySQL 中 InnoDB 引擎的加锁机制，来抛砖引玉，让读者更好的理解，在事务处理中数据库到底做了什么。 一次封锁 or 两段锁？因为有大量的并发访问，为了预防死锁，一般应用中推荐使用一次封锁法，就是在方法的开始阶段，已经预先知道会用到哪些数据，然后全部锁住，在方法运行之后，再全部解锁。这种方式可以有效的避免循环死锁，但在数据库中却不适用，因为在事务开始阶段，数据库并不知道会用到哪些数据。 数据库遵循的是两段锁协议，将事务分成两个阶段，加锁阶段和解锁阶段（所以叫两段锁） 加锁阶段：在该阶段可以进行加锁操作。在对任何数据进行读操作之前要申请并获得 S 锁（共享锁，其它事务可以继续加共享锁，但不能加排它锁），在进行写操作之前要申请并获得 X 锁（排它锁，其它事务不能再获得任何锁）。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。 解锁阶段：当事务释放了一个封锁以后，事务进入解锁阶段，在该阶段只能进行解锁操作不能再进行加锁操作。 事务加锁 / 解锁处理begin；insert into test …..加 insert 对应的锁update test set…加 update 对应的锁delete from test ….加 delete 对应的锁commit;事务提交时，同时释放 insert、update、delete 对应的锁 这种方式虽然无法避免死锁，但是两段锁协议可以保证事务的并发调度是串行化（串行化很重要，尤其是在数据恢复和备份的时候）的。 事务中的加锁方式事务的四种隔离级别脏读（Dirty Read）：一个事务读取到另一个事务未提交的数据，如果B事务回滚，那么A事务读到的数据根本不是合法的，称为脏读。比如A在B超市买鞋，A汇钱给B，汇钱这个操作还没有提交，A告诉B我打钱了，B查了一下，发现了A的汇的钱。给了A鞋子，A立刻回滚，B发现自己账户上面没有钱。 不可重复读（NonRepeatable Read）：A事务读取了B事务已经提交的更改（或删除）数据，多次读取结果不同— 行级别的问题。A去超市购物，结账时系统读到卡里有100元，而此时A的老婆正在网上转账，把A卡里的100元转到了另一账户，并在A前提交了事务，此时系统检查到A的工资卡里已经没有钱了，A非常纳闷，明明卡里有钱… 幻读（Phantom Read）：一个事务内读取到了别的事务插入的数据，导致前后读取不一致 — 表级别的问题。A平时还挺节俭，A的老婆在银行部门，她经常通过银行系统查看A的消费记录。有一天，她查到A的卡消费是80元，但是A此时正在外面胡吃海喝，消费了1000元，A的老婆打印账单时显示A的消费记录是1080元，老婆很诧异，以为出现了幻觉，幻读就这样产生了。 在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。 隔离级别脏读（Dirty Read）不可重复读（NonRepeatable Read）幻读（Phantom Read）未提交读（Read uncommitted）可能可能可能已提交读（Read committed）不可能可能可能可重复读（Repeatable read）不可能不可能可能可串行化（Serializable ）不可能不可能不可能 未提交读 (Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读 (Read Committed)：只能读取到已经提交的数据。Oracle 等多数数据库默认都是该级别 (不重复读) 可重复读 (Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB 默认级别。在 SQL 标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读 (Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 Read Uncommitted 这种级别，数据库一般都不会用，而且任何操作都不会加锁，这里就不讨论了。 MySQL 中锁的种类MySQL 中锁的种类很多，有常见的表锁和行锁，也有新加入的 Metadata Lock 等等, 表锁是对一整张表加锁，虽然可分为读锁和写锁，但毕竟是锁住整张表，会导致并发能力下降，一般是做 ddl 处理时使用。 行锁则是锁住数据行，这种加锁方法比较复杂，但是由于只锁住有限的数据，对于其它数据不加限制，所以并发能力强，MySQL 一般都是用行锁来处理并发事务。这里主要讨论的也就是行锁。 Read Committed（读取提交内容）在 RC 级别中，数据的读取都是不加锁的，但是数据的写入、修改和删除是需要加锁的。效果如下 MySQL&gt; show create table class\\_teacher \\\\G\\\\ Table: class\\_teacher Create Table: CREATE TABLE \\`class\\_teacher\\` ( \\`id\\` int(11) NOT NULL AUTO\\_INCREMENT, \\`class\\_name\\` varchar(100) COLLATE utf8mb4\\_unicode\\_ci NOT NULL, \\`teacher\\_id\\` int(11) NOT NULL, PRIMARY KEY (\\`id\\`), KEY \\`idx\\_teacher\\_id\\` (\\`teacher\\_id\\`) ) ENGINE=InnoDB AUTO\\_INCREMENT=5 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4\\_unicode\\_ci 1 row in set (0.02 sec) MySQL&gt; select \\* from class\\_teacher; + | id | class\\_name | teacher\\_id | + | 1 | 初三一班 | 1 | | 3 | 初二一班 | 2 | | 4 | 初二二班 | 2 | + 由于 MySQL 的 InnoDB 默认是使用的 RR 级别，所以我们先要将该 session 开启成 RC 级别，并且设置 binlog 的模式 SET session transaction isolation level read committed; SET SESSION binlog\\_format = &#39;ROW&#39;;（或者是MIXED）事务 A事务 Bbegin;begin;update class_teacher set class_name=‘初三二班’ where teacher_id=1;update class_teacher set class_name=‘初三三班’ where teacher_id=1;commit; 为了防止并发过程中的修改冲突，事务 A 中 MySQL 给 teacher_id=1 的数据行加锁，并一直不 commit（释放锁），那么事务 B 也就一直拿不到该行锁，wait 直到超时。 这时我们要注意到，teacher_id 是有索引的，如果是没有索引的 class_name 呢？update class_teacher set teacher_id=3 where class_name = ‘初三一班’; 那么 MySQL 会给整张表的所有数据行的加行锁。这里听起来有点不可思议，但是当 sql 运行的过程中，MySQL 并不知道哪些数据行是 class_name = ‘初三一班’的（没有索引嘛），如果一个条件无法通过索引快速过滤，存储引擎层面就会将所有记录加锁后返回，再由 MySQL Server 层进行过滤。 但在实际使用过程当中，MySQL 做了一些改进，在 MySQL Server 过滤条件，发现不满足后，会调用 unlock_row 方法，把不满足条件的记录释放锁 (违背了二段锁协议的约束)。这样做，保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。可见即使是 MySQL，为了效率也是会违反规范的。（参见《高性能 MySQL》中文第三版 p181） 这种情况同样适用于 MySQL 的默认隔离级别 RR。所以对一个数据量很大的表做批量修改的时候，如果无法使用相应的索引，MySQL Server 过滤数据的的时候特别慢，就会出现虽然没有修改某些行的数据，但是它们还是被锁住了的现象。 Repeatable Read（可重读）这是 MySQL 中 InnoDB 默认的隔离级别。我们姑且分 “读” 和“写”两个模块来讲解。 读读就是可重读，可重读这个概念是一事务的多个实例在并发读取数据时，会看到同样的数据行，有点抽象，我们来看一下效果。 RC（不可重读）模式下的展现 事务 A事务 Bbegin;begin;select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三二班12初三一班1&nbsp;&nbsp;update class_teacher set class_name='初三三班' where id=1;&nbsp;&nbsp;commit;select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三三班12初三一班1读到了事务 B 修改的数据，和第一次查询的结果不一样，是不可重读的。&nbsp;commit;&nbsp; 事务 B 修改 id=1 的数据提交之后，事务 A 同样的查询，后一次和前一次的结果不一样，这就是不可重读（重新读取产生的结果不一样）。这就很可能带来一些问题，那么我们来看看在 RR 级别中 MySQL 的表现： 事务 A事务 B事务 Cbegin;begin;begin;select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三二班12初三一班1&nbsp;update class_teacher set class_name='初三三班' where id=1;commit;&nbsp;&nbsp;&nbsp;insert into class_teacher values (null,'初三三班',1);commit;select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三二班12初三一班1没有读到事务 B 修改的数据，和第一次 sql 读取的一样，是可重复读的。没有读到事务 C 新添加的数据。&nbsp;&nbsp;commit;&nbsp;&nbsp; 我们注意到，当 teacher_id=1 时，事务 A 先做了一次读取，事务 B 中间修改了 id=1 的数据，并 commit 之后，事务 A 第二次读到的数据和第一次完全相同。所以说它是可重读的。那么 MySQL 是怎么做到的呢？这里姑且卖个关子，我们往下看。 不可重复读和幻读的区别很多人容易搞混不可重复读和幻读，确实这两者有些相似。但不可重复读重点在于 update 和 delete，而幻读的重点在于 insert。 如果使用锁机制来实现这两种隔离级别，在可重复读中，该 sql 第一次读取到数据后，就将这些数据加锁，其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住 insert 的数据，所以当事务 A 先前读取了数据，或者修改了全部数据，事务 B 还是可以 insert 数据提交，这时事务 A 就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。需要 Serializable 隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。 所以说不可重复读和幻读最大的区别，就在于如何通过锁机制来解决他们产生的问题。 上文说的，是使用悲观锁机制来处理这两种问题，但是 MySQL、ORACLE、PostgreSQL 等成熟的数据库，出于性能考虑，都是使用了以乐观锁为理论基础的 MVCC（多版本并发控制）来避免这两种问题。 悲观锁和乐观锁 悲观锁 正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。 在悲观锁的情况下，为了保证事务的隔离性，就需要一致性锁定读。读取数据时给加锁，其它事务无法修改这些数据。修改删除数据时也要加锁，其它事务无法读取这些数据。 乐观锁 相对悲观锁而言，乐观锁机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。 而乐观锁机制在一定程度上解决了这个问题。乐观锁，大多是基于数据版本（ Version ）记录机制实现。何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。 要说明的是，MVCC 的实现没有固定的规范，每个数据库都会有不同的实现方式，这里讨论的是 InnoDB 的 MVCC。 MVCC 在 MySQL 的 InnoDB 中的实现在 InnoDB 中，会在每行数据后添加两个额外的隐藏的值来实现 MVCC，这两个值一个记录这行数据何时被创建，另外一个记录这行数据何时过期（或者被删除）。 在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 在可重读 Repeatable reads 事务隔离级别下： SELECT 时，读取创建版本号 &lt;= 当前事务版本号，删除版本号为空或&gt; 当前事务版本号。 INSERT 时，保存当前事务版本号为行的创建版本号 DELETE 时，保存当前事务版本号为行的删除版本号 UPDATE 时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行 通过 MVCC，虽然每行记录都需要额外的存储空间，更多的行检查工作以及一些额外的维护工作，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，也只锁住必要行。 我们不管从数据库方面的教课书中学到，还是从网络上看到，大都是上文中事务的四种隔离级别这一模块列出的意思，RR 级别是可重复读的，但无法解决幻读，而只有在 Serializable 级别才能解决幻读。于是我就加了一个事务 C 来展示效果。在事务 C 中添加了一条 teacher_id=1 的数据 commit，RR 级别中应该会有幻读现象，事务 A 在查询 teacher_id=1 的数据时会读到事务 C 新加的数据。但是测试后发现，在 MySQL 中是不存在这种情况的，在事务 C 提交后，事务 A 还是不会读到这条数据。可见在 MySQL 的 RR 级别中，是解决了幻读的读问题的。参见下图 innodb_lock_1 读问题解决了，根据 MVCC 的定义，并发提交数据时会出现冲突，那么冲突时如何解决呢？我们再来看看 InnoDB 中 RR 级别对于写数据的处理。 “读”与 “读” 的区别可能有读者会疑惑，事务的隔离级别其实都是对于读数据的定义，但到了这里，就被拆成了读和写两个模块来讲解。这主要是因为 MySQL 中的读，和事务隔离级别中的读，是不一样的。 我们且看，在 RR 级别中，通过 MVCC 机制，虽然让数据变得可重复读，但我们读到的数据可能是历史数据，是不及时的数据，不是数据库当前的数据！这在一些对于数据的时效特别敏感的业务中，就很可能出问题。 对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。很显然，在 MVCC 中： 快照读：就是 select select * from table ….; 当前读：特殊的读操作，插入 / 更新 / 删除操作，属于当前读，处理的都是当前的数据，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete; 事务的隔离级别实际上都是定义了当前读的级别，MySQL 为了减少锁处理（包括等待其它锁）的时间，提升并发能力，引入了快照读的概念，使得 select 不用加锁。而 update、insert 这些 “当前读”，就需要另外的模块来解决了。 写（” 当前读”）事务的隔离级别中虽然只定义了读数据的要求，实际上这也可以说是写数据的要求。上文的 “读”，实际是讲的快照读；而这里说的“写” 就是当前读了。 为了解决当前读中的幻读问题，MySQL 事务使用了 Next-Key 锁。 Next-Key 锁Next-Key 锁是行锁和 GAP（间隙锁）的合并，行锁上文已经介绍了，接下来说下 GAP 间隙锁。 行锁可以防止不同事务版本的数据修改提交时造成数据冲突的情况。但如何避免别的事务插入数据就成了问题。我们可以看看 RR 级别和 RC 级别的对比 RC 级别： 事务 A事务 Bbegin;begin;select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三二班30&nbsp;update class_teacher set class_name='初三四班' where teacher_id=30;insert into class_teacher values (null,'初三二班',30);commit;select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三四班3010初三二班30&nbsp; RR 级别： 事务 A事务 Bbegin;begin;select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三二班30&nbsp;update class_teacher set class_name='初三四班' where teacher_id=30;insert into class_teacher values (null,'初三二班',30);waiting....select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三四班30&nbsp;commit;事务 Acommit 后，事务 B 的 insert 执行。 通过对比我们可以发现，在 RC 级别中，事务 A 修改了所有 teacher_id=30 的数据，但是当事务 Binsert 进新数据后，事务 A 发现莫名其妙多了一行 teacher_id=30 的数据，而且没有被之前的 update 语句所修改，这就是 “当前读” 的幻读。 RR 级别中，事务 A 在 update 后加锁，事务 B 无法插入新数据，这样事务 A 在 update 前后读的数据保持一致，避免了幻读。这个锁，就是 Gap 锁。 MySQL 是这么实现的： 在 class_teacher 这张表中，teacher_id 是个索引，那么它就会维护一套 B + 树的数据关系，为了简化，我们用链表结构来表达（实际上是个树形结构，但原理相同） innodb_lock_2 如图所示，InnoDB 使用的是聚集索引，teacher_id 身为二级索引，就要维护一个索引字段和主键 id 的树状结构（这里用链表形式表现），并保持顺序排列。 Innodb 将这段数据分成几个个区间 (negative infinity, 5], (5,30], (30,positive infinity)； update class_teacher set class_name=‘初三四班’ where teacher_id=30; 不仅用行锁，锁住了相应的数据行；同时也在两边的区间，（5,30] 和（30，positive infinity），都加入了 gap 锁。这样事务 B 就无法在这个两个区间 insert 进新数据。 受限于这种实现方式，Innodb 很多时候会锁住不需要锁的区间。如下所示： 事务 A事务 B事务 Cbegin;begin;begin;select id,class_name,teacher_id from class_teacher;idclass_nameteacher_id1初三一班52初三二班30&nbsp;&nbsp;update class_teacher set class_name='初一一班' where teacher_id=20;&nbsp;&nbsp;&nbsp;insert into class_teacher values (null,'初三五班',10);waiting .....insert into class_teacher values (null,'初三五班',40);commit;事务 A commit 之后，这条语句才插入成功commit;&nbsp;commit;&nbsp; update 的 teacher_id=20 是在 (5，30] 区间，即使没有修改任何数据，Innodb 也会在这个区间加 gap 锁，而其它区间不会影响，事务 C 正常插入。 如果使用的是没有索引的字段，比如 update class_teacher set teacher_id=7 where class_name=‘初三八班（即使没有匹配到任何数据）’, 那么会给全表加入 gap 锁。同时，它不能像上文中行锁一样经过 MySQL Server 过滤自动解除不满足条件的锁，因为没有索引，则这些字段也就没有排序，也就没有区间。除非该事务提交，否则其它事务无法插入任何数据。 行锁防止别的事务修改或删除，GAP 锁防止别的事务新增，行锁和 GAP 锁结合形成的的 Next-Key 锁共同解决了 RR 级别在写数据时的幻读问题。 Serializable这个级别很简单，读加共享锁，写加排他锁，读写互斥。使用的悲观锁的理论，实现简单，数据更加安全，但是并发能力非常差。如果你的业务并发的特别少或者没有并发，同时又要求数据及时可靠的话，可以使用这种模式。 这里要吐槽一句，不要看到 select 就说不会加锁了，在 Serializable 这个级别，还是会加锁的！ 参考资料 MySQL 参考手册 《高性能 MySQL》第三版","categories":[{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/categories/sql/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/tags/sql/"}]},{"title":"赛码-Java面试题笔记","slug":"赛码-Java面试题笔记","date":"2020-09-18T14:52:12.219Z","updated":"2023-05-09T08:07:04.067Z","comments":true,"path":"2020/09/18/赛码-Java面试题笔记/","link":"","permalink":"https://topone233.github.io/2020/09/18/%E8%B5%9B%E7%A0%81-Java%E9%9D%A2%E8%AF%95%E9%A2%98%E7%AC%94%E8%AE%B0/","excerpt":"","text":"1.有关接口说法正确的是 A. 接口中的数据成员为final static B. 接口中的数据成员为public abstract C. 接口同样存在构造方法 D. 实现接口的类必须实现该接口的所有抽象方法 AD C 接口是无法被实例化的，没有构造方法 D 如果是抽象类的话，就不需要。 2.下列程序的输出结果是 public class Test { public static void main(String[] args) { String [][]s={{\"helloworld\",\"hello world\"},{\"this is\",\"a java program\"}}; System.out.println((new StringTokenizer(s[1][1])).countTokens()&gt;2); } } A. 3&gt;2 B. 2&gt;2 C. false D. true D 3.使用折半查找算法对含有20个元素的有序表查找的平均查找长度 A. 2.3 B. 4.3 C. 5.1 D. 3 B 折半查找时间复杂度是 log2(n)，所以 log2(20)=4.3 4.对含有31个元素的序列采用直接选择排序算法排序，在最坏情况下需要进行多少次移动才能完成排序 A. 31 B. 30 C. 60 D. 90 D （n-1）次交换，3(n-1)次移动。一次交换，需要三次移动，因为交换需要借助辅助变量。 5.使用二分法在序列1,4,6,7,15,33,39,50,64,78,75,81,89,96中查找元素81时,需要经过（ ）次比较 A. 4 B. 3 C. 2 D. 12 B 编号：0-13，第一次查找编号6的，即39 39 -&gt; 75 -&gt; 89 6.已知存在8阶对称矩阵，采用压缩存储方式按行序为主序存储，每个元素占一个地址空间。若a22为元素的存储地址为1，每个元素占一个地址空间，则a74的地址为 A. 11 B. 23 C. 32 D. 33 B (1,1) (2,1) (2,2) 1 (3,1) (3,2) (3,3) 3(4,1) (4,2) (4,3) (4,4) 4(5,1) (5,2) (5,3) (5,4) (5,5) 5(6,1) (6,2) (6,3) (6,4) (6,5) (6,6) 6(7,1) (7,2) (7,3) (7,4) 4 1+3+4+5+6+4=23 7.已知主串S=“ababcabcacbab”，模式T=“abcac”。利用KMP算法进行匹配时，需要进行几次才可以匹配成功 A. 3 B. 4 C. 5 D. 6 A 关于 KMP算法 参考：http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html 首先计算部分匹配值：abcac中：a的前缀为0，后缀为0，前缀和后缀重合的部分为0ab的前缀为a，后缀为b，前缀和后缀重合的部分为0abc的前缀为[a,ab]，后缀为[bc,c],前缀和后缀重合的部分为0abca的前缀为[a,ab,abc]，后缀为[bca,ca,a],前缀和后缀重合的部分为1abcac的前缀为[a,ab,abc,abca]，后缀为[bcac,cac,ac,c],前缀和后缀重合的部分为0所以部分匹配值也就是 abcac 对应的next数组为：0 0 0 1 0 即：a b c a c 0 0 0 1 0移动位数 = 已匹配的字符数 - 对应的部分匹配值第一次 :a b a b c a b c a c b a ba b c a c匹配个数为2，最后一个匹配的字符为 ‘b’，其next值是0 ， 移动位数=2-0=2第二次： a b a b c a b c a c b a b a b c a c匹配个数为4，最后一个匹配的字符为 ‘a’，其next值是1， 移动位数=4-1=3第三次： a b a b c a b c a c b a b a b c a c匹配成功共需要三次 8.广度优先遍历二叉树的操作可以用哪种数据结构模拟 A. 栈 B. 单链表 C. 队列 D. 数组 C 深度用栈，广度用队列 9.下列关于二叉排序树说法正确的是 A. 二叉排序树的查找性能取决于二叉树的形状 B. 二叉排序树的查找性能取决于序列的大小 C. 二叉排序树复杂度介于 O(log2n) 和 O(n) 之间 D. 对二叉排序树进行层序遍历可得到有序序列 AC 10.单C PU系统中通常采用两级处理器调度，以下相关描述正确的是 A. 作业调度是从慢速存储设备中的后备队列中挑选作业加载到主存中。 B. 作业调度是从慢速存储设备中的就绪队列中挑选作业加载到主存中。 C. 进程调度是从主存中中的后备队列中挑选进程占用处理器运行。 D. 进程调度是从主存中中的就绪队列中挑选进程占用处理器运行。 AD 11.一级封锁协议可以 A. 能够避免不可重复读取问题 B. 能够避免不读“脏”数据 C. 不能避免不可重复读取和不读“脏”数据的问题 D. 可避免更新丢失的问题 CD 12.当需要控制一个类的实例只能有一个，而且客户端只能从一个全局访问点访问它，应该选择何种设计模式: A. 观察者模式 B. 单例模式 C. 迭代器模式 D. 享元模式 B 13.原型模式的本质是 A. 根据状态来分离和选择行为 B. 封装请求 C. 克隆生成对象 D. 触发联动 C 14.TCP协议的拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。常用的方法有 A. 慢启动、窗口滑动 B. 慢开始、拥塞控制 C. 快重传、快恢复 D. 快开始、快恢复 BC 15. import java.util.ArrayList; import java.util.List; public class Main { public static void main(String[] args) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); for(int i=0;i&lt;100;i++){ list.add(&quot;a&quot;); } } }JDK1.8中，执行以上程序后，该list进行了几次扩容？ A. 4 B. 5 C. 6 D. 7 C jdk 1.8，arrayList默认大小为10，每次扩容1.5倍。 int newCapacity = oldCapacity + (oldCapcity &gt;&gt; 1); 16. public class Main { public static void main(String[] args) { System.out.print(fun1()); } public static String fun1() { try { System.out.print(&quot;A&quot;); return fun2(); } finally { System.out.print(&quot;B&quot;); } } public static String fun2() { System.out.print(&quot;C&quot;); return &quot;D&quot;; } }执行以上程序后，输出结果正确的是 A. ABCD B. ACDB C. ACBD D. 不确定 C 如果有finally和return，那就在return前进行finally。 本题代码，先执行fun2()方法，获取返回值后，会存到fun1()的局部变量表中，执行finally语句后，再返回这个值。 17. public class Test { public static void main(String[] args) throws Exception{ ClassLoader classLoader=ClassLoader.getSystemClassLoader(); Class clazz=classLoader.loadClass(&quot;A&quot;); System.out.print(&quot;Test&quot;); clazz.forName(&quot;A&quot;); } } class A{ static { System.out.print(&quot;A&quot;); } } A. TestA B. ATestA C. ATestA D. Test A Class clazz=classLoader.loadClass(“A”); 加载class文件到内存中，并没有对类进行首次主动使用，所以没有初始化。 clazz.forName(“A”); 通过反射获取到A的内存中的数据结构对象，对类进行了首次使用，才会触发初始化。 static 静态代码块只在初始化时候执行一次。 对类的主动使用：创建类实例、访问类或接口的静态变量、静态方法、反射、初始化类的子类、被标明为启动类的类 18.针对类的继承，虚拟机会如何进行父类和子类的初始化加载呢 public class Test { public static void main(String[] args) { System.out.print(B.c); } } class A { public static String c = &quot;C&quot;; static { System.out.print(&quot;A&quot;); } } class B extends A{ static { System.out.print(&quot;B&quot;); } } A. AC B. ABC C. C D. BC A 子类 B引用父类A的静态字段，不会导致子类初始化，只会引发父类初始化。 19. public class Test { public static void main(String[] args) { System.out.print(B.c); } } class A { static { System.out.print(&quot;A&quot;); } } class B extends A{ static { System.out.print(&quot;B&quot;); } public final static String c = &quot;C&quot;; } A. AB B. ABC C. C D. BC C 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用定义常量的类，因此不会触发定义常量的类的初始化。 静态域被访问，而且它不是常量，才触发初始化。 20.JAVA的类加载期负责整个生命周期内的class的初始化和加载工作，就虚拟机的规范来说，以下代码会输出什么结果 public class Test { public static void main(String[] args) { System.out.println(Test2.a); } } class Test2{ public static final String a=&quot;JD&quot;; static { System.out.print(&quot;OK&quot;); } } 输出：JD System.out.println(Test2.a); 用类名直接调用静态属性，没有用对象，就没加载对象中的static方法。 常量在编译阶段会存入调用类的常量池中，本质并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 21. public class Main { private static int x = 10; private static Integer y = 10; public static void updateX(int value) { value = 3 * value; } public static void updateY(Integer value) { value = 3 * value; } public static void main(String[] args) { updateX(x); updateY(y); } }执行以上程序后，x和y的值分别是多少 10，10 Java中只有值传递，没有引用传递 Integer是final定义的。 Integer 是引用类型，但是每次对Integer的赋值操作，都是创建一个新的对象，并且给变量赋上新的地址值。 22. public class Main { public static void main(String[] args) { System.out.println(&quot;A&quot;); new Main(); new Main(); } public Main() { System.out.println(&quot;B&quot;); } { System.out.println(&quot;C&quot;); } static { System.out.println(&quot;D&quot;); } } 输出结果：DACBCB 静态代码块（先父类后子类）-&gt; 非静态代码块 -&gt; 构造函数 23.以下哪种设备工作在数据链路层？ A. 中继器 B. 集线器 C. 交换机 D. 路由器 C 物理层：中继器、集线器 数据链路层：交换机 网络层：路由器 24.一颗二叉树的叶子节点有5个，出度为1的结点有3个，该二叉树的结点总个数是 12 度数为0的节点数量 = 度数为2的节点数量 + 1 本题中：5 = x + 1 可得度数为2节点数量：x=4。 所以这颗二叉树的总节点为：5+3+4=12 25.下面linux命令中，可以用来检查内存使用状况的有 A. sed B. free C. top D. iostat BC free 查看内存使用情况。 free -m 以M为单位显示 top 动态命令，查看进程和内存的动态情况。如果此时输入M，按进程使用内存大小排序。如果输入P，按照进程占用CPU的大小排序。 26.下列关于JVM性能调优工具的描述，错误的是 A. jps主要是用来输出JVM中运行的进程状态信息 B. jmap是用来查看堆栈内存使用状况 C. jstack主要是用来查看某个Java进程内的线程堆栈信息 D. jstat是JVM统计监测工具 C jps：主要用来输出JVM中运行的进程状态信息 jmap：用来查看堆内存使用状况，一般结合jhat使用 jstack：堆栈跟踪工具，主要用来查看某个Java进程内的线程堆栈信息 jstat ：JVM统计监测工具，查看各个内存和GC的情况 hprof：展现CPU使用率，统计堆内存使用情况 27.某系统中有4个并发进程，多需要同类资源5个，试问该系统无论怎样都不会发生死锁的最少资源数是 17 4*(5-1) + 1 28. public class Person{ { System.out.println(&quot;P1&quot;); } static{ System.out.println(&quot;P2&quot;); } public Person(){ System.out.println(&quot;P3&quot;); } } public class Students extends Person{ static{ System.out.println(&quot;S1&quot;); } { System.out.println(&quot;S2&quot;); } public Students(){ System.out.println(&quot;S3&quot;); } public static void main(String[] args){ new Students(); } } P2S1P1P3S2S3 执行顺序：父类静态代码块 -&gt; 子类静态代码块 -&gt; 父类代码块 -&gt; 父类构造方法 -&gt; 子类代码块 -&gt;子类构造方法 首先 当JVM遇到new Students() 时，会去加载Student类，加载Student类之前要去加载父类Person，加载过程中会初始化变量和去初始化static的东西，所以先父类静态代码块后子类静态代码块，类加载完成后，就需要在内存里创建对象了，创建Student对象需要创建父类People，创建People类对象先执行代码块再执行构造器，父类执行完后执行子类的代码块和构造器。 29. public class Test { public static void main(String[] args) { Test t = new Test(); t.method(null); } public void method(Object o) { System.out.println(&quot;Object&quot;); } public void method(String s) { System.out.println(&quot;String&quot;); } } 输出结果：String Object 和 String 值都可以为null。但存在继承关系，因此会将不确定对象null当作子类型处理。 最精准原则String比Object精确，因为一个String本质也是一个Object，但不是每个Object都可以是String的 30. public class Arraytest { int a[] = new int[6]; public static void main (String arg[]) { System.out.println(a[0]); } } 编译时出错 a数组不是静态类型，不能在静态的main方法中直接使用","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"}]},{"title":"Redis 问答 ","slug":"Redis 问答 ","date":"2020-09-17T13:23:28.194Z","updated":"2023-02-06T11:49:15.858Z","comments":true,"path":"2020/09/17/Redis 问答 /","link":"","permalink":"https://topone233.github.io/2020/09/17/Redis%20%E9%97%AE%E7%AD%94%20/","excerpt":"","text":"原文地址 什么是 Redis?Redis 是完全开源免费的，遵守 BSD 协议，是性能极高的 nosql 数据库，Key-Value 数据库，并提供多种语言的 API 的非关系型数据库。 Redis 读的速度能达到 110000 次 / s, 写的速度能达到 81000 次 / s 。 Redis 支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis 支持数据的备份，即 master-slave 模式的数据备份。 丰富的数据类型：有五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及 zset(sorted set：有序集合)。 Redis 的所有操作是原子性的，意思就是要么成功执行要么失败完全不执行。 还有丰富的特性：Redis 还支持 publish/subscribe, 通知, key 过期等等特性。 Redis 与其他 key-value 存储有什么不同？Redis 有着更为复杂的数据结构，并且提供对他们的原子性操作，这是一个不同于其 他数据库的进化路径。Redis 的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。Redis 运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时 需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点 是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样 Redis 可以做很多内部复杂性很强的事情。同时，在磁盘格式方面他们是紧凑的以追加 的方式产生的，因为他们并不需要进行随机访问。 持久化机制 RDB（默认） 和 AOF 机制:RDBRedis DataBase 缩写快照 RDB 是 Redis 默认的持久化方式。按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为 dump.rdb。通过配置文件中的 save 参数来定义快照的周期。 优点： 1、只有一个文件 dump.rdb，方便持久化。 2、容灾性好，一个文件可以保存到安全的磁盘。 3、性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能 4. 相对于数据集大时，比 AOF 的启动效率更高。 缺点： 1、数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候) 2、AOF（Append-only file) 持久化方式： 是指所有的命令行记录以 redis 命令请 求协议的格式完全持久化存储) 保存为 aof 文件。 AOF：持久化AOF 持久化 (即 Append Only File 持久化)，则是将 Redis 执行的每次写命令记录到单独的日志文件中，当重启 Redis 会重新将持久化的日志中文件恢复数据。当两种方式同时开启时，数据恢复 Redis 会优先选择 AOF 恢复。 优点： 1、数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次 命令操作就记录到 aof 文件中一次。 2、通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。 3、AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall）) 缺点： 1、AOF 文件比 RDB 文件大，且恢复速度慢。 2、数据集大的时候，比 rdb 启动效率低。 优缺点是什么？ 1、AOF 文件比 RDB 更新频率高，优先使用 AOF 还原数据。2、AOF 比 RDB 更安全也更大3、RDB 性能比 AOF 好4、如果两个都配了优先加载 AOF redis 常见性能问题和解决方案： (1) Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件(2) 如果数据比较重要，某个 Slave 开启 AOF 备份数据，策略设置为每秒同步一次(3) 为了主从复制的速度和连接的稳定性，Master 和 Slave 最好在同一个局域网内(4) 尽量避免在压力很大的主库上增加从库(5) 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3… 这样的结构方便解决单点故障问题，实现 Slave 对 Master 的替换。如果 Master 挂了，可以立刻启用 Slave1 做 Master，其他不变。 支持的数据类型String 字符串：常用命令: set,get,decr,incr,mget 等。String 数据结构是简单的 key-value 类型，value 其实不仅可以是 String，也可以是数字。string 类型是 Redis 最基本的数据类型，一个键最大能存储 512MB。常规 key-value 缓存应用； 常规计数：微博数，粉丝数等。 Hash（哈希）常用命令： hget,hset,hgetall 等。Hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 Hash 数据结构来存储用户信息，商品信息等等。比如下面我就用 hash 类型存放了我本人的一些信息： key JavaUser293847 value { “id”: 1, “name”: “SnailClimb”, “age”: 22, “location”: “Wuhan, Hubei” }List（列表）Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）格式: lpush name value在 key 对应 list 的头部添加字符串元素格式: rpush name value在 key 对应 list 的尾部添加字符串元素格式: lrem name indexkey 对应 list 中删除 count 个和 value 相同的元素格式: llen name返回 key 对应 list 的长度 Set（集合）格式: sadd name valueRedis 的 Set 是 string 类型的无序集合。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 zset(sorted set：有序集合)格式: zadd name score valueRedis zset 和 set 一样也是 string 类型元素的集合, 且不允许重复的成员。不同的是每个元素都会关联一个 double 类型的分数。redis 正是通过分数来为集合中的成员进行从小到大的排序。zset 的成员是唯一的, 但分数 (score) 却可以重复。 分布式锁使用过 Redis 分布式锁么，它是怎么实现的？ 先拿 setnx 来争抢锁，抢到之后，再用 expire 给锁加一个过期时间防止锁忘记了释放。如果在 setnx 之后执行 expire 之前进程意外 crash 或者要重启维护了，那会怎么样？set 指令有非常复杂的参数，这个应该是可以同时把 setnx 和 expire 合成一条指令来用的！ 异步队列使用过 Redis 做异步队列么，你是怎么用的？有什么缺点？ 一般使用 list 结构作为队列，rpush 生产消息，lpop 消费消息。当 lpop 没有消息的时候，要适当 sleep 一会再重试。 缺点在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如 rabbitmq 等。能不能生产一次消费多次呢？使用 pub/sub 主题订阅者模式，可以实现 1:N 的消息队列。 缓存雪崩缓存雪崩我们可以简单的理解为：由于原有缓存失效，新缓存未到期间(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库 CPU 和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，会给后端系统带来很大压力。导致系统崩溃。 解决办法大多数系统设计者考虑用加锁（ 最多的解决方案）或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。还有一个简单方案就时讲缓存失效时间分散开。 如何避免1：在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个 key 只允许一个线程查询数据和写缓存，其他线程等待。2：做二级缓存，A1 为原始缓存，A2 为拷贝缓存，A1 失效时，可以访问 A2，A1 缓存失效时间设置为短期，A2 设置为长期3：不同的 key，设置不同的过期时间，让缓存失效的时间点尽量均匀。 缓存穿透缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。每次都要去数据库再查询一遍，然后返回空，这样请求就绕过缓存直接查数据库，如果恶意请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。 解决办法 1. 最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力。 2. 也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴。 缓存击穿缓存中的一个 Key(比如一个促销商品)，在某个时间点过期的时候，恰好在这个时间点对这个 Key 有大量的并发请求过来，这些请求发现缓存过期一般都会从后端 DB 加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端 DB 压垮。 解决方案对缓存查询加锁，如果 KEY 不存在，就加锁，然后查 DB 入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入 DB 查询。 缓存预热缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 解决思路：1、直接写个缓存刷新页面，上线时手工操作下；2、数据量不大，可以在项目启动的时候自动进行加载；3、定时刷新缓存； 缓存更新除了缓存服务器自带的缓存失效策略之外（Redis 默认的有 6 中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：（1）定时去清理过期的缓存；（2）当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。两者各有优劣，第一种的缺点是维护大量缓存的 key 是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。 缓存降级当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。以参考日志级别设置预案：（1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；（2）警告：有些服务在一段时间内成功率有波动（如在 95~100% 之间），可以自动降级或人工降级，并发送告警；（3）错误：比如可用率低于 90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；（4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。服务降级的目的，是为了防止 Redis 服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis 出现问题，不去数据库查询，而是直接返回默认值给用户。 缓存热点数据和冷数据热点数据，缓存才有价值对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。频繁修改的数据，看情况考虑使用缓存，对于上面两个例子，寿星列表、导航信息都存在一个特点，就是信息修改频率不高，读取通常非常高的场景。对于热点数据，比如我们的某 IM 产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。数据更新前至少读取两次，缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那就没有太大价值了。 过期策略以及内存淘汰机制redis 采用的是定期删除 + 惰性删除策略。 1、定时删除: 在设置键的过期时间的同时，创建一个定时器 timer). 让定时器在键 的过期时间来临时，立即执行对键的删除操作。 2、惰性删除: 放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是 否过期，如果过期的话，就删除该键; 如果没有过期，就返回该键。 3、定期删除: 每隔一段时间程序就对数据库进行一次检查，删除里面的过期键。至 于要删除多少过期键，以及要检查多少个数据库，则由算法决定。 为什么不用定时删除策略定时删除, 用一个定时器来负责监视 key, 过期则自动删除。虽然内存及时释放，但是十分消耗 CPU 资源。在大并发请求下，CPU 要将时间应用在处理请求，而不是删除 key, 因此没有采用这一策略. 如何工作定期删除，redis 默认每个 100ms 检查，是否有过期的 key, 有过期 key 则删除。需要说明的是，redis 不是每个 100ms 将所有的 key 检查一次，而是随机抽取进行检查 (如果每隔 100ms, 全部 key 进行检查，redis 岂不是卡死)。因此，如果只采用定期删除策略，会导致很多 key 到时间没有删除。于是，惰性删除派上用场。也就是说在你获取某个 key 的时候，redis 会检查一下，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除。 如果定期删除没删除 key。然后你也没即时去请求 key，也就是说惰性删除也没生效。这样，redis 的内存会越来越高。那么就应该采用内存淘汰机制。在 redis.conf 中有一行配置maxmemory-policy volatile-lru 该配置就是配内存淘汰策略的 回收策略（淘汰策略） volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最 少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过 期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意 选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 . no-enviction（驱逐）：禁止驱逐数据注意这里的 6 种机制，volatile 和 allkeys 规定了是对已设置过期时间的数据集淘 汰数据还是从全部数据集淘汰数据，后面的 lru、ttl 以及 random 是三种不同的 淘汰策略，再加上一种 no-enviction 永不回收的策略。 使用策略规则 1、如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率 低，则使用 allkeys-lru 2、如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用 allkeys-random 架构模式单机版 特点：简单 问题： 内存容量有限 处理能力有限 无法高可用。 主从复制 Redis 的复制（replication）功能允许用户根据一个 Redis 服务器来创建任意多个该服务器的复制品，其中被复制的服务器为主服务器（master），而通过复制创建出来的服务器复制品则为从服务器（slave）。 只要主从服务器之间的网络连接正常，主从服务器两者会具有相同的数据，主服务器就会一直将发生在自己身上的数据更新同步 给从服务器，从而一直保证主从服务器的数据相同。 特点： master/slave 角色 master/slave 数据相同 降低 master 读压力在转交从库 问题： 无法保证高可用没有解决 master 写的压力 哨兵 Redis sentinel 是一个分布式系统中监控 redis 主从服务器，并在主服务器下线时自动进行故障转移。其中三个特性： 监控（Monitoring）Sentinel 会不断地检查你的主服务器和从服务器是否运作正常。 提醒（Notification）当被监控的某个 Redis 服务器出现问题时， Sentinel 可以通过 API 向管理员或者其他应用程序发送通知。 自动故障迁移（Automatic failover）当一个主服务器不能正常工作时， Sentinel 会开始一次自动故障迁移操作。 特点： 1、保证高可用2、监控各个节点3、自动故障迁移 缺点 主从模式，切换需要时间丢数据没有解决 master 写的压力 集群（proxy 型）： Twemproxy 是一个 Twitter 开源的一个 redis 和 memcache 快速 / 轻量级代理服务器； Twemproxy 是一个快速的单线程代理程序，支持 Memcached ASCII 协议和 redis 协议。 特点： 1、多种 hash 算法：MD5、CRC16、CRC32、CRC32a、hsieh、murmur、Jenkins2、支持失败节点自动删除3、后端 Sharding 分片逻辑对业务透明，业务方的读写方式和操作单个 Redis 一致 缺点： 增加了新的 proxy，需要维护其高可用。failover 逻辑需要自己实现，其本身不能支持故障的自动转移可扩展性差，进行扩缩容都需要手动干预 集群（直连型）： 从 redis 3.0 之后版本支持 redis-cluster 集群，Redis-Cluster 采用无中心结构，每个节点保存数据和整个集群状态, 每个节点都和其他所有节点连接。 特点： 1、无中心架构（不存在哪个节点影响性能瓶颈），少了 proxy 层。2、数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。3、可扩展性，可线性扩展到 1000 个节点，节点可动态添加或删除。4、高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做备份数据副本5、实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制完成 Slave 到 Master 的角色提升。 缺点： 1、资源隔离性较差，容易出现相互影响的情况。2、数据通过异步复制, 不保证数据的强一致性。 常见问题1.Redis 为什么是单线程的Redis 是基于内存的操作，CPU 不是 Redis 的瓶颈，Redis 的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且 CPU 不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）Redis 利用队列技术将并发访问变为串行访问1）绝大部分请求是纯粹的内存操作（非常快速）2）采用单线程, 避免了不必要的上下文切换和竞争条件3）非阻塞 IO 优点： 速度快，因为数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是 O(1) 支持丰富数据类型，支持 string，list，set，sorted set，hash 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按 key 设置过期时间，过期后将会自动删除如何解决 redis 的并发竞争 key 问题 同时有多个子系统去 set 一个 key。这个时候要注意什么呢？ 不推荐使用 redis 的事务机制。因为我们的生产环境，基本都是 redis 集群环境，做了数据分片操作。你一个事务中有涉及到多个 key 操作的时候，这多个 key 不一定都存储在同一个 redis-server 上。因此，redis 的事务机制，十分鸡肋。(1) 如果对这个 key 操作，不要求顺序： 准备一个分布式锁，大家去抢锁，抢到锁就做 set 操作即可(2) 如果对这个 key 操作，要求顺序： 分布式锁 + 时间戳。 假设这会系统 B 先抢到锁，将 key1 设置为 {valueB 3:05}。接下来系统 A 抢到锁，发现自己的 valueA 的时间戳早于缓存中的时间戳，那就不做 set 操作了。以此类推。(3) 利用队列，将 set 方法变成串行访问也可以 redis 遇到高并发，如果保证读写 key 的一致性对 redis 的操作都是具有原子性的, 是线程安全的操作, 你不用考虑并发问题, redis 内部已经帮你处理好并发的问题了。 2.对于大量的请求怎么样处理redis 是一个单线程程序，也就说同一时刻它只能处理一个客户端请求；redis 是通过 IO 多路复用（select，epoll, kqueue，依据不同的平台，采取不同的实现）来处理多个客户端请求的 3.常见性能问题和解决方案(1) Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件(2) 如果数据比较重要，某个 Slave 开启 AOF 备份数据，策略设置为每秒同步一次(3) 为了主从复制的速度和连接的稳定性， Master 和 Slave 最好在同一个局域网内(4) 尽量避免在压力很大的主库上增加从库(5) 主从复制不要用图状结构，用单向链表结构更为稳定，即： Master &lt;- Slave1 &lt;- Slave2 &lt;-Slave3… 4.为什么 Redis 的操作是原子性的，怎么保证原子性的？对于 Redis 而言，命令的原子性指的是：一个操作的不可以再分，操作要么执行，要么不执行。Redis 的操作之所以是原子性的，是因为 Redis 是单线程的。Redis 本身提供的所有 API 都是原子操作，Redis 中的事务其实是要保证批量操作的原子性。 5.多个命令在并发中也是原子性的吗？不一定， 将 get 和 set 改成单命令操作，incr 。使用 Redis 的事务，或者使用 Redis+Lua 的方式实现. 6.Redis 事务Redis 事务功能是通过 MULTI、EXEC、DISCARD 和 WATCH 四个原语实现的Redis 会将一个事务中的所有命令序列化，然后按顺序执行。1.redis 不支持回滚 “Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。2. 如果在一个事务中的命令出现错误，那么所有的命令都不会执行；3. 如果在一个事务中出现运行错误，那么正确的命令会被执行。 1）MULTI 命令用于开启一个事务，它总是返回 OK。 MULTI 执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当 EXEC 命令被调用时，所有队列中的命令才会被执行。2）EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。3）通过调用 DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。4）WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到 EXEC 命令。 7.Redis 实现分布式锁Redis 为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对 Redis 的连接并不存在竞争关系 Redis 中可以使用 SETNX 命令实现分布式锁。将 key 的值设为 value ，当且仅当 key 不存在。 若给定的 key 已经存在，则 SETNX 不做任何动作 解锁：使用 del key 命令就能释放锁解决死锁：1）通过 Redis 中 expire() 给锁设定最大持有时间，如果超过，则 Redis 来帮我们释放锁。2） 使用 setnx key “当前系统时间 + 锁持有的时间” 和 getset key “当前系统时间 + 锁持有的时间” 组合的命令就可以实现。 8.为什么 Redis 需要把所有数据放到内存中？Redis 为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数 据写入磁盘。所以 redis 具有快速和数据持久化的特征。如果不将数据放在内存中， 磁盘 I/O 速度为严重影响 redis 的性能。在内存越来越便宜的今天，redis 将会越 来越受欢迎。如果设置了最大使用的内存，则数据已有记录数达到内存限值后不 能继续插入新值。 9.Redis 的同步机制了解么？Redis 可以使用主从同步，从从同步。第一次同步时，主节点做一次 bgsave， 并同时将后续修改操作记录到内存 buffer，待完成后将 rdb 文件全量同步到复制 节点，复制节点接受完成后将 rdb 镜像加载到内存。加载完成后，再通知主节点 将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 10.Pipeline 有什么好处，为什么要用 pipeline？可以将多次 IO 往返的时间缩减为一次，前提是 pipeline 执行的指令之间没有 因果相关性。使用 redis-benchmark 进行压测的时候可以发现影响 redis 的 QPS 峰值的一个重要因素是 pipeline 批次指令的数目。 11.是否使用过 Redis 集群，集群的原理是什么？1)、Redis Sentinal 着眼于高可用，在 master 宕机时会自动将 slave 提升为 master，继续提供服务。2)、Redis Cluster 着眼于扩展性，在单个 redis 内存不足时，使用 Cluster 进行 分片存储。 12.Redis 集群方案什么情况下会导致整个集群不可用？有 A，B，C 三个节点的集群, 在没有复制模型的情况下, 如果节点 B 失败了， 那么整个集群就会以为缺少 5501-11000 这个范围的槽而不可用。 13.Redis 支持的 Java 客户端都有哪些？官方推荐用哪个？Redisson、Jedis、lettuce 等等，官方推荐使用 Redisson。 14.Jedis 与 Redisson 对比有什么优缺点？Jedis 是 Redis 的 Java 实现的客户端，其 API 提供了比较全面的 Redis 命令 的支持；Redisson 实现了分布式和可扩展的 Java 数据结构，和 Jedis 相比，功能 较为简单，不支持字符串操作，不支持排序、事务、管道、分区等 Redis 特性。 Redisson 的宗旨是促进使用者对 Redis 的关注分离，从而让使用者能够将精力更 集中地放在处理业务逻辑上。 15.Redis 如何设置密码及验证密码？设置密码：config set requirepass 123456授权密码：auth 123456 16.说说 Redis 哈希槽的概念？Redis 集群没有使用一致性 hash, 而是引入了哈希槽的概念，Redis 集群有 16384 个哈希槽，每个 key 通过 CRC16 校验后对 16384 取模来决定放置哪个槽， 集群的每个节点负责一部分 hash 槽。 17.Redis 集群的主从复制模型是怎样的？为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所 以集群使用了主从复制模型, 每个节点都会有 N-1 个复制品. 18.Redis 集群会有写操作丢失吗？为什么？Redis 并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可 能会丢失写操作。 19.Redis 集群之间是如何复制的？异步复制 20.Redis 集群最大节点个数是多少？16384 个 21.Redis 集群如何选择数据库？Redis 集群目前无法做数据库选择，默认在 0 数据库。 22.怎么测试 Redis 的连通性？使用 ping 命令。 23.怎么理解 Redis 事务？1）事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。 事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。2）事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 24.Redis 事务相关的命令有哪几个？MULTI、EXEC、DISCARD、WATCH Redis key 的过期时间和永久有效分别怎么设置？ EXPIRE 和 PERSIST 命令。 25.Redis 如何做内存优化？尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用 的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比 如你的 web 系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码 设置单独的 key, 而是应该把这个用户的所有信息存储到一张散列表里面. 26.Redis 回收进程如何工作的？一个客户端运行了新的命令，添加了新的数据。Redi 检查内存使用情况，如 果大于 maxmemory 的限制, 则根据设定好的策略进行回收。一个新的命令被执 行，等等。所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地 回收回到边界以下。如果一个命令的结果导致大量内存被使用（例如很大的集合 的交集保存到一个新的键），不用多久内存限制就会被这个内存使用量超越。 27.哪些办法可以降低 Redis 的内存使用情况呢？如果你使用的是 32 位的 Redis 实例，可以好好利用 Hash,list,sorted set,set 等集合类型数据，因为通常情况下很多小的 Key-Value 可以用更紧凑的方式存放 到一起。 28.Redis 的内存用完了会发生什么？如果达到设置的上限，Redis 的写命令会返回错误信息（但是读命令还可以正 常返回。）或者你可以将 Redis 当缓存来使用配置淘汰机制，当 Redis 达到内存 上限时会冲刷掉旧的内容。 29.一个 Redis 实例最多能存放多少的 keys？List、Set、 Sorted Set 他们最多能存放多少元素？理论上 Redis 可以处理多达 232 的 keys，并且在实际中进行了测试，每个实 例至少存放了 2 亿 5 千万的 keys。我们正在测试一些较大的值。任何 list、set、 和 sorted set 都可以放 232 个元素。换句话说，Redis 的存储极限是系统中的可 用内存值。 MySQL 里有 2000w 数据，redis 中只存 20w 的数据，如何保证 redis 中的数据都是热点数据？ Redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。相关知识：Redis 提供 6 种数据淘汰策略：volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最 少使用的数据淘汰volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过 期的数据淘汰volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意 选择数据淘汰allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰no-enviction（驱逐）：禁止驱逐数据 30.Redis 最适合的场景？1、会话缓存（Session Cache）最常用的一种使用 Redis 的情景是会话缓存（session cache）。用 Redis 缓存会 话比其他存储（如 Memcached）的优势在于：Redis 提供持久化。当维护一个不 是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不 高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容 易找到怎么恰当的使用 Redis 来缓存会话的文档。甚至广为人知的商业平台 Magento 也提供 Redis 的插件。 2、全页缓存（FPC）除基本的会话 token 之外，Redis 还提供很简便的 FPC 平台。回到一致性问题， 即使重启了 Redis 实例，因为有磁盘的持久化，用户也不会看到页面加载速度的 下降，这是一个极大改进，类似 PHP 本地 FPC。 再次以 Magento 为例，Magento 提供一个插件来使用 Redis 作为全页缓存后端。 此外，对 WordPress 的用户来 说，Pantheon 有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加 载你曾浏览过的页面。 3、队列Reids 在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得 Redis 能作为一个很好的消息队列平台来使用。Redis 作为队列使用的操作，就类似于本 地程序语言（如 Python）对 list 的 push/pop 操作。 如果你快速的在 Google 中搜索 “Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就 是利用 Redis 创建非常好的后端工具，以满足各种队列需求。例如，Celery 有一 个后台就是使用 Redis 作为 broker，你可以从这里去查看。 4，排行榜 / 计数器Redis 在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序 集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis 只是 正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的 10 个用户–我们称之为 “user_scores”，我们只需要像下面一样执行即可： 当然， 这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数， 你需要这样执行： ZRANGE user_scores 0 10 WITHSCORES Agora Games 就 是一个很好的例子，用 Ruby 实现的，它的排行榜就是使用 Redis 来存储数据的， 你可以在这里看到。 5、发布 / 订阅最后（但肯定不是最不重要的）是 Redis 的发布 / 订阅功能。发布 / 订阅的使用场景 确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布 / 订阅的脚 本触发器，甚至用 Redis 的发布 / 订阅功能来建立聊天系统！ 31.假如 Redis 里面有 1 亿个 key，其中有 10w 个 key 是以 某个固定的已知的前缀开头的，如果将它们全部找出来？使用 keys 指令可以扫出指定模式的 key 列表。 对方接着追问：如果这个 redis 正在给线上的业务提供服务，那使用 keys 指令会 有什么问题？这个时候你要回答 redis 关键的一个特性：redis 的单线程的。keys 指令会导致线 程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时 候可以使用 scan 指令，scan 指令可以无阻塞的提取出指定模式的 key 列表，但 是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间 会比直接用 keys 指令长。 32.如果有大量的 key 需要设置同一时间过期，一般需要注意 什么？如果大量的 key 过期时间设置的过于集中，到过期的那个时间点，redis 可能 会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一 些。 33.使用过 Redis 做异步队列么，你是怎么用的？一般使用 list 结构作为队列，rpush 生产消息，lpop 消费消息。当 lpop 没有 消息的时候，要适当 sleep 一会再重试。 如果对方追问可不可以不用 sleep 呢？list 还有个指令叫 blpop，在没有消息的时候，它会阻塞住直到消息到来。如果对 方追问能不能生产一次消费多次呢？使用 pub/sub 主题订阅者模式，可以实现 1:N 的消息队列。 如果对方追问 pub/sub 有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如 RabbitMQ 等。 如果对方追问 redis 如何实现延时队列？我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这 么详细。但是你很克制，然后神态自若的回答道：使用 sortedset，拿时间戳作为 score，消息内容作为 key 调用 zadd 来生产消息，消费者用 zrangebyscore 指令 获取 N 秒之前的数据轮询进行处理。到这里，面试官暗地里已经对你竖起了大拇 指。但是他不知道的是此刻你却竖起了中指，在椅子背后。 Redis 集群方案应该怎么做？都有哪些方案？ 1.twemproxy，大概概念是，它类似于一个代理方式， 使用时在本需要连接 redis 的地方改为连接 twemproxy， 它会以一个代理的身份接收请求并使用一致性 hash 算法，将请求转接到具体 redis，将结果再返回 twemproxy。缺点： twemproxy 自身单端口实例的压力，使用一致性 hash 后，对 redis 节点数量改变时候的计算值的改变，数据无法自动移动到新的节点。 2.codis，目前用的最多的集群方案，基本和 twemproxy 一致的效果，但它支持在 节点数量改变情况下，旧节点数据可恢复到新 hash 节点 3.redis cluster3.0 自带的集群，特点在于他的分布式算法不是一致性 hash，而是 hash 槽的概念，以及自身支持节点设置从节点。具体看官方文档介绍。","categories":[{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/categories/nosql/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/tags/nosql/"},{"name":"Redis","slug":"redis","permalink":"https://topone233.github.io/tags/redis/"}]},{"title":"为什么要控制反转？","slug":"为什么要控制反转？","date":"2020-09-17T04:40:14.940Z","updated":"2023-02-06T11:45:51.477Z","comments":true,"path":"2020/09/17/为什么要控制反转？/","link":"","permalink":"https://topone233.github.io/2020/09/17/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%8E%A7%E5%88%B6%E5%8F%8D%E8%BD%AC%EF%BC%9F/","excerpt":"","text":"原文地址 是什么？ IOC 全称是 Inversion of Control控制反转。按照字面意思理解，将控制反转过来，这里的控制指的是什么？为什么要进行反转，ioc 可以解决什么问题？要回答这些问题，我们需要先了解一下 IOC 为什么会产生。 怎么来的？ Java 是一门面向对象的语言，我们的应用程序通过一个个对象之间的相互关联和作用来完成功能，就像手表里的机械结构。每一个齿轮代表一个对象，对象之间彼此紧密咬合形成一个系统，这样的系统对象之间的耦合度非常高，所谓的耦合度就是关系的依赖程度，高耦合度带来的问题显而易见，只要有一个齿轮发生故障，其它齿轮也无法工作，进而整个系统都无法正常工作，这种牵一发而动全身情况如何才能改善呢？ 再来一个 Service 层实际的例子： public class UserServiceImpl { private UserDao userDao = new UserDaoImpl(); private UserDao userDao = (UserDao)BeanFactory.getBean(&quot;userDao&quot;); public List&lt;User&gt; getAllUser(){ return userDao.getAllUser(); } }一个是独立控制通过 new 一个 UserDao 实现类来完成，一个是 Bean 工厂通过全限定类名找到 Bean 对象并创建多例对象，无法自主控制。第二者把控制权交给了 Bean 工厂来创建对象，带来的好处就是降低程序间的依赖关系，也叫削减计算机的耦合。 改善方法？ 上面机械齿轮的例子可以通过一个中间齿轮的方式来解决，也就是后面的中间 IOC 容器。所有的齿轮都交由中间这个齿轮管理，试着把中间这个齿轮拿掉我们可以看到这两个齿轮之间彼此毫无关系，即使一个齿轮出了故障，也不会影响到其它齿轮。中间这个齿轮就好比 ioc 容器，其它齿轮就是对象，可以看出引入了 ioc 容器，对象之间的耦合度降低了。当我们修改一个对象的时候不需要去考虑其它对象，因为它不会对其它对象造成影响。 IoC 原理？ 这里说到的 ioc 容器到底是个什么东东，又是什么让它具有如此神奇的力量？ 先来看一下没有 ioc 容器的时候，对象 A 依赖对象 B，A 在运行到某一时刻的时候会去创建 B 的对象，在这里 A 具有主动权，它控制了对象 B 的创建。 引入 ioc 以后对象 A 和对象 B 之间没有了直接联系，当 A 运行的时候由 ioc 容器创建 B 对象在适当的时候注入到 A 中，在这里，控制权由 A 对象转移到了 ioc 容器。这也就是控制反转名称的由来。 基于上述 UserDao 的例子我们可以通过反射来解耦，反射可以根据类的全限定名在程序运行时创建对象，可以这样做，将类的全限定名配置在 xml 文件中，在程序运行时通过反射读取该类的全限定名，动态的创建对象，赋值给 userDao 接口 userDaoImpl。这样做后 UserServiceImpl 和 UserDaoImpl 之间没有了直接的关系，当我们需要替换 UserDaoImpl 对象的时候只需要在配置文件中去修改类的全限定名就可以了，非常的灵活方便，ioc 容器的实现就是这个原理。 IOC 容器可以自动的帮我们完成以上一系列操作，我们需要做的就是通过配置文件告诉 ioc 需要创建哪个类以及类和类之间的关系。 控制反转和依赖注入 在这里需要提到一个概念依赖注入，很多初学者搞不清楚控制反转和依赖注入之间的关系，其实他们是对同一事物的不同角度的描述。控制反转是一种设计思想而依赖注入是这种思想的具体实现 具体说控制反转就是将创建 userDaoImpl 对象的控制权反转过来由 UserServiceImpl 交给了 ioc 容器，强调的是一种能力和思想，ioc 容器具有了控制权。 依赖注入就是 ioc 容器将 UserServiceImpl 所依赖的对象 userDaoImpl，注入给 UserServiceImpl，强调的是一个过程和实现。 IOC 很好的体现了面向对象设计法则之一—— 好莱坞法则：“别找我们，我们找你”。 优缺点 软件系统中由于引入了第三方 IOC 容器，生成对象的步骤变得有些复杂，本来是两者之间的事情，又凭空多出一道手续，所以，我们在刚开始使用 IOC 框架的时候，会感觉系统变得不太直观。所以，引入了一个全新的框架，就会增加团队成员学习和认识的培训成本，并且在以后的运行维护中，还得让新加入者具备同样的知识体系。 由于 IOC 容器生成对象是通过反射方式，在运行效率上有一定的损耗。如果你要追求运行效率的话，就必须对此进行权衡。 具体到 IOC 框架产品 (比如：Spring) 来讲，需要进行大量的配制工作，比较繁琐，对于一些小的项目而言，客观上也可能加大一些工作成本。 IOC 框架产品本身的成熟度需要进行评估，如果引入一个不成熟的 IOC 框架产品，那么会影响到整个项目，所以这也是一个隐性的风险。 我们大体可以得出这样的结论：一些工作量不大的项目或者产品，不太适合使用 IOC 框架产品。另外，如果团队成员的知识能力欠缺，对于 IOC 框架产品缺乏深入的理解，也不要贸然引入。最后，特别强调运行效率的项目或者产品，也不太适合引入 IOC 框架产品，像 WEB2.0 网站就是这种情况。 Spring 框架文档：https://yoyling.com/spring5 本文由 YOYLING. 发表， 最后编辑时间为：2020-09-08 13:02如果你觉得我的文章不错，不妨鼓励我继续写作。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"Redis 趣讲","slug":"Redis 趣讲","date":"2020-09-15T16:00:00.000Z","updated":"2023-02-06T11:48:59.680Z","comments":true,"path":"2020/09/16/Redis 趣讲/","link":"","permalink":"https://topone233.github.io/2020/09/16/Redis%20%E8%B6%A3%E8%AE%B2/","excerpt":"","text":"原文地址 我是 Redis我是 Redis，一个叫 Antirez 的男人把我带到了这个世界上。 说起我的诞生，跟关系数据库 MySQL 还挺有渊源的。 在我还没来到这个世界上的时候，MySQL 过的很辛苦，互联网发展的越来越快，它容纳的数据也越来越多，用户请求也随之暴涨，而每一个用户请求都变成了对它的一个又一个读写操作，MySQL 是苦不堪言。尤其是到 “双 11”、“618“这种全民购物狂欢的日子，都是 MySQL 受苦受难的日子。 据后来 MySQL 告诉我说，其实有一大半的用户请求都是读操作，而且经常都是重复查询一个东西，浪费它很多时间去进行磁盘 I/O。 后来有人就琢磨，是不是可以学学 CPU，给数据库也加一个缓存呢？于是我就诞生了！ 出生不久，我就和 MySQL 成为了好朋友，我们俩常常携手出现在后端服务器中。 应用程序们从 MySQL 查询到的数据，在我这里登记一下，后面再需要用到的时候，就先找我要，我这里没有再找 MySQL 要。 为了方便使用，我支持好几种数据结构的存储： String Hash List Set SortedSet Bitmap ······ 因为我把登记的数据都记录在内存中，不用去执行慢如蜗牛的 I/O 操作，所以找我要比找 MySQL 要省去了不少的时间呢。 可别小瞧这简单的一个改变，我可为 MySQL 减轻了不小的负担！随着程序的运行，我缓存的数据越来越多，有相当部分时间我都给它挡住了用户请求，这一下它可乐得清闲自在了！ 有了我的加入，网络服务的性能提升了不少，这都归功于我为数据库挨了不少枪子儿。 缓存过期 &amp;&amp; 缓存淘汰不过很快我发现事情不妙了，我缓存的数据都是在内存中，可是就算是在服务器上，内存的空间资源还是很有限的，不能无节制的这么存下去，我得想个办法，不然吃枣药丸。 不久，我想到了一个办法：给缓存内容设置一个超时时间，具体设置多长交给应用程序们去设置，我要做的就是把过期了的内容从我里面删除掉，及时腾出空间就行了。 超时时间有了，我该在什么时候去干这个清理的活呢？ 最简单的就是定期删除，我决定 100ms 就做一次，一秒钟就是 10 次！ 我清理的时候也不能一口气把所有过期的都给删除掉，我这里面存了大量的数据，要全面扫一遍的话那不知道要花多久时间，会严重影响我接待新的客户请求的！ 时间紧任务重，我只好随机选择一部分来清理，能缓解内存压力就行了。 就这样过了一段日子，我发现有些个键值运气比较好，每次都没有被我的随机算法选中，每次都能幸免于难，这可不行，这些长时间过期的数据一直霸占着不少的内存空间！气抖冷！ 我眼里可揉不得沙子！于是在原来定期删除的基础上，又加了一招： 那些原来逃脱我随机选择算法的键值，一旦遇到查询请求，被我发现已经超期了，那我就绝不客气，立即删除。 这种方式因为是被动式触发的，不查询就不会发生，所以也叫惰性删除！ 可是，还是有部分键值，既逃脱了我的随机选择算法，又一直没有被查询，导致它们一直逍遥法外！而于此同时，可以使用的内存空间却越来越少。 而且就算退一步讲，我能够把过期的数据都删除掉，那万一过期时间设置的很长，还没等到我去清理，内存就吃满了，一样要吃枣药丸，所以我还得想个办法。 我苦思良久，终于憋出了个大招：内存淘汰策略，这一次我要彻底解决问题！ 我提供了 8 种策略供应用程序选择，用于我遇到内存不足时该如何决策： noeviction：返回错误，不会删除任何键值 allkeys-lru：使用 LRU 算法删除最近最少使用的键值 volatile-lru：使用 LRU 算法从设置了过期时间的键集合中删除最近最少使用的键值 allkeys-random：从所有 key 随机删除 volatile-random：从设置了过期时间的键的集合中随机删除 volatile-ttl：从设置了过期时间的键中删除剩余时间最短的键 volatile-lfu：从配置了过期时间的键中删除使用频率最少的键 allkeys-lfu：从所有键中删除使用频率最少的键 有了上面几套组合拳，我再也不用担心过期数据多了把空间撑满的问题了~ 缓存穿透 &amp;&amp; 布隆过滤器我的日子过的还挺舒坦，不过 MySQL 大哥就没我这么舒坦了，有时候遇到些烦人的请求，查询的数据不存在，MySQL 就要白忙活一场！不仅如此，因为不存在，我也没法缓存啊，导致同样的请求来了每次都要去让 MySQL 白忙活一场。我作为缓存的价值就没得到体现啦！这就是人们常说的缓存穿透。 这一来二去，MySQL 大哥忍不住了：“唉，兄弟，能不能帮忙想个办法，把那些明知道不会有结果的查询请求给我挡一下” 这时我想到了我的另外一个好朋友：布隆过滤器 我这位朋友别的本事没有，就擅长从超大的数据集中快速告诉你查找的数据存不存在（悄悄告诉你，我的这位朋友有一点不靠谱，它告诉你存在的话不能全信，其实有可能是不存在的，不过他要是告诉你不存在的话，那就一定不存在）。 如果你对我这位朋友感兴趣的话，可以看看这里《白话布隆过滤器 BloomFilter》。 我把这位朋友介绍给了应用程序，不存在的数据就不必去叨扰 MySQL 了，轻松帮忙解决了缓存穿透的问题。 缓存击穿 &amp;&amp; 缓存雪崩这之后过了一段时间太平日子，直到那一天 ··· 有一次，MySQL 那家伙正优哉游哉的摸鱼，突然一大堆请求给他怼了过去，给他打了一个措手不及。 一阵忙活之后，MySQL 怒气冲冲的找到了我，“兄弟，咋回事啊，怎么一下子来的这么猛” 我查看了日志，赶紧解释到：“大哥，实在不好意思，刚刚有一个热点数据到了过期时间，被我删掉了，不巧的是随后就有对这个数据的大量查询请求来了，我这里已经删了，所以请求都发到你那里来了” “你这干的叫啥事，下次注意点啊”，MySQL 大哥一脸不高兴的离开了。 这一件小事我也没怎么放在心上，随后就抛之脑后了，却没曾想几天之后竟捅了更大的篓子。 那一天，又出现了大量的网络请求发到了 MySQL 那边，比上一次的规模大得多，MySQL 大哥一会儿功夫就给干趴下了好几次！ 等了好半天这一波流量才算过去，MySQL 才缓过神来。 “老弟，这一次又是什么原因？”，MySQL 大哥累的没了力气。 “这一次比上一次更不巧，这一次是一大批数据几乎同时过了有效期，然后又发生了很多对这些数据的请求，所以比起上一次这规模更大了” MySQL 大哥听了眉头一皱，“那你倒是想个办法啊，三天两头折磨我，这谁顶得住啊？” “其实我也很无奈，这个时间也不是我设置的，要不我去找应用程序说说，让他把缓存过期时间设置的均匀一些？至少别让大量数据集体失效” “走，咱俩一起去” 后来，我俩去找应用程序商量了，不仅把键值的过期时间随机了一下，还设置了热点数据永不过期，这个问题缓解了不少。哦对了，我们还把这两次发生的问题分别取了个名字：缓存击穿和缓存雪崩。 我们终于又过上了舒适的日子 ···","categories":[{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/categories/nosql/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/tags/nosql/"},{"name":"Redis","slug":"redis","permalink":"https://topone233.github.io/tags/redis/"}]},{"title":"深入浅出 Maven","slug":"深入浅出 Maven","date":"2020-09-12T16:00:00.000Z","updated":"2023-05-09T08:04:05.226Z","comments":true,"path":"2020/09/13/深入浅出 Maven/","link":"","permalink":"https://topone233.github.io/2020/09/13/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%20Maven/","excerpt":"","text":"互联网开发，特别 Java 领域，可以说 Maven 随处可见。Maven 的仓库管理、依赖管理、继承和聚合等特性为项目构建提供了一整套完善的解决方案，可以说如果你搞不懂 Maven，那么一个多模块的项目足以让你头疼，依赖冲突就会让你不知所措，甚至搞不清楚项目是如何运行起来的….. 是的，我也曾经被 Maven 这样 “伤害” 过…… Thinking in Maven 回想一下，当你新到一家公司，安装完 JDK 后就会安装配置 Maven（MAVEN_HOME、path），很大可能性你需要修改 settings.xml 文件，比如你会修改本地仓库地址路径，比如你很可能会 copy 一段配置到你的 settings.xml 中（很可能就是私服的一些配置）。接下来，你会到 IDEA 或者 Eclipse 中进行 Maven 插件配置，然后你就可以在工程中的 pom.xml 里面开始添加 标签来管理 jar 包，在 Maven 规范的目录结构下进行编写代码，最后你会通过插件的方式来进行测试、打包（jar or war）、部署、运行。 上面描述了我们对 Maven 的一些使用方式，下面我们进行一些思考： Q1：本地仓库？Maven 到底有哪些仓库？它们什么关系？ 本地仓库路径配置 你要 jar 包，不可能每次都要联网去下载吧，多费劲，所以本地仓库就是相当于加了一层 jar 包缓存，先到这里来查。如果这里查不到，那么就去私服上找，如果私服也找不到，那么去中央仓库去找，找到 jar 后，会把 jar 的信息同步到私服和本地仓库中。 私服，就是公司内部局域网的一台服务器而已，你想一下，当你的工程 Project-A 依赖别人的 Project-B 的接口，怎么做呢？没有 Maven 的时候，当然是 copy Project-B jar 到你的本地 lib 中引入，那么 Maven 的方式，很显然需要其他人把 Project-B deploy 到私服仓库中供你使用。因此私服中存储了本公司的内部专用的 jar！不仅如此，私服还充当了中央仓库的镜像，说白了就是一个代理！ 中央仓库：该仓库存储了互联网上的 jar，由 Maven 团队来维护，比如地址是类似这样的：https://repo1.maven.org/XXXX。 Q2：关于 的使用 依赖管理 其实这个标签揭示了 jar 的查找坐标：groupId、artifactId、version。 一般而言，我们可以到私服上输入 artifactId 进行搜索，或者到 http://search.maven.org/、http://mvnrepository.com / 上进行查找确定坐标。 version 分为开发版本（Snapshot）和发布版本（Release），那么为什么要分呢？ 在实际开发中，我们经常遇到这样的场景，比如 A 服务依赖于 B 服务，A 和 B 同时开发，B 在开发中发现了 BUG，修改后，将版本由 1.0 升级为 2.0，那么 A 必须也跟着在 POM.XML 中进行版本升级。过了几天后，B 又发现了问题，进行修改后升级版本发布，然后通知 A 进行升级… 可以说这是开发过程中的版本不稳定导致了这样的问题。 Maven，已经替我们想好了解决方案，就是使用 Snapshot 版本，在开发过程中 B 发布的版本标志为 Snapshot 版本，A 进行依赖的时候选择 Snapshot 版本，那么每次 B 发布的话，会在私服仓库中，形成带有时间戳的 Snapshot 版本，而 A 构建的时候会自动下载 B 最新时间戳的 Snapshot 版本！ Q3：既然 Maven 进行了依赖管理，为什么还会出现依赖冲突？处理依赖冲突的手段是？ 依赖的版本？ 首先来说，对于 Maven 而言，同一个 groupId 同一个 artifactId 下，只能使用一个 version！ 根据上图的依赖顺序，将使用 1.2 版本的 jar。 现在，我们可以思考下，比如工程中需要引入 A、B，而 A 依赖 1.0 版本的 C，B 依赖 2.0 版本的 C，那么问题来了，C 使用的版本将由引入 A、B 的顺序而定？这显然不靠谱！如果 A 的依赖写在 B 的依赖后面，将意味着最后引入的是 1.0 版本的 C，很可能在运行阶段出现类（ClassNotFoundException）、方法（NoSuchMethodError）找不到的错误（因为 B 使用的是高版本的 C）！ 这里其实涉及到了 2 个概念：依赖传递（transitive）、Maven 的最近依赖策略。 依赖传递：如果 A 依赖 B，B 依赖 C，那么引入 A，意味着 B 和 C 都会被引入。 Maven 的最近依赖策略：如果一个项目依赖相同的 groupId、artifactId 的多个版本，那么在依赖树（mvn dependency:tree）中离项目最近的那个版本将会被使用。（从这里可以看出 Maven 是不是有点小问题呢？能不能选择高版本的进行依赖么？据了解，Gradle 就是 version + 策略） 现在，我们可以想想如何处理依赖冲突呢？ 想法 1：要使用哪个版本，我们是清楚的，那么能不能不管如何依赖传递，都可以进行版本锁定呢？ 使用 [这种主要用于子模块的版本一致性中] 想法 2：在依赖传递中，能不能去掉我们不想依赖的？ 使用 [在实际中我们可以在 IDEA 中直接利用插件帮助我们生成] 想法 3：既然是最近依赖策略，那么我们就直接使用显式依赖指定版本，那不就是最靠近项目的么？ 使用 Q4：引入依赖的最佳实践，提前发现问题！在工程中，我们避免不了需要加一些依赖，也许加了依赖后运行时才发现存在依赖冲突在去解决，似乎有点晚！那么能不能提前发现问题呢？ 如果我们新加入一个依赖的话，那么先通过mvn dependency:tree命令形成依赖树，看看我们新加入的依赖，是否存在传递依赖，传递依赖中是否和依赖树中的版本存在冲突，如果存在多个版本冲突，利用上文的方式进行解决！ Q5：Maven 规范化目录结构 简单 Java 工程目录结构 这里需要注意 2 点： src/main 下内容最终会打包到 Jar/War 中，而 src/test 下是测试内容，并不会打包进去。 src/main/resources 中的资源文件会copy至目标目录，这是 Maven 的默认生命周期中的一个规定动作。（想一想，hibernate/mybatis 的映射 XML 需要放入 resources 下，而不能在放在其他地方了） Q6：Maven 的生命周期 Maven 生命周期 我们只需要注意一点：执行后面的命令时，前面的命令自动得到执行。 实际上，我们最常用的就是这么几个： clean：有问题，多清理！ package：打成 Jar or War 包，会自动进行 clean+compile install：将本地工程 Jar 上传到本地仓库 deploy：上传到私服 Q7：**关于 scope 依赖范围**Maven 的生命周期存在编译、测试、运行这些过程。 有些依赖只用于测试，比如 junit； 有些依赖编译用不到，只有运行的时候才能用到，比如 mysql 的驱动包在编译期就用不到（编译期用的是 JDBC 接口），而是在运行时用到的； 还有些依赖，编译期要用到，而运行期不需要提供，因为有些容器已经提供了，比如 servlet-api 在 tomcat 中已经提供了，我们只需要的是编译期提供而已。 总结来说： compile：默认的 scope，运行期有效，需要打入包中。 provided：编译期有效，运行期不需要提供，不会打入包中。 runtime：编译不需要，在运行期有效，需要导入包中。（接口与实现分离） test：测试需要，不会打入包中。 system：非本地仓库引入、存在系统的某个路径下的 jar。（一般不使用） 每天进步一点点 慢一点才能更快","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"Maven","slug":"maven","permalink":"https://topone233.github.io/tags/maven/"}]},{"title":"SSL协议","slug":"SSL协议","date":"2020-09-11T08:05:13.045Z","updated":"2023-02-06T11:50:57.191Z","comments":true,"path":"2020/09/11/SSL协议/","link":"","permalink":"https://topone233.github.io/2020/09/11/SSL%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"1.基本概念安全套接层（secure sockets layer，SSL）协议是Netscape公司1994年提出的用于Web应用的传输层安全协议。 SSL协议使用非对称加密体制和数字证书技术，保护信息传输的秘密性和完整性。 2.特点2.1 主要应用于HTTP协议SSL协议尽管可以用于HTTP、FTP、TELNET等协议，但是目前主要应用于HTTP协议，为基于Web服务的各种网络应用中客户和服务器之间的用户身份认证与安全数据传输提供服务。 2.2 加密的安全通道SSL协议处于应用层和传输层之间，在TCP协议之上建立一个加密的安全通道，为TCP协议之间传输的数据提供安全保障。 2.3 加密与解密当HTTP协议使用SSL协议时，HTTP的请求、应答报文格式、处理方法不变。不同之处是：应用进程所产生的报文将通过SSL协议加密之后，再通过TCP连接传送出去。接收端TCP协议将加密的报文传送给SSL协议解密之后，再传送到应用层HTTP协议 2.4 不同的使用形式当Web系统采用SSL协议时，Web服务器的默认端口号从80 变换成 443；Web客户端使用HTTPS 取代HTTP。 2.5 握手协议、记录协议SSL协议包含两个协议：SSL握手协议（SSL Handshake Protocol）与 SSL记录协议（SSL Record Protocol）。SSL握手协议实现双方加密算法的协商与密钥传递，SSL记录协议定义SSL数据传输格式，实现对数据的加密与解密操作。","categories":[{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"SSL","slug":"ssl","permalink":"https://topone233.github.io/tags/ssl/"},{"name":"安全","slug":"安全","permalink":"https://topone233.github.io/tags/%E5%AE%89%E5%85%A8/"}]},{"title":"TCP协议","slug":"TCP协议","date":"2020-09-10T14:31:50.471Z","updated":"2023-02-06T11:51:07.755Z","comments":true,"path":"2020/09/10/TCP协议/","link":"","permalink":"https://topone233.github.io/2020/09/10/TCP%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"1.特点1.1 支持面向连接的传输服务如果将UDP协议提供的服务比作一封邮件的话，那么TCP协议所能提供的服务相当于语音聊天，必须要通信双方建立连接。 面向连接对提高系统数据传输的可靠性是很重要的，使用TCP传输数据之前，必须在源进程端口与目的进程端口之间建立一条TCP传输连接。用双方端口号来标识。TCP建立在不可靠的网络层 IP 协议上，IP 协议不能提供任何可靠性保障机制，因此TCP 协议的可靠性需要自己解决。 1.2 支持字节流的传输流（stream）相当于是一个管道（水管），从一端放入什么，另一端可以照原样取出。描述了一个不出现 ：丢失、重复、乱序的数据传输过程。TCP 协议将数据看成是一连串的、无结构的字节流。 如果用户是通过键盘输入数据，应用进程将字符逐个提交给发送端。如果数据是从文件得到，那么数据可能是逐行或逐块交付发送端。 为了支持字节流传输，发送端和接收端都需要使用缓存。发送端将几个写操作组合成一个报文的，提交给 IP 协议，由 IP 协议封装成 IP 分组之后传输到接收端， 1.3 支持全双工通信由于通信双方都设置有发送和接收的缓冲区，TCP 协议允许通信双方在任何时候都可以发送数据。 1.4 支持同时建立多个并发的TCP连接TCP 协议需要支持同时建立多个连接，这个特点在服务端表现的更为突出。一个服务器必须同时处理多个客户端的访问。例如，一个Web服务器的套接字为“ 141.8.22.51:80 ”，同时有三个客户端需要访问这个服务器，它们的套接字分别为“ 202.1.12.5:30001 “ “ 242.1.12.5:300022 “ “ 212.1.12.5:300023 “则服务器端需要同时建立三个 TCP 连接。 也支持一个客户端与多个服务器同时建立多个 TCP 连接。 1.5 支持可靠的传输服务TCP 协议使用确认机制检查数据是否安全和完整，并且提供拥塞控制功能。对发送和接收的数据进行跟踪、确认和重传。 但是 TCP 协议是建立在不可靠的网络层 IP 协议之上，一旦 IP 协议及以下层出现传输错误，TCP 协议只能不断进行重传，可靠性会受到底层限制。 2.报文格式窗口 窗口字段长度为16位，表示以字节（B）为单位的窗口大小。 由于接收端的接收缓冲区是受到限制的，因此需要设置一个窗口字段，表示下一次传输接收端还有多大的接收容量。窗口字段值是准备接收下一个 TCP报文段的接收端，通知即将发送报文段的发送端，下一次最多可以发送报文段的字节数。 发送端将根据接收端通知的窗口值调整自己的发送窗口值大小。 窗口字段值是动态变化的。 主机A 发给主机 B的TCP报头中确认号是 502，窗口值 1000。表示：下一次主机B要向主机A发送的 TCP，字段第一字节号应该是 502，字段最大长度是 1000，最后一个字节号最大是 1501。 序号 序号字段长度位32位 TCP 是面向字节流的，它需要为发送字节流中的每个字节都按顺序编号。 TCP连接建立时，每方都需要使用随机数产生器，生成一个初始序号。 不能为0。避免因TCP连接非正常断开而可能引起的混乱。如果在连接突然中断时，可能有一个或两个进程同时等待对方的确认应答，而这个时候有一个新连接的序号也是从0开始，那么接收进程就有可能认为是对方重传的报文，这样就可能造成连接过程的错误。 ACK（确认位） 在TCP连接建立后发送的所有报文段的ACK位都要置 1。 SYN（同步位） 在连接建立时用来同步序号。例如 SYN=1，ACK=0时，表示这是一个连接建立请求报文，同意建立连接的响应报文：SYN=1，ACK=1。 3. 连接建立与释放3.1 三次握手3.1.1 最初，客户端TCP 进程是处于 CLOSE 状态。准备发起TCP连接时，进入 SYN-SEND 状态，向处于LISTEN 状态的服务器端 TCP 进程发送第一个控制位 SYN = 1 的”连接建立请求报文“，不携带数据字段，但需要给报文一个序号 seq=x。 SYN=1 seq=x 3.1.2 服务器端接收到” 连接建立请求报文“之后，如果同意建立连接，则向客户端发送第二个控制位 SYN=1， ACK=1 的 ” 连接建立请求确认报文 “。确认号 ack = x + 1，表示是对第一个” 连接建立请求报文“（序号 seq=x）的确认。同样不携带任何数据字段，但是需要给报文一个序号 seq=y。这时服务器进入 SYN-RCVD（准备接收）状态。 SYN=1 ACK=1 seq=y ack=x+1 3.1.3 客户端发送第三个控制位 ACK=1 ” 连接建立请求确认报文 “。由于该报文是对” 连接建立请求确认报文”（seq=y）的确认，因此确认序号 ack=y+1。同样不携带数据字段，但需要给一个序号，仍为 x+1 。ACK=1 seq=x+1 ack =y+1。这时客户端进入 ESTABLISHED（已建立连接）状态。服务器端在接收到ACK报文之后也进入 ESTABLISHED （已建立连接）状态。 三次握手完成。TCP连接建立。 3.1.4 为什么需要第三次握手？主要是防止已经失效的连接请求报文段突然又传回到服务端而产生错误： 客户端发出的第一个连接请求报文段， 但是由于某些原因在某个网络节点滞留了很长时间，客户端一直等不到确认报文，于是客户端再次发出一次新的连接请求，并成功收到服务端确认，建立了连接。 之前的请求报文段并没有丢失，延误到客户端与服务端连接释放后才到达服务端，本来这个请求已经失效了，但是服务端收到此请求报文段后，误以为是客户端发出的新的请求连接，于是服务端又向客户端发出确认报文段。 假如不采用三次握手，那么只要服务端发出确认，连接就建立了。 但是客户端并没有发出连接建立的请求，因此不会理会服务端的确认，也不会向服务端发出数据。 而服务端以为却一直在等待客户端发来数据，这样服务端的许多资源就白白浪费了 常用三次握手可以防止上诉现象发生。客户端不向服务端发出确认请求，服务端就不会建立连接。 3.2 报文传输TCP传输连接建立之后，双方可以使用这个连接，进行全双工的字节流传输。 为了保证TCP工作正常、有序的进行，TCP在服务器端设置了保持计时器（keep timer），用来防止TCP连接处于长时间空闲。当服务器端收到客户端的报文时，就保持计时器复位。如果没有收到客户端的信息，他就发送探测报文。如果发送10个探测报文（每个相隔 75s）还没有响应，就假设客户端出现故障，终止该连接。 3.3 四次挥手3.3.1 客户端主动提出释放TCP连接，进入 FIN-WAIT-1（释放等待-1）状态。向服务器发送第一个控制位FIN=1，的 ” 连接释放请求报文 ”，提出连接释放请求，停止发送数据。不携带任何数据字段。但是需要给报文一个序号。 FIN=1，seq=u 。u等于客户端发送的最后一个字节的序号加1。 3.3.2 服务器端收到之后，需要向客户发送 “ 连接释放请求确认报文 ”，表示对报文的确认， ack=u+1。这个 “ 连接释放请求报文 ” 的序号 v 等于服务器发送的最后一个字节序号加1。 ACK=1，seq=v，ack=u+1。 TCP服务器进程向高层应用进程通知客户请求释放TCP连接，客户到服务器的TCP连接断开，但是服务器到客户的TCP连接还没有断开，如果服务器还有数据报文需要发送时，它还可以继续发送直至完毕。这种状态称为半关闭（helf-close）状态。这个状态需要持续一段时间。 客户在接收到服务器发送的ACK报文之后进入 FIN-WAIT-2状态；服务器进入CLOSE-WAIT状态。 3.3.3 服务器的高层应用程序已经没有数据需要发送时，它会通知TCP可以释放连接，这时服务器向客户发送 “ 连接释放请求报文 ”。报文的序号（假定为w），取决于在半关闭状态时，服务器端是否发送过数据报文。服务器端经过 LAST-ACK状态之后转回到LISTEN（收听）状态。 ACK=1，FIN=1，seq=w，ack=u+1。 3.3.4 客户接收到FIN报文之后，向服务器发送 “ 连接释放请求确认报文 ”，ACK=1，seq=u+1，ack=w+1 3.3.5 为什么TCP的连接的建立只需要三次握手，而连接的释放需要四次呢？因为服务端在LISTEN状态下，收到建立请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。 而连接关闭时，当收到对方的FIN报文时，仅仅表示客户端没有需要发送的数据了，但是还能接收数据。服务端上层的应用程序未必数据已经全部发送给对方了，此时服务端可以立即关闭，也可以将应该发送的数据全部发送完毕后，再发送FIN报文给客户端来表示同意现在关闭连接。 从这个角度而言，服务端的ACK和FIN一般都会分开发送。从而导致多了一次。 3.4 时间等待计时器为了保证TCP连接释放过程正常的进行，TCP设置了时间等待计时器（TIME-WAIT Timer）。当TCP关闭一个连接时，它并不认为这个连接马上就真正的关闭。这时，客户端进入TIME-WAIT状态，需要再等待两个最长报文寿命（maximum segment lifetime，MSL）时间之后，才真正进入CLOSE（关闭）状态。 四次挥手之后，确认双方已经同意释放连接，客户端仍需要采取延迟2MSL时间，确保服务器在最后阶段发送给客户端的数据，以及客户端发送给服务器的最后一个ACK报文都能正确的被接收，防止因个别报文传输错误导致连接释放失败。 4.滑动窗口与确认、重传机制4.1 滑动窗口TCP协议使用以字节为单位的滑动窗口协议（Sliding-Windows Protocol），来控制字节流的发送、接收、确认、重传过程。 TCP使用两个缓存和一个窗口来控制字节流的传输过程。 发送端的TCP有一个缓存，用来存储应用进程准备发送的数据。对这个缓存设置一个发送窗口。 接收端也有一个缓存，将正确接收到字节流写入缓存，等待应用进程读取。也有一个接收窗口。 只要发送窗口值不为0就可以发送报文段。发送窗口的大小取决于接收窗口的大小。发送端每一次能够连续发送字节数取决于发送窗口的大小。不能超过接收窗口值，发送端可以根据自身的需要来决定。 接收窗口值等于接收缓存还可以继续接收的字节流的大小。由接收端根据接收缓存剩余空间的大小，以及应用进程读取数据的速度决定。 接收端通过TCP报头通知发送端，已经正确接收的字节号，以及发送端还能够连续发送的字节数。 虽然TCP协议是面向字节流的，但是它不可能每次传送一个字节，就对这个字节进行确认。它是将字节流分成段，一个段的多个字节打包成一个TCP报文段一起传送、一起确认。TCP协议通过报头的“序号”来标识发送的字节，用“确认号”表示哪些字节已经被正确的接收。 4.1.2 字节流传输状态为了达到利用滑动窗口协议控制差错的目的，TCP协议引入了“字节流传输状态”的概念。为了对正确传输的字节流进行确认，必须对字节流的传输状态进行跟踪： 假设发送的第一个字节的序号是1。 第一类：已经发送，且已得到确认的字节。假设序号为19之前的字节已经被接收端正确的接收，并且发送端发送了确认信息。1-19的字节属于第一类。 第二类：已经发送，但未收到确认的字节。 第三类：尚未发送，但是接收端表示接收缓冲区已经准备好，如果发送端准备好就可以立即发送这些字节。 第四类：尚未发送，且接收端也未做好接收准备的字节。 4.1.3 发送窗口与可用窗口发送端每一次能够连续发送字节数取决于发送窗口的大小。 发送窗口的长度等于第二类与第三类字节数之和。 可用窗口长度等于第三类字节数。如果没有任何问题，发送端可以立即发送可用窗口的字节。 接收端确认发送窗口的字节，为保持发送窗口值不变，需要将窗口向左滑动（例如从序号1 移动到 20）。 4.2 容错控制TCP协议通过滑动窗口机制来跟踪和记录发送字节的状态，实现差错控制功能。 TCP协议的设计思想是让应用进程将数据作为一个字节流传送，而不是限制应用层数据的长度。应用进程不需要考虑发送数据的长度，由TCP协议来负责将这些字节分段打包。 发送端利用已建立的TCP连接，将字节流传送到接收端的应用进程，并且是顺序的，没有差错、丢失、重复的。 TCP协议发送的报文是交给 IP 协议传输的，IP协议只能提供尽力而为的服务，IP分组在传输过程中出错是不可避免的，TCP协议必须提供差错控制、确认、重传功能，以保证接收的字节流是正确的。 4.3 选择重传策略上面我们没有考虑到报文段丢失的情况，但是在Internet中，报文段丢失是不可避免的，会造成接收的字节流序号不连续的现象。 接收字节流序号不连续的处理方法有两种：拉回、选择重传 4.3.1 拉回在丢失第二个报文段时，不管之后的报文段接收是否正确，都要求从第二个报文段开始，重传后面的所有的报文段。显然，拉回方式发效率是很低的。 4.3.2 选择重传选择重传（selective ACK，SACK），如果收到字节流序号不连续时，如果这些字节的序号都在接收窗口之内，则首先完成接收窗口内字节的接收，然后将丢失的字节序号通知发送端，发送端只需要重传丢失的报文段，而不需要重传已经接收的报文段。 4.4 重传计时器TCP使用重传计时器（retransmission timer）来控制报文确认与等待重传的时间。当发送端发送一个报文时，首先将它的一个报文的副本放入重传队列，同时启动一个重传计时器。重传计时器设定一个值，例如400ms，然后开始倒计时。时间结束前收到确认，表示传输成功，否则说明传输失败，准备重传该报文。 4.4.1 影响超时重传的元素设定重传计时器的时间值时很重要的。如果设定值过低，可能出现已被接收端正确接收的报文被重传，造成接收报文重复的现象。如果设定值过高，造成一个报文已经丢失，而发送端长时间等待，造成效率降低的现象。 如果一个主机同时与其他两个主机建立两条TCP连接，那么它就需要分别为每个连接启动一个重传计时器。如果一个用于本地局域网中传输文本文件，另一个用于Internet远程访问Web服务，那么两个TCP报文的往返时间相差很大。因此，需要对不同的TCP连接设定不同的重传计时器的时间。 由于Internet在不同时间段的用户数量变化很大，流量与传输延迟变化也很大，报文传输延迟也不会相同。 正是由于这些原因，为TCP连接确定合适的重传定时器数值是很可能的。TCP不会采用简单的静态方法，必须采用动态的自适应的方法。根据端对端报文往返时间的连续测量，不断调整和设定重传定时器的超时重传时间。 4.5超时重传时间的选择略。。。（写不动了，下次再补吧） 5.滑动窗口与流量控制、拥塞控制5.1 TCP窗口与流量控制研究流量控制（flow control）算法的目的是控制发送端发送速率，使之不超过接收端的接收速率，防止由于接收端来不及接收送达的字节流，而出现报文段丢失的现象。滑动窗口协议可以利用TCP报头中窗口字段，方便的实现流量控制。","categories":[{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"TCP","slug":"tcp","permalink":"https://topone233.github.io/tags/tcp/"},{"name":"滑动窗口","slug":"滑动窗口","permalink":"https://topone233.github.io/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"}]},{"title":"JVM 初步理解","slug":"JVM 初步理解","date":"2020-09-04T02:25:57.056Z","updated":"2023-02-06T11:54:54.520Z","comments":true,"path":"2020/09/04/JVM 初步理解/","link":"","permalink":"https://topone233.github.io/2020/09/04/JVM%20%E5%88%9D%E6%AD%A5%E7%90%86%E8%A7%A3/","excerpt":"","text":"原文地址 [www.zhihu.com\\](https://www.zhihu.com/question/20097631/answer/817071740) 如果 JAVA 开发是降龙十八掌，JVM 就是九阳神功，具备九阳神功的内力基础降龙十八掌的威力会被发挥的淋漓尽致。 JVM 的内存结构和类加载机制是玩转 JVM 的入口，如果弄通了接下来的路该怎么走你自然就知道了 下面就从这两个方面来深入解析一下 JVM 1.JVM 内存结构jvm 内存分为五大块： 标灰的是线程公有的内存区域，没有标灰的是线程私有。 1.1 程序计数器程序计数器是用来指示当前线程要执行哪条指令，并且在执行完该条指令后让程序计数器指向下一条指令，直到将程序执行完毕。指令需要靠 cpu 来执行，在多线程中，多个线程是通过轮流切换分配 cpu 的时间片而执行的，在切换时需要记录当前执行到了哪条指令以便将来继续执行，每一个线程都需要有自己的程序计数器，所以程序计数器是线程私有的内存。 1.2 虚拟机栈通常我们把 jvm 的内存粗略的分为堆和栈，其中的栈指的就是虚拟机栈, 虚拟机栈也是线程私有的。 虚拟机栈对应的是方法的内存区域，每个方法执行时都会创建一个栈帧，用来存储该方法的局部变量表，操作数栈，动态链接，方法返回地址： 1.2.1 局部变量表局部变量表中存储的是方法的参数和方法中定义的局部变量，在编译期间就为局部变量表分配好了内存空间。局部变量表中存储三种类型的数据： （1） 基本数据类型 （2） 引用类型：指向一个对象在内存中的地址 （3） returnAddress 类型：指向指令的地址（已经很少见了，指向异常处理的指令，现在已经由异常表代替） 1.2.2 操作数栈当虚拟机执行一些指令的时候会对操作数栈进行入栈或出栈的操作，比如 iadd 指令将两个数相加，会先将操作数栈中的两个数弹出来（出栈），相加后再压入栈（入栈）中。 1.2.3 动态链接在运行时常量池中存储了诸如类名，方法名，我们要找到目标类，执行相应的方法就需要用到动态链接，栈帧中有一个指向运行时常量池的引用，通过这个引用可以找到相应的类名和方法名，但是光知道名称是没法执行方法的，需要通过名称找到相应的类和方法在内存中的地址，这个过程就是动态链接。 1.2.4 方法返回地址当方法执行完以后如果有返回值，就会把这个返回值返回给该方法的调用者，方法的返回就是我们 java 中用到的 return 命令。方法返回之后调用者需要继续往下执行就需要知道要执行的地址，该地址就是方法返回地址，它被记录在了栈帧中，当然在发生异常的情况下不会有返回值，要继续执行的地址可以通过异常处理器表来确定。 虚拟机栈可能出现两种类型的异常： 1. 线程请求的栈深度大于虚拟机允许的栈深度会抛出 StackOverflowError,（虚拟机栈空间不能动态扩展的情况下） 2. 如果虚拟机栈空间可以动态扩展（目前多数的虚拟机都可以），当动态扩展无法申请到足够的空间时会抛出 OutOfMemory 异常。 1.3 本地方法栈本地方法栈与虚拟机栈的作用是一样的，区别在于虚拟机栈为虚拟机执行 java 方法服务，而本地方法栈为虚拟机执行 native 方法服务，native 方法为本地方法，不是用 java 语言写的有可能是 c 或者 c++ 写的，在 jdk 中就有很多 c 的代码，就是提供给本地方法来调用的。 1.4 堆通常我们把 jvm 的内存粗略的分为堆和栈，其中的堆就是指它，它是虚拟机中占用内存最大的一块，是被所有线程共享的一块区域，它是用来存放对象实例的。是垃圾收集器管理的主要区域。 1.5 方法区方法区也是被所有线程共享的一块区域，它存储的是类信息，常量，静态变量，编译后的字节码等信息。方法区中还有一块区域 “运行时常量池 “：运行时常量池中存储的是编译期生成的各种字面量和符号引用。字面量相当于 Java 里常量的概念，比如字符串，声明为 final 的常量值等，符号引用包括了：类和接口名，字段名，方法名。 2.类加载机制2.1 java 类的加载过程编译后的 Java 类是以字节码的形式存在的，它只有被加载到虚拟机内存中才能被使用，它是如何被加载到内存中的呢？ 下图为类加载到内存的机制： 2.1.1 加载在加载（注意和类加载是不同的概念）阶段虚拟机需要完成三件事 （1）. 通过一个类的全限定名（类名全称，带包路径的用点隔开，例如: java.lang.String）来获取其定义的二进制字节流（被编译以后的字节码文件就是二进制的）。 （2）. 将这个字节流所代表的静态存储结构（字节码文件就是其中一种）转化为方法区的运行时数据结构（能够在虚拟机中存储的结构）。 （3）. 在 Java 堆中生成一个代表这个类的 java.lang.Class 对象（用于表示这个类的信息），作为对方法区中这些数据的访问入口。 2.1.2 验证验证，准备，解析统称为连接，作为连接阶段的第一步，验证的主要作用是保证加载进来的二进制流中的信息是符合当前虚拟机要求的，并且不会对虚拟机的安全造成危害。主要包括： （1）文件格式验证：主要是验证二进制流是否符合 class 文件格式的规范，并且能被当前的虚拟机处理，例如：主次版本号是否在当前虚拟机处理范围之内，常量池的常量中是否有不被支持的常量类型。只有通过了这个阶段的验证后字节流才会进入内存的方法区中进行存储，从这里我们可以看出加载和验证阶段是交叉进行的，加载还未完成，文件格式验证就已经开始了。 （2）元数据验证：对字节码描述的信息进行语义分析以保证其描述的信息符合 java 语言规范的要求：例如：这个类是否有父类（所有类除了 Object 都应该有父类） （3）字节码验证：确定程序语义是合法的符合逻辑的，如将子类对象赋给父类类型是符合逻辑的，反之，将父类对象赋给子类类型则是不合法的。 （4）符号引用验证：可以对常量池中各种符号引用的信息进行匹配性校验。例如：符号引用中通过字符串描述的全限定名是否能找到对应的类。 2.1.3 准备准备阶段会为静态变量分配内存并设置初始值，注意：该初始值为数据类型的零值例如： Public static int num = 3; 在准备阶段会将 num 值设置为 0 而不是 3. 只有在初始化阶段才会赋值为 3. 2.1.4 解析解析阶段是把类中的符号引用转换为直接引用的过程： 符号引用：在编译的时候是不知道类所引用的类的具体地址，因此只能使用符号引用来代替，比如：com.Student 类引用了 com.Grade 类，编译时 Student 类并不知道 Grade 类在内存中的实际地址，只能用符号 com.Grade。 直接引用; 引用的实际内存地址。 2.1.5 初始化初始化阶段是类加载过程的最后一步，在这个阶段会根据程序中的赋值语句给变量赋值，当有继承关系时先初始化父类，再初始化子类。如果该类还没有被加载和连接，那么初始化之前先加加载和连接。 什么时候会进行初始化呢？ 1. 使用 new 关键字实例化对象的时候 2. 读取或设置一个类的静态字段的时候 3. 调用一个类的静态方法的时候 4. 对类进行反射调用的时候 5. 当虚拟机启动时执行一个类的 main 方法，会先初始化这个类 2.2 类加载器加载阶段中的第一步：“通过一个类的全限定名来获取其定义的二进制字节流” 是通过类加载器来完成的，类加载器分为三种： 2.2.1 启动类加载器(BootStrap ClassLoader)这个类加载器负责将 jdk\\jre\\lib 下的类库加载到内存中，启动类加载器无法被应用程序直接使用。 2.2.2 扩展类加载器(Extension ClassLoader)它负责加载 jdk\\jre\\lib\\ext 中的类库。开发者可以直接使用扩展类加载器。 2.2.3 应用程序类加载器 (Application ClassLoader)它用来加载 classpath 路径（src 路径下的文件在编译后会放到 WEB-INF/classes 路径下。默认的 classpath 是在这里）指定的类。开发者可以直接使用这个类加载器，如果如果应用程序中没有定义自己的类加载器，一般情况下这个就是程序中默认的类加载器。 我们的应用程序都是由这三种类加载器互相配合进行加载的，如果有必要还可以加入自己定义的类加载器。 2.3 双亲委派模型下图为双亲委派模型的类加载器的层次关系： 双亲委派模型的的工作过程是：如果一个类加载器收到了类加载的请求，它会把这个请求委派给父类加载器去完成，每一个层次的加载器都是如此，因此所有的加载请求最终都会传送到顶层的启动类加载器中。只有当父加载器反馈自己无法完成这个加载请求时（在自己的加载范围内没有搜索到该类）, 子加载器才会尝试自己去加载。例如： 1. 当应用程序类加载器加载一个类时，它会把类加载请求委派给扩展类加载器。 2. 扩展类加载器又把这个类加载请求委派给启动类加载器。 3. 启动类加载器如果加载失败，在 (jdk/jre/lib) 里没有找到这个类，会使用扩展类加载器进行加载。 4. 扩展类加载器如果加载失败，在（jdk/jre/lib/ext）里没有找到这个类，会使用应用程序类加载器来加载 5. 应用程序加载器加载失败则会报：ClassNotFoundException 异常。 使用双亲委派模型的意义： 例如类 java.lang.Object 存放在 rt.jar 中，无论哪个类加载器要加载这个类，最终都会委派启动类加载器来加载，如果没有双亲委派模型用户自己编写了一个 java.lang.Object 类，放到 ClassPath 中，系统中将会出现多个不同的 Object 类，应用程序也会变得混乱。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"JVM","slug":"jvm","permalink":"https://topone233.github.io/tags/jvm/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"常用工具集","slug":"常用工具集","date":"2020-09-03T12:22:56.040Z","updated":"2023-02-06T11:43:39.399Z","comments":true,"path":"2020/09/03/常用工具集/","link":"","permalink":"https://topone233.github.io/2020/09/03/%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E9%9B%86/","excerpt":"","text":"1. Jetbrains系列产品无限重置试用 Jetbrains家的产品有一个很良心的地方，他会允许你试用30天（这个数字写死在代码里了）以评估是否你真的需要为它而付费. 有一款插件可以无限重置试用时间。但切记不要无休止的一直试用，这并不是这个插件的本意！ 如何安装插件市场搜索 IDE Eval Reset 安装即可. 项目地址 使用说明 2. IDEA 插件系列2.1 阿里 Java开发规约插件插件市场搜索 Alibaba Java Coding Guidelines 安装即可. 2.2 官方中文插件插件市场搜索 Chinese(Simplified)Language Pack EAP 安装即可. 2.3 JSON格式化插件市场搜索 Json Parser 安装即可. 2.4 快捷键提示插件市场搜索 Key Promoter X 安装即可. 3. Markdown写作工具3.1 写作工具：Typora 下载地址（官网）：Typora 自用：0.9.89版本官网安装包：蓝奏云 3.2 图片上传工具：Picgo搭配Typora，效率神器 下载地址（官网）：Picgo 自用：2.2.2版本官网安装包: 蓝奏云 3.3 图床：去不图床配置说明：https://dusays.com/241/ Typora + Picgo + 去不图床 设置： Typora： Ctrl + 逗号 进入偏好设置 -&gt; 图像 插入图片时 -&gt; 上传图片 -&gt; 勾选前两项 上传服务设定 -&gt; PicGo（app）-&gt; PicGo本地安装路径 打开Picgo -&gt; 打开配置文件 -&gt; 修改server端口号为：36677 上传区修改图床 重启Picgo 回到第三步Typora页面 -&gt; 验证图片上传路径 -&gt; 验证成功 4. Chrome 插件4.1 网页代理(免费) 下载地址：Hoxx VPN Proxy 用了两年的网页代理插件（免费节点推荐选择 印度） 速度上，如果只是加速GitHub、访问Twitter、Youtube、谷歌市场等 足够用的了。 4.2 广告拦截 下载地址：uBlock Origin 各大视频网站的广告基本都能拦截，从大一开始用，效果不用多说，还你一个干净清爽的上网环境。 4.3 新标签页 下载地址：Infinity 新标签页 （很好看，每天自动同步Bing的壁纸） 4.4 翻译 下载地址：Saladict 沙拉查词 聚合了各大翻译源，支持网页翻译、划词翻译、搜索翻译等，超级方便、好看、好用，强推！ 4.5 阅读 下载地址：简阅 如杂志般沉浸式阅读体验，提取文章主体，收藏，md格式下载等功能，超好用（贫穷如我 依然支持了一波 开通了终身会员）。 4.6 Tab标签页管理 下载地址： OneTab 对于我来说，我的浏览器日常开几十个标签页（遇到一个问题，往往需要看多个文章相互补充，才能解决我所遇到的问题。另一种情况就是……拖延症，没错，滚进我的收藏夹吃灰，每次看到好的或者需要的文章，总是会舍不得关闭 晚点就有时间把它看完，嗯。。。一晚就是永远。。。），开几十个标签页带来的问题：一是实在太多了，chrome容不下了，对 容不下了。。。二是chrome的每一个标签页都是一个独立的进程，我8G的内存，此时风扇已经开始有噪音了，如果是在家还好，可是办公室里可太尬尴了。 标签页管理的插件完美的解决了我的问题(据说可以节省高达95％的内存！)。 4.7 MEGA云盘 下载地址：MEGA 虽然是境外的网盘，但是搭配上第一条推荐的代理插件，下载速度丝毫不逊色某度云网盘 doge 4.8 RSS订阅 下载地址: RSS Feed Reader 很喜欢去发现一些个人博客，他们往往风格各异，内容丰富多彩 涉及各个领域。如果对博主的内容大多都很感兴趣，那么这时候就需要一个订阅功能 以实时查看其最新内容，那么你需要一个RSS订阅插件。 4.9 GitLab Tree 下载地址：SpanTree - GitLab Tree 此插件可以实现像IDE那样目录展示，很方便，不用再一层一层点开查看。公司用的GitLab私服也可以用。 4.10 Tampermonkey 油猴 下载地址：Tampermonkey 油猴不必多说，久负盛名。不过其本质还是个容器、载体。这里主要推荐下一些有趣的插件（当然 你也可以设计自己的插件工具）。 BiliBIli 港澳台 智慧树网课助手（包含考试） 超星网课助手（包含考试） 百度文库免费下载 …… 还有很多有趣的 自行挖掘 5. 常用网站5.1 超赞的壁纸网站​ 如果要安利一个壁纸网站的话，那么不管你问我多少遍，我都强烈推荐 Wallhaven.cc 5.2 音乐聚合​ 聚合QQ音乐、网易云音乐、咪咕音乐（主要用来听周董的），可以登录获取歌单、支持MP3下载、界面美观。 ​ 地址：http://music.jsososo.com/#/ 6. Windows工具[TOC]","categories":[{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/categories/%E8%B5%84%E6%BA%90/"}],"tags":[{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/tags/%E8%B5%84%E6%BA%90/"}]},{"title":"SQL Or NoSQL","slug":"SQL Or NoSQL","date":"2020-09-03T11:43:12.624Z","updated":"2023-02-06T11:50:45.360Z","comments":true,"path":"2020/09/03/SQL Or NoSQL/","link":"","permalink":"https://topone233.github.io/2020/09/03/SQL%20Or%20NoSQL/","excerpt":"","text":"原文地址:https://www.cnblogs.com/xrq730/p/11039384.html 前言你是否在为系统的数据库来一波大流量就几乎打满 CPU，日常 CPU 居高不下烦恼？你是否在各种 NoSql 间纠结不定，到底该选用那种最好？今天的你就是昨天的我，这也是写这篇文章的初衷。 这篇文章是我好几个月来一直想写的一篇文章，也是一直想学习的一个内容，作为互联网从业人员，我们要知道关系型数据库（MySql、Oracle）无法满足我们对存储的所有要求，因此对底层存储的选型，对每种存储引擎的理解非常重要。同时也由于过去一段时间的工作经历，对这块有了一些更多的思考，想通过自己的总结把这块写出来分享给大家。 结构化数据、非结构化数据与半结构化数据文章的开始，聊一下结构化数据、非结构化数据与半结构化数据，因为数据特点的不同，将在技术上直接影响存储引擎的选型。 结构化数据根据定义结构化数据指的是由二维表结构来逻辑表达和实现的数据，严格遵循数据格式与长度规范，也称作为行数据，特点为：数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的。例如： 因此关系型数据库完美契合结构化数据的特点，关系型数据库也是关系型数据最主要的存储与管理引擎。 非结构化数据指的是数据结构不规则或不完整，没有任何预定义的数据模型，不方便用二维逻辑表来表现的数据，例如办公文档（Word）、文本、图片、HTML、各类报表、视频音频等。 半结构化数据介于结构化与非结构化数据之间的数据就是半结构化数据了，它是结构化数据的一种形式，虽然不符合二维逻辑这种数据模型结构，但是包含相关标记，用来分割语义元素以及对记录和字段进行分层。常见的半结构化数据有 XML 和 JSON，例如： &lt;person&gt; &lt;name&gt;张三&lt;/name&gt; &lt;age&gt;18&lt;/age&gt; &lt;phone&gt;12345&lt;/phone&gt; &lt;/person&gt;这种结构也被成为自描述的结构。 以关系型数据库的方式做存储的架构演进首先，我们看一下使用关系型数据库的方式，企业一个系统发展的几个阶段的架构演进（由于本文写的是 Sql 与 NoSql，因此只以存储方式作为切入点，不会涉及类似 MQ、ZK 这些中间件内容）： 阶段一：企业刚发展的阶段，最简单，一个应用服务器配一个关系型数据库，每次读写数据库。 阶段二：无论是使用 MySQL 还是 Oracle 还是别的关系型数据库，数据库通常不会先成为性能瓶颈，通常随着企业规模的扩大，一台应用服务器扛不住上游过来的流量且一台应用服务器会产生单点故障的问题，因此加应用服务器并且在流量入口使用 Nginx 做一层负载均衡，保证把流量均匀打到应用服务器上。 阶段三：随着企业规模的继续扩大，此时由于读写都在同一个数据库上，数据库性能出现一定的瓶颈，此时简单地做一层读写分离，每次写主库，读备库，主备库之间通过 binlog 同步数据，就能很大程度上解决这个阶段的数据库性能问题 阶段四：企业发展越来越好了，业务越来越大了，做了读写分离数据库压力还是越来越大，这时候怎么办呢，一台数据库扛不住，那我们就分几台吧，做分库分表，对表做垂直拆分，对库做水平拆分。以扩数据库为例，扩出两台数据库，以一定的单号（例如交易单号），以一定的规则（例如取模），交易单号对 2 取模为 0 的丢到数据库 1 去，交易单号对 2 取模为 1 的丢到数据库 2 去，通过这样的方式将写数据库的流量均分到两台数据库上。一般分库分表会使用 Shard 的方式，通过一个中间件，便于连接管理、数据监控且客户端无需感知数据库 ip 关系型数据库的优点上面的方式，看似可以解决问题（实际上确实也能解决很多问题），正常对关系型数据库做一下读写分离 + 分库分表，支撑个 1W + 的读写 QPS 还是问题不大的。但是受限于关系型数据库本身，这套架构方案依然有着明显的不足，下面对利用关系型数据库方式做存储的方案的优点先进行一下分析，后一部分再分析一下缺点，对某个技术的优缺点的充分理解是技术选型的前提。 易理解 因为行 + 列的二维表逻辑是非常贴近逻辑世界的一个概念，关系模型相对网状、层次等其他模型更加容易被理解 操作方便 通用的 SQL 语言使得操作关系型数据库非常方便，支持 join 等复杂查询，Sql + 二维关系是关系型数据库最无可比拟的优点，这种易用性非常贴近开发者 数据一致性 支持 ACID 特性，可以维护数据之间的一致性，这是使用数据库非常重要的一个理由之一，例如同银行转账，张三转给李四 100 元钱，张三扣 100 元，李四加 100 元，而且必须同时成功或者同时失败，否则就会造成用户的资损 数据稳定 数据持久化到磁盘，没有丢失数据风险，支持海量数据存储 服务稳定 最常用的关系型数据库产品 MySql、Oracle 服务器性能卓越，服务稳定，通常很少出现宕机异常 关系型数据库的缺点紧接着的，我们看一下关系型数据库的缺点，也是比较明显的。 高并发下 IO 压力大 数据按行存储，即使只针对其中某一列进行运算，也会将整行数据从存储设备中读入内存，导致 IO 较高 为维护索引付出的代价大 为了提供丰富的查询能力，通常热点表都会有多个二级索引，一旦有了二级索引，数据的新增必然伴随着所有二级索引的新增，数据的更新也必然伴随着所有二级索引的更新，这不可避免地降低了关系型数据库的读写能力，且索引越多读写能力越差。有机会的话可以看一下自己公司的数据库，除了数据文件不可避免地占空间外，索引占的空间其实也并不少 为维护数据一致性付出的代价大 数据一致性是关系型数据库的核心，但是同样为了维护数据一致性的代价也是非常大的。我们都知道 SQL 标准为事务定义了不同的隔离级别，从低到高依次是读未提交、读已提交、可重复度、串行化，事务隔离级别越低，可能出现的并发异常越多，但是通常而言能提供的并发能力越强。那么为了保证事务一致性，数据库就需要提供并发控制与故障恢复两种技术，前者用于减少并发异常，后者可以在系统异常的时候保证事务与数据库状态不会被破坏。对于并发控制，其核心思想就是加锁，无论是乐观锁还是悲观锁，只要提供的隔离级别越高，那么读写性能必然越差 水平扩展后带来的种种问题难处理 前文提过，随着企业规模扩大，一种方式是对数据库做分库，做了分库之后，数据迁移（1 个库的数据按照一定规则打到 2 个库中）、跨库 join（订单数据里有用户数据，两条数据不在同一个库中）、分布式事务处理都是需要考虑的问题，尤其是分布式事务处理，业界当前都没有特别好的解决方案 表结构扩展不方便 由于数据库存储的是结构化数据，因此表结构 schema 是固定的，扩展不方便，如果需要修改表结构，需要执行 DDL（data definition language）语句修改，修改期间会导致锁表，部分服务不可用 全文搜索功能弱 例如 like “% 中国真伟大 %”，只能搜索到 “2019 年中国真伟大，爱祖国”，无法搜索到 “中国真是太伟大了” 这样的文本，即不具备分词能力，且 like 查询在 “% 中国真伟大” 这样的搜索条件下，无法命中索引，将会导致查询效率大大降低 写了这么多，我的理解核心还是前三点，它反映出的一个问题是关系型数据库在高并发下的能力是有瓶颈的，尤其是写入 / 更新频繁的情况下，出现瓶颈的结果就是数据库 CPU 高、Sql 执行慢、客户端报数据库连接池不够等错误，因此例如万人秒杀这种场景，我们绝对不可能通过数据库直接去扣减库存。 可能有朋友说，数据库在高并发下的能力有瓶颈，我公司有钱，加 CPU、换固态硬盘、继续买服务器加数据库做分库不就好了，问题是这是一种性价比非常低的方式，花 1000 万达到的效果，换其他方式可能 100 万就达到了，不考虑人员、服务器投入产出比的 Leader 就是个不合格的 Leader，且关系型数据库的方式，受限于它本身的特点，可能花了钱都未必能达到想要的效果。至于什么是花 100 万就能达到花 1000 万效果的方式呢？可以继续往下看，这就是我们要说的 NoSql。 结合 NoSql 的方式做存储的架构演进像上文分析的，数据库作为一种关系型数据的存储引擎，存储的是关系型数据，它有优点，同时也有明显的缺点，因此通常在企业规模不断扩大的情况下，不会一味指望通过增强数据库的能力来解决数据存储问题，而是会引入其他存储，也就是我们说的 NoSql。 NoSql 的全称为 Not Only SQL，泛指非关系型数据库，是对关系型数据库的一种补充，特别注意补充这两个字，这意味着 NoSql 与关系型数据库并不是对立关系，二者各有优劣，取长补短，在合适的场景下选择合适的存储引擎才是正确的做法。 比较简单的 NoSql 就是缓存： 针对那些读远多于写的数据，引入一层缓存，每次读从缓存中读取，缓存中读取不到，再去数据库中取，取完之后再写入到缓存，对数据做好失效机制通常就没有大问题了。通常来说，缓存是性能优化的第一选择也是见效最明显的方案。 但是，缓存通常都是 KV 型存储且容量有限（基于内存），无法解决所有问题，于是再进一步的优化，我们继续引入其他 NoSql： 数据库、缓存与其他 NoSql 并行工作，充分发挥每种 NoSql 的特点。当然 NoSql 在性能方面大大优于关系挺数据库的同时，往往也伴随着一些特性的缺失，比较常见的就是事务功能的缺失。 下面看一下常用的 NoSql 及他们的代表产品，并对每种 NoSql 的优缺点和适用场景做一下分析，便于熟悉每种 NoSql 的特点，方便技术选型。 KV 型 NoSql（Redis）KV 型 NoSql 顾名思义就是以键值对形式存储的非关系型数据库，是最简单、最容易理解也是大家最熟悉的一种 NoSql，因此比较快地带过。Redis、MemCache 是其中的代表，Redis 又是 KV 型 NoSql 中应用最广泛的 NoSql，KV 型数据库以 Redis 为例，最大的优点我总结下来就两点： 数据基于内存，读写效率高 KV 型数据，时间复杂度为 O(1)，查询速度快 因此，KV 型 NoSql 最大的优点就是高性能，利用 Redis 自带的 BenchMark 做基准测试，TPS 可达到 10 万的级别，性能非常强劲。同样的 Redis 也有所有 KV 型 NoSql 都有的比较明显的缺点： 只能根据 K 查 V，无法根据 V 查 K 查询方式单一，只有 KV 的方式，不支持条件查询，多条件查询唯一的做法就是数据冗余，但这会极大的浪费存储空间 内存是有限的，无法支持海量数据存储 同样的，由于 KV 型 NoSql 的存储是基于内存的，会有丢失数据的风险 综上所述，KV 型 NoSql 最合适的场景就是缓存的场景： 读远多于写 读取能力强 没有持久化的需求，可以容忍数据丢失，反正丢了再查询一把写入就是了 例如根据用户 id 查询用户信息，每次根据用户 id 去缓存中查询一把，查到数据直接返回，查不到去关系型数据库里面根据 id 查询一把数据写到缓存中去。 搜索型 NoSql（ElasticSearch）传统关系型数据库主要通过索引来达到快速查询的目的，但是在全文搜索的场景下，索引是无能为力的，like 查询一来无法满足所有模糊匹配需求，二来使用限制太大且使用不当容易造成慢查询，搜索型 NoSql 的诞生正是为了解决关系型数据库全文搜索能力较弱的问题，ElasticSearch 是搜索型 NoSql 的代表产品。 全文搜索的原理是倒排索引，我们看一下什么是倒排索引。要说倒排索引我们先看下什么是正排索引，传统的正排索引是文档 –&gt; 关键字的映射，例如 “Tom is my friend” 这句话，会将其切分为 “Tom”、”is”、”my”、”friend” 四个单词，在搜索的时候对文档进行扫描，符合条件的查出来。这种方式原理非常简单，但是由于其检索效率太低，基本没什么实用价值。 倒排索引则完全相反，它是关键字 –&gt; 文档的映射，我用张表格展示一下就比较清楚了： 意思是我现在这里有四个短句： “Tom is Tom” “Tom is my friend” “Thank you, Betty” “Tom is Betty’s husband” 搜索引擎会根据一定的切分规则将这句话切成 N 个关键字，并以关键字的维度维护关键字在每个文本中的出现次数。这样下次搜索 “Tom” 的时候，由于 Tom 这个词语在 “Tom is Tom”、”Tom is my friend”、”Tom is Betty’s husband”三句话中都有出现，因此这三条记录都会被检索出来，且由于”Tom is Tom”这句话中”Tom”出现了 2 次，因此这条记录对”Tom” 这个单词的匹配度最高，最先展示。这就是搜索引擎倒排索引的基本原理，假设某个关键字在某个文档中出现，那么倒排索引中有两部分内容： 文档 ID 在该文档中出现的位置情况 可以举一反三，我们搜索 “Betty Tom” 这两个词语也是一样，搜索引擎将 “Betty Tom” 切分为 “Tom”、”Betty” 两个单词，根据开发者指定的满足率，比如满足率 = 50%，那么只要记录中出现了两个单词之一的记录都会被检索出来，再按照匹配度进行展示。 搜索型 NoSql 以 ElasticSearch 为例，它的优点为： 支持分词场景、全文搜索，这是区别于关系型数据库最大特点 支持条件查询，支持聚合操作，类似关系型数据库的 Group By，但是功能更加强大，适合做数据分析 数据写文件无丢失风险，在集群环境下可以方便横向扩展，可承载 PB 级别的数据 高可用，自动发现新的或者失败的节点，重组和重新平衡数据，确保数据是安全和可访问的 同样，ElasticSearch 也有比较明显的缺点： 性能全靠内存来顶，也是使用的时候最需要注意的点，非常吃硬件资源、吃内存，大数据量下 64G + SSD 基本是标配，算得上是数据库中的爱马仕了。为什么要专门提一下内存呢，因为内存这个东西是很值钱的，相同的配置多一倍内存，一个月差不多就要多花几百块钱，至于 ElasticSearch 内存用在什么地方，大概有如下这些： Indexing Buffer—-ElasticSearch 基于 Luence，Lucene 的倒排索引是先在内存里生成，然后定期以 Segment File 的方式刷磁盘的，每个 Segment File 实际就是一个完整的倒排索引 Segment Memory—- 倒排索引前面说过是基于关键字的，Lucene 在 4.0 后会将所有关键字以 FST 这种数据结构的方式将所有关键字在启动的时候全量加载到内存，加快查询速度，官方建议至少留系统一半内存给 Lucene 各类缓存 —-Filter Cache、Field Cache、Indexing Cache 等，用于提升查询分析性能，例如 Filter Cache 用于缓存使用过的 Filter 的结果集 Cluter State Buffer—-ElasticSearch 被设计为每个 Node 都可以响应用户请求，因此每个 Node 的内存中都包含有一份集群状态的拷贝，一个规模很大的集群这个状态信息可能会非常大 读写之间有延迟，写入的数据差不多 1s 样子会被读取到，这也正常，写入的时候自动加入这么多索引肯定影响性能 数据结构灵活性不高，ElasticSearch 这个东西，字段一旦建立就没法修改类型了，假如建立的数据表某个字段没有加全文索引，想加上，那么只能把整个表删了再重建 因此，搜索型 NoSql 最适用的场景就是有条件搜索尤其是全文搜索的场景，作为关系型数据库的一种替代方案。 另外，搜索型数据库还有一种特别重要的应用场景。我们可以想，一旦对数据库做了分库分表后，原来可以在单表中做的聚合操作、统计操作是否统统失效？例如我把订单表分 16 个库，1024 张表，那么订单数据就散落在 1024 张表中，我想要统计昨天浙江省单笔成交金额最高的订单是哪笔如何做？我想要把昨天的所有订单按照时间排序分页展示如何做？这就是搜索型 NoSql 的另一大作用了，我们可以把分表之后的数据统一打在搜索型 NoSql 中，利用搜索型 NoSql 的搜索与聚合能力完成对全量数据的查询。 至于为什么把它放在 KV 型 NoSql 后面作为第二个写呢，因为通常搜索型 NoSql 也会作为一层前置缓存，来对关系型数据库进行保护。 列式 NoSql（HBase）列式 NoSql，大数据时代最具代表性的技术之一了，以 HBase 为代表。 列式 NoSql 是基于列式存储的，那么什么是列式存储呢，列式 NoSql 和关系型数据库一样都有主键的概念，区别在于关系型数据库是按照行组织的数据： 看到每行有 name、phone、address 三个字段，这是行式存储的方式，且可以观察 id = 2 的这条数据，即使 phone 字段没有，它也是占空间的。 列式存储完全是另一种方式，它是按每一列进行组织的数据： 这么做有什么好处呢？大致有以下几点： 查询时只有指定的列会被读取，不会读取所有列 存储上节约空间，Null 值不会被存储，一列中有时候会有很多重复数据（尤其是枚举数据，性别、状态等），这类数据可压缩，行式数据库压缩率通常在 3:15:1 之间，列式数据库的压缩率一般在 8:130:1 左右 列数据被组织到一起，一次磁盘 IO 可以将一列数据一次性读取到内存中 第二点说到了数据压缩，什么意思呢，以比较常见的字典表压缩方式举例： 自己看图理解一下，应该就懂了。 接着继续讲讲优缺点，列式 NoSql，以 HBase 为代表的，优点为： 海量数据无限存储，PB 级别数据随便存，底层基于 HDFS（Hadoop 文件系统），数据持久化 读写性能好，只要没有滥用造成数据热点，读写基本随便玩 横向扩展在关系型数据库及非关系型数据库中都是最方便的之一，只需要添加新机器就可以实现数据容量的线性增长，且可用在廉价服务器上，节省成本 本身没有单点故障，可用性高 可存储结构化或者半结构化的数据 列数理论上无限，HBase 本身只对列族数量有要求，建议 1~3 个 说了这么多 HBase 的优点，又到了说 HBase 缺点的时候了： HBase 是 Hadoop 生态的一部分，因此它本身是一款比较重的产品，依赖很多 Hadoop 组件，数据规模不大没必要用，运维还是有点复杂的 KV 式，不支持条件查询，或者说条件查询非常非常弱吧，HBase 在 Scan 扫描一批数据的情况下还是提供了前缀匹配这种 API 的，条件查询除非定义多个 RowKey 做数据冗余 不支持分页查询，因为统计不了数据总数 因此 HBase 比较适用于那种 KV 型的且未来无法预估数据增长量的场景，另外 HBase 使用还是需要一定的经验，主要体现在 RowKey 的设计上。 文档型 NoSql（MongoDB）坦白讲，根据我的工作经历，文档型 NoSql 我只有比较浅的使用经验，因此这部分只能结合之前的使用与网上的文章大致给大家介绍一下。 什么是文档型 NoSql 呢，文档型 NoSql 指的是将半结构化数据存储为文档的一种 NoSql，文档型 NoSql 通常以 JSON 或者 XML 格式存储数据，因此文档型 NoSql 是没有 Schema 的，由于没有 Schema 的特性，我们可以随意地存储与读取数据，因此文档型 NoSql 的出现是解决关系型数据库表结构扩展不方便的问题的。 MongoDB 是文档型 NoSql 的代表产品，同时也是所有 NoSql 产品中的明星产品之一，因此这里以 MongoDB 为例。按我的理解，作为文档型 NoSql，MongoDB 是一款完全和关系型数据库对标的产品，就我们从存储上来看： 看到，关系型数据库是按部就班地每个字段一列存，在 MongDB 里面就是一个 JSON 字符串存储。关系型数据可以为 name、phone 建立索引，MongoDB 使用 createIndex 命令一样可以为列建立索引，建立索引之后可以大大提升查询效率。其他方面而言，就大的基本概念，二者之间基本也是类似的： 因此，对于 MongDB，我们只要理解成一个 Free-Schema 的关系型数据库就完事了，它的优缺点比较一目了然，优点： 没有预定义的字段，扩展字段容易 相较于关系型数据库，读写性能优越，命中二级索引的查询不会比关系型数据库慢，对于非索引字段的查询则是全面胜出 缺点在于： 不支持事务操作，虽然 Mongodb4.0 之后宣称支持事务，但是效果待观测 多表之间的关联查询不支持（虽然有嵌入文档的方式），join 查询还是需要多次操作 空间占用较大，这个是 MongDB 的设计问题，空间预分配机制 + 删除数据后空间不释放，只有用 db.repairDatabase() 去修复才能释放 目前没发现 MongoDB 有关系型数据库例如 MySql 的 Navicat 这种成熟的运维工具 总而言之，MongDB 的使用场景很大程度上可以对标关系型数据库，但是比较适合处理那些没有 join、没有强一致性要求且表 Schema 会常变化的数据。 总结：数据库与 NoSql 及各种 NoSql 间的对比最后一部分，做一个总结，本文归根到底是两个话题： 何时选用关系型数据库，何时选用非关系型数据库 选用非关系型数据库，使用哪种非关系型数据库 首先是第一个话题，关系型数据库与非关系型数据库的选择，在我理解里面无非就是两点考虑： 第一点，不多解释应该都理解，非关系型数据库都是通过牺牲了 ACID 特性来获取更高的性能的，假设两张表之间有比较强的一致性需求，那么这类数据是不适合放在非关系型数据库中的。 第二点，核心数据不走非关系型数据库，例如用户表、订单表，但是这有一个前提，就是这一类核心数据会有多种查询模式，例如用户表有 ABCD 四个字段，可能根据 AB 查，可能根据 AC 查，可能根据 D 查，假设核心数据，但是就是个 KV 形式，比如用户的聊天记录，那么 HBase 一存就完事了。 这几年的工作经验来看，非核心数据尤其是日志、流水一类中间数据千万不要写在关系型数据库中，这一类数据通常有两个特点： 写远高于读 写入量巨大 一旦使用关系型数据库作为存储引擎，将大大降低关系型数据库的能力，正常读写 QPS 不高的核心服务会受这一类数据读写的拖累。 接着是第二个问题，如果我们使用非关系型数据库作为存储引擎，那么如何选型？其实上面的文章基本都写了，这里只是做一个总结（所有的缺点都不会体现事务这个点，因为这是所有 NoSql 相比关系型数据库共有的一个问题）： 但是这里特别说明，选型一定要结合实际情况而不是照本宣科，比如： 企业发展之初，明明一个关系型数据库就能搞定且支撑一年的架构，搞一套大而全的技术方案出来 有一些数据条件查询多，更适合使用 ElasticSearch 做存储降低关系型数据库压力，但是公司成本有限，这种情况下这类数据可以尝试继续使用关系型数据库做存储 有一类数据格式简单，就是个 KV 类型且增长量大，但是公司没有 HBase 这方面的人才，运维上可能会有一定难度，出于实际情况考虑，可先用关系型数据库顶一阵子 所以，如果不考虑实际情况，虽然合适有些存储引擎更加合适，但是强行使用反而适得其反，总而言之，适合自己的才是最好的。","categories":[{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/categories/nosql/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/tags/sql/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/tags/nosql/"}]},{"title":"Spring 事务管理","slug":"Spring 事务管理","date":"2020-09-02T03:25:57.245Z","updated":"2023-02-06T11:49:41.794Z","comments":true,"path":"2020/09/02/Spring 事务管理/","link":"","permalink":"https://topone233.github.io/2020/09/02/Spring%20%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/","excerpt":"","text":"在数据库操作中事务管理是一个重要的概念，例如银行转账。 Spring的事务管理简化了传统的数据库事务管理流程，提高了开发效率。 1.编程式事务管理 在代码中显示调用beginTransaction、commit、rollback 等与事务处理相关的方法，这就是编程式事务管理。当只有少数事务操作时，编程式事务管理才比较合适。 基于底层API的编程式事务管理 基于 TransactionTemplate 的编程式事务管理 2.声明式事务管理 Spring的声明式事务管理是通过AOP技术实现的事务管理，其本质是对方法前后进行拦截，然后在目标方法开始之前创建或加入一个事务，在执行完目标方法之后根据执行情况提交或回滚事务。 声明式事务管理最大的优点是不需要通过编程的方式管理事务，因而不需要在业务逻辑代码中掺杂事务处理的代码，只需相关的事务规则声明便可以将事务规则应用到业务逻辑中。 使用声明式事务管理不仅因为其简单，更主要的是可以使得纯业务代码不被污染，极大的方便了后期的代码维护。 与编程式事务管理相比，唯一不足的是：最细粒度只能作用到方法级别，无法像编程式那样可以作用到代码块级别。（不过可以通过变通的方法解决，比如将需要进行事务处理的代码块独立为方法等）。 Spring的声明式事务管理通过两种方式实现：XML、@Transactional 注解 2.1 基于XML创建Dao层 package com.statement.dao; public interface TestDao { public int save(String sql, Object param[]); public int delete(String sql, Object param[]); }package com.statement.dao; @Repository(&quot;TestDao&quot;) public class TestDaoImpl implements TestDao { @Autowired private JdbcTemplate jdbcTemplate; @Override public int save(String sql, Object[] param) { return jdbcTemplate.update(sql, param); } @Override public int delete(String sql, Object[] param) { return jdbcTemplate.update(sql, param); } }创建Service层，依赖注入数据访问层。 package com.statement.service; public interface TestService { public int save(String sql, Object param[]); public int delete(String sql, Object param[]); }package com.statement.service; @Service(&quot;testService&quot;) public class TestServiceImpl implements TestService { @Autowired private TestDao testDao; @Override public int save(String sql, Object[] param) { return testDao.save(sql, param); } @Override public int delete(String sql, Object[] param) { return testDao.delete(sql, param); } }创建Controller层，依赖注入Service层。 package com.statement.controller; @Controller(&quot;statementController&quot;) public class StatementController { @Autowired private TestService testService; public String test() { String message=&quot;&quot;; String deleteSql=&quot;delete from user&quot;; String saveSql=&quot;insert into user values(?,?,?)&quot;; Object param[]={1,&quot;chen&quot;,&quot;男&quot;}; try{ testService.delete(deletSql, null); testService.save(saveSql, param); testService.save(saveSql, param); }catch(Exception e) { message=&quot;主键重复，事务回滚！&quot;; e.printStackTrace(); } return message; } }创建配置文件 &lt;!-- 为数据源添加事务管理器 --&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 编写通知声明事务 --&gt; &lt;tx:advice id=&quot;myAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;!-- *表示任意方法 --&gt; &lt;tx:method name=&quot;*&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 编写AOP，让Spring自动对目标对象生成代理，需要使用AspectJ的表达式 --&gt; &lt;aop:config&gt; &lt;!-- 定义切入点 --&gt; &lt;aop:pointcut expression=&quot;execution(* com.statement.sservice.*.*())&quot; id=&quot;txPointCut&quot;/&gt; &lt;!-- 切面：将切入点与通知关联 --&gt; &lt;aop:advisor advice-ref=&quot;myAdvice&quot; pointcut-ref=&quot;txPointCut&quot;/&gt; &lt;/aop:config&gt;创建测试类 package com.statement.test; public class Test { public static void main(String[] args) { ApplicationContext appCon=new ClassPathXmlApplicationContext(&quot;/com/statement/applicationContext.xml&quot;); StatementController ct=(StatementController)appCon.getBean(&quot;statementController&quot;); String result=ct.test(); System.out.println(result); } } 运行结果： 主键重复，事务回滚！2.2 基于@Transactional@Transactional 可以作用于接口、接口方法、类、类的方法上。 当作用于类上时，该类的所有public方法都将具有该类型的事务属性，同时也可以在方法级别使用该注解来覆盖类级别的定义。 Spring小组不建议在接口或接口方法上使用该注解，因为它只有在使用基于接口的代理时才会生效。 Dao、Service、Controller层相同，仅展示修改的部分代码。 配置文件 &lt;!-- 为数据源添加事务管理器 --&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 为事务管理器注册注解驱动器 --&gt; &lt;tx:annotation-driven transaction-manager=&quot;txManager&quot;/&gt;Spring MVC通常在Service层进行事务管理。 package com.statement.service; @Service(&quot;testService&quot;) @Transactional // 指定这个类需要接受Spring的事务管理 // 只能针对public属性范围内的方法 public class TestServiceImpl implements TestService {","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"Spring AOP","slug":"Spring AOP","date":"2020-08-24T04:05:54.847Z","updated":"2023-02-06T11:49:50.953Z","comments":true,"path":"2020/08/24/Spring AOP/","link":"","permalink":"https://topone233.github.io/2020/08/24/Spring%20AOP/","excerpt":"","text":"1.AOP的概念AOP（Aspect Oriented Programming）面向切面编程，与OOP（Object Oriented Programming，面向对象编程）相辅相成，提供了与OOP不同的抽象软件结构的视角。在OOP中，以类为程序的基本单元，而AOP的基本单元是Aspect（切面）。 尽管OOP可以通过封装、继承达到代码的复用，但仍然有同样的代码分散在各个方法中。AOP采取横向抽取机制，将分散在各个方法中的重复代码提取出来，然后在编译或运行阶段应用到需要执行的地方。这种横向抽取是OOP无法实现的，因为OOP实现的父子关系的纵向复用。AOP不是OOP的替代品，而是补充，两者相辅相成。 1.1 切面Aspect 是指：封装横切到系统功能（如事务处理）的类。 1.2 连接点Joinpoint ：程序运行中的一些时间点，例如：方法的调用、异常的抛出。 1.3 切入点Pointcut ：指需要处理的连接点。Spring AOP中，所有的方法执行都是连接点，而切入点是一个描述信息，修饰连接点。通过切入点确定哪些连接点需要被处理。 1.4 通知Advice ：由切面添加到特定的连接点（满足切入点规则）的一段代码，即在定义好的切入点处所要执行的代码。可以理解为切面开启后切面的方法，通知是切面的具体实现。 根据Spring中通知中目标类方法中的连接点位置，通知可分为6种类型： 1.4.1 环绕通知​ MethodInterceptor 在目标方法执行前、后实施增强，可用于日志记录、事务处理等功能。@Around 1.4.2 前置通知​ MethodBeforeAdvice 在目标方法执行前实施增强，可用于权限管理等功能。@Before 1.4.3 后置返回通知​ AfterReturningAdvice 在目标方法执行成功后实施增强，可用于关闭流、删除临时文件等功能。@AfterReturning 1.4.4 后置（最终）通知​ AfterAdvice 在目标方法执行后实施增强，与后置返回不同的是，不管是否发生异常都要执行，类似于finally。可用于释放资源。@After 1.4.5 异常通知​ ThrowsAdvice 在方法抛出异常后实施增强，可用于处理异常、记录日志等功能。@AfterThrowing 1.4.6 引入通知​ IntroductionInterceptor 在目标类中添加一些新的方法和属性，可用于修改目标类（增强类）。 1.5 引入Introduction ：在不修改代码的前提下，引入可以在运行期为实现类动态的添加自定义的方法和属性 1.6 目标对象Target Object ：指所有被通知的对象。 1.7 代理Proxy ：通知应用到目标对象之后被动态创建的对象。 1.8 织入Weaving ：将切面代码插入到目标对象上，从而生成代理对象的过程。 织入方式： 编译期织入：需要有特殊的Java编译器 类装载期织入：需要有特殊的类装载器 动态代理织入：在运行期为目标类添加通知生成子类的方式。Spring AOP默认采用动态代理织入。 2.动态代理动态代理 Spring AOP中常用JDK和CGLIB两种动态代理技术。 2.1 JDK动态代理JDK动态代理必须借助一个接口才能产生代理对象。对于使用业务接口的类，Spring默认使用JDK动态代理实现AOP。 package dynamic.jdk; public interface TestDao { public void save(); public void modify(); public void delete(); }创建接口实现类作为目标类，在代理类中对其方法进行增强处理。 package dynamic.jdk; public class TestDaoImpl implements TestDao { @Override public void save() { System.out.println(&quot;保存&quot;); } @Override public void modify() { System.out.println(&quot;修改&quot;); } @Override public void delete() { System.out.println(&quot;删除&quot;); } }创建切面类，定义多个通知（增强处理的功能方法）。 package aspect; public class MyAspect { public void check() { System.out.println(&quot;模拟权限控制&quot;); } public void except() { System.out.println(&quot;模拟异常处理&quot;); } public void log() { System.out.println(&quot;模拟日志记录&quot;); } }创建代理类，JDK动态代理中代理类必须实现java.lang.reflect.InvocationHandler接口，并编写代理方法。 package dynamic.jdk; public class JDKDynamicProxy implements InvocationHandler { // 声明目标类接口对象（真实对象） private TestDao testDao; /** * 创建代理的方法，建立代理对象和真实对象的代理关系，并返回代理对象 */ public Object createProxy(TestDao testDao) { this.testDao=testDao; // 类加载器 ClassLoader cld=JDKDynamicProxy.class.getClassLoader(); // 被代理对象实现的所有接口 Class[] clazz=testDao.getClass().getInterFaces(); // 使用代理类进行增强，返回代理后的对象 return Proxy.newProxyInstance(cld, clazz, this); } /** * 代理的逻辑方法，所有动态代理类的方法调用都交给该方法处理 * proxy 是被代理对象 * method 是将要被执行的方法 * args 是执行方法是需要的参数 * return 是返回代理结果 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 创建一个切面 MyAspect myAspect=new MyAspect(); // 前置增强 myAspect.check(); myAspect.except(); // 在目标类上调用方法并传入参数，相当于调用testDao中的方法 Object obj=method.invoke(testDao, args); // 后置增强 myAspect.log(); myAspect.monitor(); return obj; } }创建测试类，在main方法中创建代理对象和目标对象，然后从代理对象中获取对目标对象增强后的对象，最后调用该对象的添加、修改、删除方法。 package dynamic.jdk; public class JDKDynamicTest { public static void main(String[] args) { // 创建代理对象 JDKDynamicProxy jdkProxy=new JDKDynamicProxy(); // 创建目标对象 TestDao testDao=new TestDaoImpl(); // 从代理对象中获取增强后的目标对象。该对象是一个被代理的对象，它会进入代理的逻辑方法invoke中 TestDao testDaoAdvice=(TestDao)jdkProxy.createProxy(testDao); // 执行方法 testDaoAdvice.save(); System.out.println(&quot;===============&quot;); testDaoAdvice.modify(); System.out.println(&quot;===============&quot;); testDaoAdvice.delete(); } } 运行结果： 模拟权限控制 模拟异常处理 保存 模拟日志记录 =============== 模拟权限控制 模拟异常处理 修改 模拟日志记录 =============== 模拟权限控制 模拟异常处理 删除 模拟日志记录2.2 CGLIB 动态代理JDK动态代理必须提供接口才能使用，对于没有提供接口的类，只能采用CGLIB动态代理。 CGLIB（Code Generation Library，代码生成库）是一个高性能开源的代码生成库，采用非常底层的字节码技术，对指定的目标类生成一个子类，并对子类进行增强。Spring Core包已经集成了所需的jar包。 创建目标类，不需要实现任何接口。 package dynamic.cglib; public class TestDao { public void save() { System.out.println(&quot;保存&quot;); } public void modify() { System.out.println(&quot;修改&quot;); } public void delete() { System.out.println(&quot;删除&quot;); } }创建代理类，该类实现MethodInterceptor接口 package dynamic.cglib; public class CglibDynamicProxy implements MethodInterceptor { /** * 创建代理的方法，生成CGLIB代理对象 * target 是目标对象，需要增强的对象 * 返回目标对象的CGLLIB代理对象 */ public Object createProxy(Object target) { // 创建一个动态类对象，即增强类对象 Enhancer enhancer=new Enhancer(); // 确定需要增强的类，设置其父类 enhancer.setSuperclass(target.getClass()); // 确定代理逻辑对象为当前对象，要求当前对象实现MethodInterceptor的方法 enhancer.setCallback(this); // 返回创建的代理对象 return enhancer.create(); } /** * intercept 方法会在程序执行目标方法时被调用 * proxy 是CGLIb根据指定父类生成的代理对象 * method 是拦截方法 * args 是拦截方法的参数数组 * methodProxy 是方法的代理对象，用于执行父类的方法 * 返回代理结果 */ @Override public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { MyAspect myAspect=new MyAspect(); myAspect.check(); myAspect.except(); // 目标方法执行，返回代理结果 Object obj=methodProxy.invokeSuper(proxy, args); myAspect.log(); myAspect.monitor(); return obj; } }创建测试类。 package dynamic.cglib; public class CglibDynamicTest { public static void main(String[] args) { // 创建代理对象 CglibDynamicProxy cdp=new CglibDynamicProxy(); // 创建目标对象 TestDao testDao=new TestDao(); // 获取增强后的目标对象 TestDao testDaoAdvice=(TestDao)cdp.createProxy(testDao); // 执行方法 testDaoAdvice.save(); System.out.println(&quot;==============&quot;); testDaoAdvice.modify(); System.out.println(&quot;==============&quot;); testDaoAdvice.delete(); } } 运行结果相同3.基于代理类的AOP实现纵观AOP编程，程序员只需要参与三个部分： 定义普通业务组件 定义切入点，一个切入点可能横切多个业务组件 定义增强处理，增强处理就是AOP框架为普通业务组件织入的处理动作 所以进行AOP编程的关键就是定义切入点和定义增强处理，一旦定义了合适的切入点和增强处理，AOP框架将自动生成AOP代理，即：代理对象的方法=增强处理+被代理对象的方法。 Spring默认使用JDK动态代理实现AOP编程。使用org.springframework.aop.framework.ProxyFactoryBean 创建代理是Spring AOP实现的最基本方式。 3.1 ProxyFactoryBeanProxyFactoryBean 是org.springframework.beans.factory.FactoryBean 接口的实现类，FactoryBean负责实例化一个Bean实例，ProxyFactoryBean负责为其他Bean实例创建代理实例。 下面通过一个实现环绕通知的实例演示Spring使用ProxyFactoryBean创建AOP代理的过程。 3.1.1 创建切面类由于该实例实现环绕通知，切面类需要实现 MethodInterceptor 接口。 package spring.proxyfactorybean; public class MyAspect implements MethodInterceptor { @Override public Object invoke(MethodInvocation arg) throw Throwable { check(); except(); Object obj=arg.proceed(); log(); return obj; } public void check() { System.out.println(&quot;模拟权限控制&quot;); } public void except() { System.out.println(&quot;模拟异常处理&quot;); } public void log() { System.out.println(&quot;模拟日志记录&quot;); } }3.1.2 配置切面并指定代理切面类需要配置为Bean实例，这样Spring容器才能识别为切面对象。 applicationContext.xml &lt;!-- 定义目标对象（使用上一个案例的） --&gt; &lt;bean id=&quot;testDao&quot; class=&quot;dynamic.jdk.TestDaoImpl&quot;/&gt; &lt;!-- 创建一个切面 --&gt; &lt;bean id=&quot;myAspect&quot; class=&quot;spring.proxyfactorybean.MyAspect&quot;/&gt; &lt;!-- 使用Spring代理工厂定义一个名为testDaoProxy的代理对象 --&gt; &lt;bean id=&quot;testDaoProxy&quot; class=&quot;org.springframework.aop.framework.ProxyFactoryBean&quot;&gt; &lt;!-- 指定代理实现的接口 --&gt; &lt;property name=&quot;proxyInterfaces&quot; value=&quot;dynamic.jdk.TestDao&quot;/&gt; &lt;!-- 指定目标对象 --&gt; &lt;property name=&quot;target&quot; ref=&quot;testDao&quot;/&gt; &lt;!-- 指定切面，织入环绕通知 --&gt; &lt;property name=&quot;interceptorNames&quot; value=&quot;myAspect&quot;/&gt; &lt;!-- 指定代理方式，true指定CGLIB动态代理（默认为false，指定JDK动态代理）--&gt; &lt;property name=&quot;proxyTargetClass&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt;3.1.3 测试package spring.proxyfactorbean; public class ProxyFactoryBeanTest { public static void main(String[] args) { ApplicationContext appCon=new ClassPathXmlApplicationContext(&quot;/spring/proxyfactorybean/applicationContext.xml&quot;); TestDao testDaoAdvice=(TestDao)appCon.getBean(&quot;testDaoProxy&quot;); testDaoAdvice.save(); System.out.println(&quot;===============&quot;); testDaoAdvice.modify(); System.out.println(&quot;===============&quot;); testDaoAdvice.delete(); } } 运行结果与之前相同4.基于注解开发AspectJ AspectJ 是一个基于Java的AOP框架。从Spring 2.0以后引入了AspectJ的支持。建议使用AspectJ实现AOP。 AspectJ实现Spring AOP有两种方式：基于XML、基于注解。基于注解要比基于XML配置开发便捷许多。 注解名称 描述 @Aspect 用于定义一个切面。注解在切面类上。 @Pointcut 用于定义切入点表达式。在使用时需要定义一个切入点方法，该方法是一个返回void且方法体为空的普通方法。 @Before 用于定义前置通知。在使用时通常为其指定value属性值。 @AfterReturning 用于定义后置返回通知。在使用时通常为其指定value属性值。 @Around 用于定义环绕通知。在使用时通常为其指定value属性值。 @AfterThrowing 用于定义异常通知。在使用时通常为其指定value属性值。还有一个throwing属性，用于访问目标方法抛出的异常，该属性值与异常通知方法中同名的形参一致。 @After 用于定义后置（最终）通知。在使用时通常为其指定value属性值。 4.1 创建切面类，并进行注解首先需要使用@Aspect 定义一个切面类，由于该类在Spring中是作为组件使用的，所以还需要使用@Component 。然后使用@Pointcut 注解切入点表达式，并通过定义方法来表示切入点名称。最后在每个通知方法上添加相应的注解，并将切入点名称作为参数传递给需要执行增强的通知方法。 package aspectj.annotation; @Aspect @Component public class MyAspect { /** * execution(* dynamic.jdk.*.*(..))定义切入点表达式 * 意思是：匹配dynamic.jdk包中任意类的任意方法的执行 * 第一个* 返回类型，使用*代表所有类型。注意第一个*与包名之间有一个空格 * 第二个* 表示的类名，使用*代表匹配包中的所有类 * 第三个* 表示的是方法名，使用*表示所有方法 * (..)表示方法的参数，&quot;...&quot;表示任意参数 */ @Pointcut(&quot;execution(* dynamic.jdk.*.*(..))&quot;) private void myPointCut() {} /** * 前置通知，使用Joinpoint接口作为参数获取目标对象信息 */ @Before(&quot;myPointCut()&quot;) public void before(JoinPoint jp) { System.out.print(&quot;前置通知：模拟权限控制&quot;); System.out.println(&quot;, 目标类对象：&quot; + jp.getTarget() + &quot;, 被增强处理的方法：&quot; + jp.getSignature().getName()); } /** * 后置返回通知 */ @AfterReturning(&quot;myPointCut()&quot;) public void afterReturning(JoinPoint jp) { System.out.print(&quot;后置返回通知：模拟删除临时文件&quot;); System.out.println(&quot;, 被增强处理的方法：&quot; + jp.getSignature().getName()); } /** * 环绕通知 * ProceedingJoinPoint 是JoinPoint的子接口，代表可以执行的目标方法 * 返回值的类型必须是Object * 必须一个参数是ProceedingJoinPoint类型 * 必须 throws Throwable */ @Around(&quot;myPointCut()&quot;) public Object around(ProceedingJoinPoint pjp) throws Throwable { System.out.print(&quot;环绕开始：执行目标方法前，模拟开启事务&quot;); // 执行当前目标方法 Object obj=pjp,proceed(); System.out.println(&quot;, 目标类对象：&quot; + jp.getTarget() + &quot;, 被增强处理的方法：&quot; + jp.getSignature().getName()); return obj; } /** * 异常通知 */ @AfterThrowing(value=&quot;myPointCut()&quot;, throwing=&quot;e&quot;) public void except(Throwable e) { System.out.println(&quot;异常通知：&quot; + &quot;程序执行异常&quot; + e.getMessage()); } /** * 后置（最终）通知 */ @After(&quot;myPointCut()&quot;) public void after() { System.out.println(&quot;最终通知：模拟释放资源&quot;); } }4.2 注解目标类使用@Repository 将目标类 TestDaoImpl 注解为目标对象。 @Repository(&quot;testDao&quot;)4.3 创建配置文件applicationContext.xml &lt;!-- 指定需要扫描的包，使注解生效 --&gt; &lt;context:component-scan base-package=&quot;aspectj.annotation&quot;/&gt; &lt;context:component-scan base-package=&quot;dynamic.jdk&quot;/&gt; &lt;!-- 启动基于注解的AspectJ支持 --&gt; &lt;aop:aspectj-autoproxy /&gt;4.4 测试public class AnnotationAspectJTest { public static void main(String[] args) { ApplicationContext appCon=new ClassPathXmlApplicationContext(&quot;/aspectj/applicationContext.xml&quot;); TestDao testDaoAdvice=(TestDao)appCon.getBean(&quot;testDao&quot;); testDaoAdvice.save(); } } 运行结果： 前置通知：模拟权限控制，目标类对象：dynamic.jdk.TestDaoImpl@647fd8ce，被增强处理的方法：save 环绕开始：执行目标方法前，模拟开启事务 保存 最终通知：模拟释放资源 环绕结束：执行目标方法后，模拟关闭事务 后置返回通知：模拟删除临时文件，被增强处理的方法：save异常通知得到执行，需要在TestDaoImpl类的save方法中添加异常代码，例如“ int n = 10/0; ”。 运行结果： 前置通知：模拟权限控制，目标类对象：dynamic.jdk.TestDaoImpl@647fd8ce，被增强处理的方法：save 环绕开始：执行目标方法前，模拟开启事务 最终通知：模拟释放资源 异常通知：程序执行异常/ by zero","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"Spring Bean","slug":"Spring Bean","date":"2020-08-22T08:14:36.429Z","updated":"2023-02-06T11:50:01.068Z","comments":true,"path":"2020/08/22/Spring Bean/","link":"","permalink":"https://topone233.github.io/2020/08/22/Spring%20Bean/","excerpt":"","text":"Spring IoC容器可以创建、装配、配置应用组件对象，这里的组件对象称为Bean。 1.Bean 的配置Spring可以看作一个工厂，用于生产和管理Spring容器中的Bean。 如果要使用这个工厂生产和管理Bean，需要将Bean配置在Spring的配置文件中。支持 XML 和 Properties 两种格式的配置文件，常用 XML格式。 元素的常用属性及其子元素： 属性或子元素 描述 id Bean在BeanFactory中的唯一标识，在代码中通过BeanFactory获取Bean实例时需要依次作为索引名称 class Bean的具体实现类，例如dao.TestDIDaoImpl scope 指定Bean实例的作用域 &lt; property&gt; 用于设置一个属性。name 指定Bean实例中相应的属性名称、value 指定Bean的属性值、ref 指定属性对BeanFactory中其他Bean的引用关系 &lt; constructor-arg&gt; 使用构造方法注入，指定构造方法的参数。index指定参数的序号，ref指定对BeanFactory中其他Bean的引用关系，type指定参数类型，value指定参数的常量值 &lt; map&gt;、 &lt; set&gt; 的子元素，封装对应类型的依赖注入 &lt; list&gt; 的子元素，封装List 、数组类型的依赖注入 &lt; entry&gt; 的子元素，用于设置一个键值对 2.Bean 的实例化在面向对象编程时，如果要使用某个对象，需要事先实例化该对象。Spring中，也需要先实例化Bean。 有3种方式：构造方法实例化、静态工厂实例化、实例工厂实例化。最常用构造方法实例化。 2.1构造方法实例化Spring容器调用Bean对应类中的无参数构造方法来实例化Bean。 创建BeanClass类 package instance; public class BeanClass { public String message; public BeanClass() { message = &quot;构造方法实例化Bean&quot;; } public BeanClass(String s) { message = s; } }创建配置文件applicationContext.xml &lt;!-- 构造方法实例化Bean --&gt; &lt;bean id=&quot;demo&quot; class=&quot;instance.BeanClass&quot;/&gt;创建测试类 package test; public class TestInstance { public static void mani(String[] args) { ApplicationContext appCon = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); BeanClass b = (BeanClass)appCon.getBean(&quot;demo&quot;); System.out.println(b1 + b1.message); } } 结果：instance.BeanClass@490ab905构造方法实例化Bean3.Bean的作用域 scope参数 描述 singleton 默认的作用域，定义的Bean在Spring容器中只有一个Bean实例（返回同一个） prototype Spring容器每次获取Bean，都将创建一个新的Bean实例（每次都返回新的） request 每次HTTP请求都会返回一个Bean实例 session 在一个session中，将返回同一个Bean实例 application 为每个ServletContext对象创建一个实例，即同一个应用共享一个 websocket 为每个WebSocket对象创建一个实例 singleton 与 prototype是最常用的，后面4种仅在Web Spring应用上下文中使用。 4.Bean的生命周期一个对象的生命周期包括：创建（实例化与初始化）、使用、销毁。Bean也遵循这一过程。但是Spring通过了许多对外接口，允许开发者对 实例化（开辟空间）、初始化（属性初始化）、销毁 3个过程的前后做一些操作。 Spring容器可以管理singleton作用域下Bean的生命周期，能够精确知道Bean何时被创建，何时初始化完成，何时被销毁。而对于prototype作用域下的Bean，Spring只负责创建。创建之后Bean实例就交给客户端的代码管理，Spring将不再跟踪其生命周期，也不会管理这些Bean。 Bean的生命周期： 根据Bean的配置情况实例化一个Bean。 根据Spring上下文对实例化的Bean进行依赖注入，即对Bean的属性进行初始化。 如果Bean实现了 BeanNameAware接口，将调用它实现的 setBeanName(String beanId)方法，此处参数传递的是Bean的id，让实现这个接口的Bean知道自己在Spring容器中的的名字。 如果Bean实现了 BeanFactoryAware接口，将调用它实现的 setBeanFactory方法，此处参数传递的是当前Spring工厂实例的引用。此接口是在Bean实例化后、Setter方法之前调用。可以使得Bean获取容器的内部信息，从而进行某些定制化的操作。 如果Bean实现了 ApplicationContextAware接口，将调用它实现的setApplicationContext(ApplicationContext)方法，此处参数传递的是Spring上下文实例的引用。该方法会将容器本身作为参数传给该方法，将Spring传入的参数赋给该类对象的applicationContext实例变量，接下来可以通过该变量来访问容器本身。 如果Bean关联了 BeanPostProcessor接口，将调用初始化方法 postProcessBeforeInitialization(Object obj, String beanName)对Bean进行前置处理。BeanFactoryPostProcessor是Bean属性处理容器，管理所有未实例化的数据（修改属性）。 如果Bean实现了 InitializingBean接口，将调用 afterPropertiesSet方法。 如果Bean在Spring配置文件中配置了 init-method属性，将自动调用其配置的初始化方法。 注意：Spring为Bean提供了两种初始化方式：实现InitializingBean接口、init-method指定。 ​ 两种方式可以同时使用，但如果调用afterPropertiesSet时出错，则不会调用init-method指定的方法。 ​ 通过反射调用init-method指定的方法效率相对较低，但是消除了对Spring的依赖。 如果Bean关联了 BeanPostProcessor接口，将调用 postProcessAfterInitialization(Object obj, String beanName)方法进行后置处理，由于是在Bean初始化结束时调用After方法，也可用于内存或缓存技术。 注意：此时已经可以使用该Bean，由于该Bean的作用域是singleton，所以调用的是同一个Bean实例。 当Bean不再需要时将进入销毁阶段，如果Bean实现了 DisposableBean接口，则调用其实现的destroy方法将Bean销毁 如果在配置文件中通过 destroy-method属性指定了Bean的销毁方法，将调用其配置的销毁方法进行销毁 实例演示： package life; public class BeanLife { public void initMyself() { System.out.println(this.getClass().getName() + &quot;执行自定义的初始化方法&quot;); } public void destroyMyself() { System.out.println(this.getClass().getName() + &quot;执行自定义的销毁方法&quot;)； } }&lt;!-- 使用init-method属性指定初始化方法，使用destroy-method属性指定销毁方法--&gt; &lt;bean id=&quot;beanLife&quot; class=&quot;life.BeanLife&quot; init-method=&quot;initMyself&quot; destroy-method=&quot;destroyMyself&quot;/&gt;package test; public class TestLife { public static void main(String[] args) { ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); BeanLife blife = (BeanLife)ctx.genBean(&quot;beanLife&quot;); System.out.println(&quot;获得对象后&quot; + blife); // 关闭容器，销毁Bean对象 ctx.close(); } }5.Bean的装配Bean的装配可以理解为将Bean依赖注入到Spring容器中。Spring容器支持：基于XML配置、基于注解、自动装配等多种装配方式，最常见的是基于注解。 5.1基于注解的装配尽管使用XML配置文件可以很简单的装配Bean，但如果有大量的Bean，会导致XML配置文件过于庞大，不方便升级与维护，因此更推荐使用注解（annotation）。 @Repository 将数据访问层（DAO）的类标识为Bean。 @Service 将业务逻辑组件类（Service层）标注为Bean。 @Controller 将控制器组件类标注为Bean。 @Component 可以作用在任何层次上，标注一个Bean。为了更加层次化，不推荐使用。 @Autowired 对类成员变量、方法、构造方法进行标注，完成自动装配工作。可以消除setter和getter方法。默认按照Bean的类型进行装配，如果想按照名称装配，需要和@Qualifier 一起使用 @Resource(name=” “) 与@Autowired 功能一样，区别在于该注解默认是按照名称来装配注入的，只有找不到与名称匹配的Bean时才会按照Bean的类型来装配注入。@Resource有两个属性：name、type。name指定Bean实例名称，即按照名称来装配；type指定类型，即按照类型来装配。 @Qualifier 与@Autowired配合使用，当需要按照名称装配时。Bean的实例名称由@Qualifier 的参数指定。","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"Spring IoC","slug":"Spring IoC","date":"2020-08-22T08:09:05.509Z","updated":"2023-02-06T11:50:10.093Z","comments":true,"path":"2020/08/22/Spring IoC/","link":"","permalink":"https://topone233.github.io/2020/08/22/Spring%20IoC/","excerpt":"","text":"1. IoC 的基本概念 IoC（Inversion of Control，控制反转）是一个比较抽象的概念，是Spring框架的核心，用来消减程序的耦合问题。DI（Dependency Injection，依赖注入）是IoC的另外一种说法，只是从不同的角度描述相同的概念。 想吃面包，你可以自己做。也可以选择在面包店下单，告诉店家你的需求，然后等着吃就行。 想买汽车，直接给工厂下单付款即可。 上面的例子包含了控制反转的思想：把制作面包的主动权交给面包店。 当某个Java对象（调用者，例如我），需要调用另一个Java对象（被调用者，即被依赖对象，例如面包）时，以前我们通常会“new 被调用者”来创建对象（例如我们自己做面包）。这种方式会增加调用者与被调用者之间的耦合性，不利于后期代码的升级和维护。 Spring出现后，对象的实例由Spring容器（例如面包店）来创建。Spring容器会负责控制程序之间的关系（例如面包店负责控制我们与面包的关系）。这样控制权就由调用者转移到Spring容器，控制权发生了反转。 依赖注入：Spring容器负责将依赖对象赋值给调用者的成员变量，相当于为调用者注入它所依赖的实例。这就是依赖注入。 综上所述，控制反转是一种通过描述（XML或者注解）并通过第三方去产生或获取特定对象的方式。实现控制反转的是IoC容器，其实现方式是依赖注入。 2.IoC 容器前面我们知道，实现控制反转的是IoC容器。IoC容器的设计主要是基于BeanFactory 和 ApplicationContext 两个接口。 2.1 BeanFactory 接口BeanFactory 由org.springframework.beans.factory.BeanFactory接口定义，提供了完整的IoC服务支持，是一个管理Bean的工厂，主要负责初始化各种Bean。 BeanFactory接口有很多实现类，常用的是XmlBeanFactory，根据XML配置文件中的定义来配置Bean。 使用BeanFactory实例加载Spring配置文件实际并不多见，仅作了解。 2.2 ApplicationContext 接口ApplicationContext 是BeanFactory的子接口，也称应用上下文。除了包含BeanFactory的所有功能以外，还添加了对国际化、资源访问、事件传播等内容的支持。 创建ApplicationContext接口实例通常有以下三种方法： 2.2.1 通过ClassPathXmlApplicationContextpublic static void main(String[] args) { // 初始化Spring容器ApplicationContext，加载指定的XML配置文件 ApplicationContext appCon = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); // 通过容器获取test实例 TestDao tt = (TestDao)appCon.getBean(&quot;test&quot;); tt.sayHello(); }2.2.2通过FileSystemXmlApplicationContext创建// 仅需要修改这一行代码 ApplicationContext appCon = new FileSystemXmlApplicationContext(&quot;D:\\test\\applicationContext.xml&quot;);FileSystemXmlApplicationContext 将从指定文件的绝对路径中寻找XML配置文件，但是绝对路径会导致程序的灵活性变差，不推荐使用。通常Spring的Java应用采用ClassPathXmlApplicationContext类来实例化ApplicationContext容器，而Web应用中，将交给Web服务器完成。 2.2.3通过Web服务器实例化ApplicationContext容器Web服务器实例化ApplicationContext容器时，一般使用基于org.springframework.web.context.ContextLoaderListener的实现方式。 &lt;context-param&gt; &lt;!-- 加载src目录下的applicationContext.xml文件 --&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath:applicationContext.xml &lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 指定以ContextLoaderListener方式启动Spring容器 --&gt; &lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt; &lt;/listener&gt;3.依赖注入 Spring中实现IoC容器的方法是依赖注入。依赖注入的作用是在使用Spring创建对象时，动态地将其所依赖的对象（例如属性值）注入Bean组件中。 3.1使用属性的setter方法注入依赖注入通常有两种实现方式：构造方法注入、属性的setter方法注入。（这两种方式都是基于Java的反射机制）。 使用setter方法注入是Spring中最主流的注入方式，利用Java Bean规范所定义的setter方法来完成注入，灵活且可读性高。 下面将两者放在一起进行，方便对比。 3.1.1创建dao包在dao包创建TestDIDao接口及其实现类TestDIDaoImpl package dao; public interface TestDIDao { public void sayHello(); }package dao; public class TestDIDaoImpl implements TestDIDao { @Override public void sayHello() { System.out.println(&quot;TestDIDao say: Hello&quot;); } }3.1.2创建service包service包中创建TestDIService接口及其实现类TestDIServiceImpl package service; public interface TestDIService { public void sayHello(); }package service; import dao.TestDIDao; public class TestDIServiceImpl implements TestDIService { private TestDIDao testDIDao; // 添加testDIDao属性的setter方法，用于实现依赖注入 public void setTestDIDao(TestDIDao testDIDao) { this.testDIDao = testDIDao; } /* 构造方法，用于实现依赖注入接口对象testDIDao public TestDIServiceImpl(TestDIDao testDIDao) { super(); this.testDIDao = testDIDao; } */ @Override public void sayHello() { // 调用testDIDao中的sayHelllo方法 testDIDao.sayHello(); System.out.println(&quot;TestDIService setter方法注入 say: Hello&quot;); } }3.1.3编写配置文件在src根目录下创建Spring配置文件 applicationContext.xml。将TestDIServiceImpl类托管给Spring，让Spring创建其对象，同时调用其setter方法完成依赖注入。 &lt;!-- 将TestDIDaoImpl类配置给Spring，让Spring创建其实例 --&gt; &lt;bean id=&quot;myTestDIDao&quot; class=&quot;dao.TestDIDaoImpl&quot;/&gt; &lt;!-- 使用setter方法注入 --&gt; &lt;bean id=&quot;testDIService&quot; class=&quot;service.TestDIServiceImpl&quot;&gt; &lt;!-- 调用TestDIServiceImpl类的setter方法，将myTestDIDao注入到TestDIServiceImpl类的属性testDIDao上--&gt; &lt;property name=&quot;testDIDao&quot; ref=&quot;myTestDIDao&quot;/&gt; &lt;/bean&gt; /* &lt;!-- 使用构造方法注入 --&gt; &lt;bean id=&quot;testDIService&quot; class=&quot;service.TestDIServiceImpl&quot;&gt; &lt;!-- 将myTestDIDao注入到TestDIServiceImpl类的属性testDIDao上--&gt; &lt;!-- constructor-arg元素用于定义类构造方法的参数，index定义参数的位置--&gt; &lt;!-- ref指定某个实例的引用，如果参数是常量值，ref由value代替--&gt; &lt;constructor-arg index=&quot;o&quot; ref=&quot;myTestDIDao&quot;/&gt; &lt;/bean&gt; */3.1.4测试创建test包，并创建TestDI测试类 package test; public class TestDI { public static void main(String[] args) { ApplicationContext appCon = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;) TestDIservice ts = (TestDIService)appCon.getBean(&quot;testDIService&quot;); ts.sayHello(); } }","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"Java 集合","slug":"Java 集合","date":"2020-07-31T16:00:00.000Z","updated":"2020-09-17T02:21:21.757Z","comments":true,"path":"2020/08/01/Java 集合/","link":"","permalink":"https://topone233.github.io/2020/08/01/Java%20%E9%9B%86%E5%90%88/","excerpt":"","text":"","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"集合","slug":"集合","permalink":"https://topone233.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"Java 多线程","slug":"Java 多线程","date":"2020-07-28T16:00:00.000Z","updated":"2023-02-06T11:47:44.662Z","comments":true,"path":"2020/07/29/Java 多线程/","link":"","permalink":"https://topone233.github.io/2020/07/29/Java%20%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"多线程","slug":"多线程","permalink":"https://topone233.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"Java IO","slug":"Java IO","date":"2020-07-26T16:00:00.000Z","updated":"2020-09-30T08:34:04.870Z","comments":true,"path":"2020/07/27/Java IO/","link":"","permalink":"https://topone233.github.io/2020/07/27/Java%20IO/","excerpt":"","text":"","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"IO","slug":"io","permalink":"https://topone233.github.io/tags/io/"}]},{"title":"关于 Java 中 length、length()、size() 的区别","slug":"关于 Java 中 length、length()、size() 的区别","date":"2020-07-21T16:00:00.000Z","updated":"2023-02-06T11:44:37.265Z","comments":true,"path":"2020/07/22/关于 Java 中 length、length()、size() 的区别/","link":"","permalink":"https://topone233.github.io/2020/07/22/%E5%85%B3%E4%BA%8E%20Java%20%E4%B8%AD%20length%E3%80%81length()%E3%80%81size()%20%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"以前总是觉得自己好像会了，但是某天忽然面对这个笔试题还是会恍惚一下，混淆和答错的几率也很大，不知道有没有其他人像我一样的。 所以今天把这个问题记一下，希望印象更深刻。 首先区分一下 length 和 length()； length 不是方法，是属性，数组的属性； public static void main(String\\[\\] args) { int\\[\\] intArray = {1,2,3}; System.out.println(&quot;这个数组的长度为：&quot; + intArray.length); }length() 是字符串 String 的一个方法； public static void main(String\\[\\] args) { String str = &quot;HelloWorld&quot;; System.out.println(&quot;这个字符串的长度为：&quot; + str.length()); }进入 length() 方法看一下实现 private final char value\\[\\]; public int length() { return value.length; }注释中的解释是 @return the length of the sequence of characters represented by this object. 即由该对象所代表的字符序列的长度，所以归根结底最后要找的还是 length 这个底层的属性； size() 方法，是 List 集合的一个方法； public static void main(String\\[\\] args) { List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;a&quot;); list.add(&quot;b&quot;); list.add(&quot;c&quot;); System.out.println(&quot;这个list的长度为：&quot; + list.size()); }在 List 的方法中，是没有 length() 方法的； 也看一段 ArrayList 的源码 private final E\\[\\] a; ArrayList(E\\[\\] array) { if (array==null) throw new NullPointerException(); a = array; } public int size() { return a.length; }由这段就可以看出 list 的底层实现其实就是数组，size() 方法最后要找的其实还是数组的 length 属性； 另外，除了 List，Set 和 Map 也有 size() 方法，所以准确说 size() 方法是针对集合而言。 总结： length——数组的属性； length()——String 的方法； size()——集合的方法； 谨记。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"集合","slug":"集合","permalink":"https://topone233.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"十大经典排序算法(Java)","slug":"十大经典排序算法(Java)","date":"2020-07-21T06:14:54.671Z","updated":"2023-05-09T07:41:45.770Z","comments":true,"path":"2020/07/21/十大经典排序算法(Java)/","link":"","permalink":"https://topone233.github.io/2020/07/21/%E5%8D%81%E5%A4%A7%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95(Java)/","excerpt":"","text":"排序算法说明1. 排序的定义对一序列对象根据某个关键字进行排序。本文对十大排序算法进行解读。 2. 术语说明 稳定：如果 a 原本在 b 前面，而 a=b，排序之后 a 仍然在 b 的前面； 不稳定：如果 a 原本在 b 的前面，而 a=b，排序之后 a 可能会出现在 b 的后面； 内排序：所有排序操作都在内存中完成； 外排序：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行； 时间复杂度： 一个算法执行所耗费的时间。 空间复杂度：运行完一个程序所需内存的大小。 3. 算法总结 图片名词解释： n: 数据规模 k: “桶” 的个数 In-place: 占用常数内存，不占用额外内存 Out-place: 占用额外内存 4. 算法分类 5. 比较和非比较的区别常见的快速排序、归并排序、堆排序、冒泡排序等属于比较排序。在排序的最终结果里，元素之间的次序依赖于它们之间的比较。每个数都必须和其他数进行比较，才能确定自己的位置。在冒泡排序之类的排序中，问题规模为 n，又因为需要比较 n 次，所以平均时间复杂度为 O(n²)。在归并排序、快速排序之类的排序中，问题规模通过分治法消减为 logN 次，所以时间复杂度平均 O(nlogn)。比较排序的优势是，适用于各种规模的数据，也不在乎数据的分布，都能进行排序。可以说，比较排序适用于一切需要排序的情况。 计数排序、基数排序、桶排序则属于非比较排序。非比较排序是通过确定每个元素之前，应该有多少个元素来排序。针对数组 arr，计算 arr[i] 之前有多少个元素，则唯一确定了 arr[i] 在排序后数组中的位置。非比较排序只要确定每个元素之前的已有的元素个数即可，所有一次遍历即可解决。算法时间复杂度 O(n)。非比较排序时间复杂度底，但由于非比较排序需要占用空间来确定唯一位置。所以对数据规模和数据分布有一定的要求。 1.冒泡排序（Bubble Sort）冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢 “浮” 到数列的顶端。 1.1 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤 1~3，直到排序完成。 1.2 动图演示 1.3 代码实现易错点：注意内层循环还是从0开始的 /** * 比较相邻两个元素，每轮循环将最大值冒泡出来。最右侧是有序区 */ public static void bubbleSort(int[] array) { if (array == null || array.length &lt; 2) { return; } int len = array.length; for (int i = 0; i &lt; len; i++) { // 最右侧是有序区，因此内循环的范围是0到有序区 // 易错点：注意内层循环还是从0开始的 for (int j = 0; j &lt; len - 1 - i; j++) { if (array[j] > array[j + 1]) { int temp = array[j + 1]; array[j + 1] = array[j]; array[j] = temp; } } } System.out.println(\"冒泡排序： \" + Arrays.toString(array)); } 1.4 算法分析冒泡排序是稳定的排序算法，最容易实现的排序。 由于冒泡排序中只有缓存的temp变量需要内存空间, 因此空间复杂度为常量O(1)。 最坏的情况是每次都需要交换, 共需遍历并交换将近n²/2次, 时间复杂度为O(n²)。 最佳的情况是内循环遍历一次后发现排序是对的, 因此退出循环, 时间复杂度为O(n)。 平均来讲, 时间复杂度为O(n²)。 2.选择排序（Selection Sort）无论什么数据进去都是 O(n²) 的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间。理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 选择排序 (Selection-sort) 是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 2.1 算法描述n 个记录的直接选择排序可经过 n-1 趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为 R[1..n]，有序区为空； 第 i 趟排序 (i=1,2,3…n-1) 开始时，当前有序区和无序区分别为 R[1..i-1]和 R(i..n）。该趟排序从当前无序区中 - 选出关键字最小的记录 R[k]，将它与无序区的第 1 个记录 R 交换，使 R[1..i]和 R[i+1..n)分别变为记录个数增加 1 个的新有序区和记录个数减少 1 个的新无序区； n-1 趟结束，数组有序化了。 2.2 动图演示 2.3 代码实现易错点：最值要在循环内定义，因为这是未排序部分的最值，而不是整个数组的最值。 /** * n轮循环，每轮循环找到未排序区域的最小值，放到左侧有序区域。最终全部为有序。 */ public static void selectSort(int[] array) { if (array == null || array.length &lt; 2) { return; } int len = array.length; // 每一轮循环找到一个最小值，在未排序元素中 for (int i = 0; i &lt; len; i++) { // 记录最小值下标 // 易错点：最值要在循环内定义，因为这是未排序部分的最值，而不是整个数组的最值。 int index = i; // 左侧为有序区。内循环从当前的下一个（i + 1）开始 for (int j = i + 1; j &lt; len; j++) { if (array[j] &lt; array[index]) { index = j; } } if (index != i) { int temp = array[index]; array[index] = array[i]; array[i] = temp; } } System.out.println(\"选择排序： \" + Arrays.toString(array)); } 2.4 算法分析最佳情况：O(n²) 最差情况：O(n²) 平均情况： O(n²) 3.插入排序（Insertion Sort）插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用 in-place 排序（即只需用到 O(1) 的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 3.1 算法描述一般来说，插入排序都采用 in-place 在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤 3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤 2~5。 3.2 动图演示 3.3 代码实现提供两种写法，一种是移位法，一种是交换法。移位法是完全按照以上算法描述实，再插入过程中将有序序列中比待插入数字大的数据向后移动，由于移动时会覆盖待插入数据，所以需要额外的临时变量保存待插入数据，代码实现如下： /** * 插入排序-移位法 * 每次在右侧未排序队列，选中一个元素，在左侧有序队列中，找适合自己的位置，并插入 */ public static void insertionSort(int[] array) { if (array == null || array.length &lt; 2) { return; } for (int i = 0; i &lt; array.length; i++) { int j = i - 1; // cur即被选中，进行操作的元素 int cur = array[i]; // 如果比待插入数据大，就后移 while (j >= 0 &amp;&amp; cur &lt; array[j]) { // j位置元素大，后移，腾出位置 array[j + 1] = array[j]; // 一直向前，直到找到合适的位置 j--; } // 找到比待插入数据小的位置，将待插入数据插入 // 为什么要+1？因为最后一次循环会将j变成-1，多做了一次减法，所以要加回来 array[j + 1] = cur; } System.out.println(\"插入排序： \" + Arrays.toString(array)); } 而交换法不需求额外的保存待插入数据，通过不停的向前交换带插入数据，类似冒泡法，直到找到比它小的值，也就是待插入数据找到了自己的位置: public static void insertionSort(int[] array) { if (array == null || array.length &lt; 2) { return; } for (int i = 1; i &lt; array.length; i++) { int j = i - 1; while (j >= 0 &amp;&amp; array[j] > array[i]) { // 只要大就交换操作 // 这里采用一种不需要额外空间的交换方法，但是加法存在溢出的风险。 // 将要交换的两数累加 array[j + 1] = array[j] + array[j+1]; // 第一个位置 = 和 - 原来的值 = 第二个位置的值 array[j] = array[j + 1] - array[j]; // 第二个位置 = 和 - 第一个位置的值 = 第一个位置原来的值 array[j + 1] = array[j + 1] - array[j]; } // 插入排序的另一种写法 // int j; // for (int j = i; j >= gap &amp;&amp; array[j - gap] > temp; j -= gap) { // array[j] = array[j - gap]; // } // array[j] = temp; } System.out.println(\"插入排序： \" + Arrays.toString(array)); } 3.4 算法分析最佳情况： O(n) 最坏情况： O(n²) 平均情况： O(n²) 空间复杂度：O(1) 4.希尔排序（Shell Sort）希尔排序是希尔（Donald Shell）于 1959 年提出的一种排序算法。希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破 O(n2）的第一批算法之一。它与插入排序的不同之处在于，它会优先比较距离较远的元素; 直接插入排序是稳定的；而希尔排序是不稳定的。希尔排序又叫缩小增量排序。 希尔排序是把记录按一定增量分组（gap 间隔），对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至 1 时，整个文件恰被分成一组，算法便终止。 4.1 算法描述我们来看下希尔排序的基本步骤，在此我们选择增量 gap=length/2，缩小增量继续以 gap = gap/2 的方式，这种增量选择我们可以用一个序列来表示，{n/2,(n/2)/2…1}，称为增量序列。希尔排序的增量序列的选择与证明是个数学难题，我们选择的这个增量序列是比较常用的，也是希尔建议的增量，称为希尔增量，但其实这个增量序列不是最优的。此处我们做示例使用希尔增量。 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列 t1，t2，…，tk，其中 ti&gt;tj，tk=1； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 4.2 过程演示 4.3 代码实现/** * 希尔排序，关键是gap 就是间隔进行分组，然后组内进行插入排序 * 是插入排序的改良版，区别在于插入排序的preIndex = i - 1，也就是说间隔是1 * 改进的地方在于：如果最后一位是最小值，要找到合适的位置，需要和前面的n-1个数字依次比较。如果用希尔排序 * 间隔大，移动次数小。间隔小，移动距离短。 * 缺点：不稳定 * [8,9,1,7,2,3,5,4,6,0] */ public static void ShellSort(int[] array) { if (array == null || array.length &lt; 2) { return; } int len = array.length; int temp; for (int gap = len / 2; gap > 0; gap /= 2) { // 插入排序。 for (int i = gap; i &lt; len; i++) { // 右侧分组第一个数，即中位数gap右边元素 temp = array[i]; // 左侧分组第一个数 = i与中位数gap的差 = i自增的次数 = 左侧下标 // 固定距离的双指针，i和j的距离始终是gap，在循环中同步增长 int j = i - gap; while (j >= 0 &amp;&amp; array[j] > temp) { array[j + gap] = array[j]; j -= gap; } array[j + gap] = temp; } } System.out.println(\"希尔排序： \" + Arrays.toString(array)); } // 第二种写法： public static void ShellSort(int[] array) { if (array == null || array.length &lt; 2) { return; } int len = array.length; int gap = array.length / 2; // 不断缩小gap，直到1为止 while (gap > 0) { // j用来控制分组内多个元素时，比较的次数 for (int j = 0; (j + gap) &lt; len; j++) { // k左侧元素，k+gap=右侧元素。依次调整每个分组 for (int k = 0; (k + gap) &lt; len; k++) { if (arr[k] > array[k + gap]) { // 交换操作 array[k] = array[k] + array[k + gap]; array[k + gap] = array[k] - array[k + gap]; array[k] = array[k] - array[k + gap]; } } } gap /= 2; } System.out.println(\"希尔排序： \" + Arrays.toString(array)); } 4.4 算法分析*最佳情况： O(nlog n) 最坏情况： O(nlog n) 平均情况：O(nlog n) * 5.归并排序（Merge Sort）和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是 O(n log n）的时间复杂度。代价是需要额外的内存空间。 归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。归并排序是一种稳定的排序方法。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为 2 - 路归并。 5.1 算法描述 把长度为 n 的输入序列分成两个长度为 n/2 的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 5.2 动图演示 5.3 代码实现 申请空间用来存放合并后的序列，大小为两个已排序序列之和 设定两个指针 ，初始位置分别为两个已排序序列的起始位置 比较两个指针指向的元素，选择相对小的元素放入结果空间，并移动指针到下一个位置 重复步骤 3直到某一指针到达序列尾部 将另一序列剩下的所有元素直接合并到序列尾部 public static void sort(int[] array) { if (array == null || array.length &lt; 2) { return; } int[] result = MergeSort(array); System.out.println(\"归并排序： \" + Arrays.toString(result)); } /** * 归并排序 * 先分治，在合并 */ public static int[] MergeSort(int[] array) { // >> 1 等价于 /2 int mid = array.length >> 1; int[] left = Arrays.copyOfRange(array, 0, mid); int[] right = Arrays.copyOfRange(array, mid, array.length); return merge(MergeSort(left), MergeSort(right)); } /** * 将两段排序好的数组结合成一个排序数组 */ public static int[] merge(int[] left, int[] right) { int[] result = new int[left.length + right.length]; int i = 0, j = 0, k = 0; while (i &lt; left.length &amp;&amp; j &lt; right.length) { if (left[i] &lt;= right[j]) { result[k++] = left[i++]; }else { result[k++] = right[j++]; } } // left中的剩余元素移入结果数组 while (i &lt; left.length) { result[k++] = left[i++]; } // right中的剩余元素移入结果数组 while (j &lt; right.length) { result[k++] = right[j++]; } return result; } 5.4 算法分析最佳情况：O(nlog n) 最差情况：O(nlog n) 平均情况： O(nlog n) 空间复杂度：O(n) 6.快速排序（Quick Sort）快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行快速排序，以达到整个序列有序。 6.1 算法描述快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 6.2 动图演示 6.3 代码实现public static void Sort(int[] array) { if (array == null || array.length &lt; 2) { return; } int[] copyArray = Arrays.copyOf(array, array.length); QuickSort(array, 0, array.length - 1); System.out.println(\"快速排序-递归： \" + Arrays.toString(array)); QuickSortIterate(copyArray, 0, array.length - 1); System.out.println(\"快速排序-非递归： \" + Arrays.toString(copyArray)); } public static void QuickSort(int[] array, int left, int right) { if (left &lt; right) { // 每次递归确定一个元素的位置，这个元素在整个序列中已经是固定的。 int partitionIndex = swapPartition(array, left, right); // 左右子序列各自递归排序 QuickSort(array, left, partitionIndex - 1); QuickSort(array, partitionIndex + 1, right); } } /** * 交换法： * 1.右指针移动到比key小的元素位置，左指针移动到比key大的元素位置； * 2.交换左右指针位置的元素； * 3.重复，直到左右指针相遇，交换相遇位置元素和key，返回相遇位置下标（key在整个序列中应该在的位置） * 注意，右指针先移动，找到目标后停下来让左指针开始工作 */ public static int swapPartition(int[] array, int left, int right) { int key = left; // 循环结束的条件：左右指针相遇 while (left &lt; right) { // 目的是让右侧都大于key，所以 要找到比key小的 while (left &lt; right &amp;&amp; array[right] >= array[key]) { // 如果不小于key则向左移动指针 right--; } // 目的是让左侧都小于key，所以要找到比key大的 while (left &lt; right &amp;&amp; array[left] &lt;= array[key]) { // 如果不大于key则向右移动指针 left++; } // 交换遇到的元素，让应该在左边但是在右边的回到左边去，右边同理 swap(array, array[left], array[right]); } // 左右指针相遇，交换key和相遇位置元素，key的位置确定 swap(array, array[key], array[left]); // 返回相遇位置下标（key在整个序列中应该在的位置） return left; } /** * 挖坑法 * 单趟排序的思路是：取最左或最右边的元素为key，假设把这个位置“挖空”，让最右边的向左走，直到遇到比key小的数，将其放到key的位置，自己的位置变“空”。 * 每一次指针停下来时，只需要将当前位置元素与空位置交换。 * 直到pq相遇，那么这个位置就是最终的坑位，再将key填入这个坑位，就完成了一趟排序。 */ public static int waKengPartition(int[] array, int left, int right) { int key = left; int value = array[key]; while (left &lt; right) { while (left &lt; right &amp;&amp; array[right] >= value) { right--; } // 将right位置元素放到key位置，right位置置空。 array[key] = array[right]; key = right; while (left &lt; right &amp;&amp; array[left] &lt;= value) { left++; } // 将left位置元素放到key位置，left位置置空。 array[key] = array[left]; key = left; } array[key] = value; return key; } /** * 交换数组内两个元素 */ public static void swap(int[] array, int i, int j) { int temp = array[i]; array[i] = array[j]; array[j] = temp; } /** * 快速排序-非递归 */ public static void QuickSortIterate(int[] array, int left, int right) { LinkedList&lt;Integer> stack = new LinkedList(); stack.push(left); stack.push(right); while (!stack.isEmpty()) { right = stack.pop(); left = stack.pop(); // 对区间进行一趟排序，取得key值 int key = swapPartition(array, left, right); // 如果左区间可以再分，就入栈 if (left &lt; key - 1) { stack.push(left); stack.push(key - 1); } // 如果右区间可以再分，就入栈 if (right > key + 1) { stack.push(key + 1); stack.push(right); } } } TODO 快慢指针法 非递归实现 如何优化？ key的选择：三数取中 面对完全有序的数组，快速排序每趟排序后，key的位置都在边缘，每层递归都只能固定一个数，时间复杂度变成了O(N^2)。 面对这种情况，我们可以在取key时动手脚。每次取key我们不再只取最左或最右的值。而是对比最左、最右、中间的三个元素，取三个元素中，值在另外两者中间的元素作为key。这样，就打乱了有序数组，大大加快了快速排序在面对这种情况时的速度。 小区间优化 快速排序对一个元素不多的数组排序，仍需要进行多次递归调用，我们知道递归是比较消耗资源的，所以为了避免在快速排序递归的最后几层大量调用函数，我们可以在数组元素较少时不再递归，而是采用选择排序替代，这样就能在不损失多少速度的情况下减少大量的递归次数，达到优化速度的目的。 6.4 算法分析最佳情况： O(nlog n) 最差情况： O(n²) 平均情况： O(nlog n) 空间复杂度：O(1) 7.堆排序（Heap Sort）堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 7.1 算法描述 将初始待排序关键字序列 (R1,R2….Rn) 构建成大顶堆，此堆为初始的无序区；（一般升序采用大顶堆，降序采用小顶堆）； 将堆顶元素 R[1]与最后一个元素 R[n]交换，此时得到新的无序区 (R1,R2,……Rn-1) 和新的有序区(Rn，), 且满足 R[1,2…n-1]&lt;=R[n]； 恢复堆。由于交换后新的堆顶 R[1]可能违反堆的性质，因此需要对当前无序区 (R1,R2,……Rn-1) 调整为新堆，然后再次将 R[1]与无序区最后一个元素交换，得到新的无序区 (R1,R2….Rn-2) 和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为 n-1，则整个排序过程完成。 7.2 动图演示 7.3 代码实现注意：这里用到了完全二叉树的部分性质：详情见《数据结构二叉树知识点总结》 /** * 堆排序 * 构建大顶堆，交换堆顶元素与末尾元素，恢复大顶堆 * 每次将最大值“沉”到数组末端，升序 */ public static void heapSort(int[] array) { int len = array.length; // 1.构建大顶堆 // 从最后一个非叶子节点开始 for (int i = len / 2 - 1; i >= 0; i--) { // 第一个非叶子节点i，从下至上、从左至右调整结构 adjustHeap(array, i, len); } // 2.交换堆顶与末尾 + 调整堆结构 // j就是末尾元素 for (int j = len - 1; j > 0; j--) { // swap方法，交换操作 swap(array, i, j); // adjustHeap方法，重新调整堆结构 adjustHeap(array, i, j); } } /** * 调整大顶堆（仅是调整过程，建立在大顶堆已构建的基础上） */ public static void adjustHeap(int[] array, int i, int length) { // 以i节点为父节点，先将值放到temp保存 int temp = array[i]; // 从i节点的左子节点开始，找出最大值，放到父节点位置 for (int k = i*2+1; k &lt; length; k = k*2+1) { // 如果左子节点小于右子节点，k指向右子节点 if (k+1 &lt; length &amp;&amp; array[k] &lt; array[k + 1]) { k++; } // 如果子节点大于父节点，将子节点值赋给父节点，最终使i为最大值 // 注意，这里k覆盖了i位置元素 if (array[k] > temp) { array[i] = array[k]; i = k; }else { break; } } // 此时才真正确定i的位置，将保存的值还给i array[i] = temp; } /** * 交换元素 */ public static void swap(int[] array, int a, int b) { int temp = array[a]; array[a] = array[b]; array[b] = temp; } 7.4 算法分析由于堆排序中初始化堆的过程比较次数较多, 因此它不太适用于小序列。同时由于多次任意下标相互交换位置, 相同元素之间原本相对的顺序被破坏了, 因此, 它是不稳定的排序。 最佳情况： O(nlogn) 最差情况： O(nlogn) 平均情况： O(nlogn) 空间复杂度：O(1) 8.计数排序（Counting Sort）计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 计数排序 (Counting sort) 是一种稳定的排序算法。计数排序使用一个额外的数组 C，其中第 i 个元素是待排序数组 A 中值等于 i 的元素的个数。然后根据数组 C 来将 A 中的元素排到正确的位置。它只能对整数进行排序。 8.1 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为 i 的元素出现的次数，存入数组 C 的第 i 项； 对所有的计数累加（从 C 中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素 i 放在新数组的第 C(i) 项，每放一个元素就将 C(i) 减去 1。 8.2 动图演示 8.3 代码实现/** * 计数排序 */ public static int[] CountingSort(int[] array) { int bias; int min = array[0]; int max = array[0]; for (int i = 1; i &lt; array.length; i++) { if (array[i] > max) max = array[i]; if (array[i] &lt; min) min = array[i]; } bias = 0 - min; int[] bucket = new int[max - min + 1]; Arrays.fill(bucket, 0); for (int i = 0; i &lt; array.length; i++) { bucket[array[i] + bias]++; } int index = 0; int i = 0; while (index &lt; array.length) { if (bucket[i] != 0) { array[index] = i - bias; bucket[i]--; index++; } else { i++; } } System.out.println(\"计数排序： \" + Arrays.toString(array)); } 8.4 算法分析当输入的元素是 n 个 0 到 k 之间的整数时，它的运行时间是 O(n + k)。计数排序不是比较排序，排序的速度快于任何比较排序算法。由于用来计数的数组 C 的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上 1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。 最佳情况： O(n+k) 最差情况： O(n+k) 平均情况： O(n+k) 9.桶排序（Bucket Sort）桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。 桶排序 (Bucket sort) 的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排 9.1 算法描述 人为设置一个 BucketSize，作为每个桶所能放置多少个不同数值（例如当 BucketSize==5 时，该桶可以存放｛1,2,3,4,5｝这几种数字，但是容量不限，即可以存放 100 个 3）； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序，可以使用其它排序方法，也可以递归使用桶排序； 从不是空的桶里把排好序的数据拼接起来。 注意，如果递归使用桶排序为各个桶排序，则当桶数量为 1 时要手动减小 BucketSize 增加下一循环桶的数量，否则会陷入死循环，导致内存溢出。 9.2 图片演示 9.3 代码实现/** * 桶排序 */ public static void BucketSort(ArrayList&lt;Integer> array, int bucketSize) { if (array == null || array.length &lt; 2) { return; } int max = array.get(0), min = array.get(0); // 找到最大值最小值 for (int i = 0; i &lt; array.size(); i++) { if (array.get(i) > max) max = array.get(i); if (array.get(i) &lt; min) min = array.get(i); } int bucketCount = (max - min) / bucketSize + 1; ArrayList&lt;ArrayList&lt;Integer>> bucketArr = new ArrayList&lt;>(bucketCount); ArrayList&lt;Integer> resultArr = new ArrayList&lt;>(); for (int i = 0; i &lt; bucketCount; i++) { bucketArr.add(new ArrayList&lt;Integer>()); } for (int i = 0; i &lt; array.size(); i++) { bucketArr.get((array.get(i) - min) / bucketSize).add(array.get(i)); } for (int i = 0; i &lt; bucketCount; i++) { if (bucketSize == 1) { // 如果带排序数组中有重复数字时 感谢 @见风任然是风 朋友指出错误 for (int j = 0; j &lt; bucketArr.get(i).size(); j++) resultArr.add(bucketArr.get(i).get(j)); } else { if (bucketCount == 1) bucketSize--; ArrayList&lt;Integer> temp = BucketSort(bucketArr.get(i), bucketSize); for (int j = 0; j &lt; temp.size(); j++) resultArr.add(temp.get(j)); } } System.out.println(\"桶排序： \" + Arrays.toString(resultArr)); } 9.4 算法分析桶排序最好情况下使用线性时间 O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为 O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 *最佳情况： O(n+k) 最差情况： O(n+k) 平均情况： O(n^2) * 10.基数排序（Radix Sort）将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。 基数排序按照优先从高位或低位来排序有两种实现方案： MSD（Most significant digital） 从最左侧高位开始进行排序。先按k1排序分组, 同一组中记录, 关键码k1相等, 再对各组按k2排序分成子组, 之后, 对后面的关键码继续这样的排序分组, 直到按最次位关键码kd对各子组排序后. 再将各组连接起来, 便得到一个有序序列。MSD方式适用于位数多的序列。 LSD（Least significant digital） 从最右侧低位开始进行排序。先从kd开始排序，再对kd-1进行排序，依次重复，直到对k1排序后便得到一个有序序列。LSD方式适用于位数少的序列。 基数排序基于分别排序，分别收集，不改变相同元素之间的相对顺序，所以是稳定的。 下面以LSD为例。 10.1 算法描述 取得数组中的最大数，并取得位数； arr 为原始数组，从最低位开始取每个位组成 radix 数组； 对 radix 进行计数排序（利用计数排序适用于小范围数的特点）； 10.2 动图演示 10.3 代码实现/** * 基数排序 * @param array * @return */ public static void RadixSort(int[] array) { if (array == null || array.length &lt; 2) { return; } // 1.先算出最大数的位数； int max = array[0]; for (int i = 1; i &lt; array.length; i++) { if (a[i] > max) { max = a[i]; } } int maxDigit = 0; while (max != 0) { max /= 10; maxDigit++; } int[][] buckets = new int[10][a.length]; int base = 10; //从低位到高位，对每一位遍历，将所有元素分配到桶中 for (int i = 0; i &lt; maxDigit; i++) { //存储各个桶中存储元素的数量 int[] bucketLen = new int[10]; //收集：将不同桶里数据挨个捞出来,为下一轮高位排序做准备,由于靠近桶底的元素排名靠前,因此从桶底先捞 for (int j = 0; j &lt; a.length; j++) { int whichBucket = (a[j] % base) / (base / 10); buckets[whichBucket][bucketLen[whichBucket]] = a[j]; bucketLen[whichBucket]++; } int k = 0; //收集：将不同桶里数据挨个捞出来,为下一轮高位排序做准备,由于靠近桶底的元素排名靠前,因此从桶底先捞 for (int l = 0; l &lt; buckets.length; l++) { for (int m =0; m &lt; bucketLen[l]; m++) { a[k++] = buckets[l][m]; } } System.out.println(\"Sorting: \" + Arrays.toString(a)); base *= 10; } System.out.println(\"基数排序： \" + Arrays.toString(array)); } 10.4 算法分析最佳情况：O(d(n+r)) 最差情况：O(d(n+r)) 平均情况：O(d*(n+r)) 空间复杂度: O(n+r) 其中，d 为位数，r 为基数，n 为原数组个数。在基数排序中，因为没有比较操作，所以在复杂上，最好的情况与最坏的情况在时间上是一致的，均为 O(d*(n + r))。 基数排序更适合用于对时间, 字符串等这些整体权值未知的数据进行排序，适用于。 (1)数据范围较小，建议在小于1000 (2)每个数值都要大于等于0 基数排序 vs 计数排序 vs 桶排序 这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异： 基数排序：根据键值的每位数字来分配桶 计数排序：每个桶只存储单一键值 桶排序：每个桶存储一定范围的数值 参考资料：https://juejin.im/post/5b95da8a5188255c775d8124 https://www.cnblogs.com/guoyaohua/p/8600214.html https://www.cnblogs.com/Young111/p/11300929.html 快速排序详解","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"排序算法","slug":"排序算法","permalink":"https://topone233.github.io/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"反射、注解和动态代理","slug":"反射、注解和动态代理","date":"2020-07-20T08:48:44.137Z","updated":"2023-02-06T11:44:17.943Z","comments":true,"path":"2020/07/20/反射、注解和动态代理/","link":"","permalink":"https://topone233.github.io/2020/07/20/%E5%8F%8D%E5%B0%84%E3%80%81%E6%B3%A8%E8%A7%A3%E5%92%8C%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","excerpt":"","text":"原文地址 一、Java 反射机制及基本用法反射是指计算机程序在运行时访问、检测和修改它本身状态或行为的一种能力，是一种元编程语言特性，有很多语言都提供了对反射机制的支持，它使程序能够编写程序。Java 的反射机制使得 Java 能够动态的获取类的信息和调用对象的方法。反射有时也被称为内省，这两个词汇都隐喻了“让类型自我审视并提供自身的描述信息”。 在 Java 中，Class（类类型）是反射编程的起点，代表运行时类型信息（RTTI，Run-Time Type Identification）。java.lang.reflect 包含了 Java 支持反射的主要组件，Field提供了有关类和接口的属性信息以及对它的动态访问权限；Constructor提供关于类的单个构造方法的信息以及它的访问权限；Method提供类或接口中某个方法的信息。它们的关系如下图所示。 Constructor 和 Method 与 Field 的区别在于前者继承自抽象类 Executable，是可以在运行时动态调用的，而 Field 仅仅具备可访问的特性，且默认为不可访问。下面了解下它们的基本用法： 获取 Class 对象的三种方式静态方法 Class.forName 适合于已知类的全路径名，典型应用如加载 JDBC 驱动。对同一个类，不同方式获得的 Class 对象是相同的。 // 1. 采用静态方法Class.forName(全限类名)获取类的Class对象。最安全/性能最好/最常用 Class clazz0 = Class.forName(&quot;com.yhthu.java.ClassTest&quot;); System.out.println(&quot;clazz0:&quot; + clazz0); // 2. 采用类字面常量（类的名字.class） Class clazz1 = ClassTest.class; System.out.println(&quot;clazz1:&quot; + clazz1); // 3. 采用对象的getClass方法获取它的Class对象 ClassTest classTest = new ClassTest(); Class clazz2 = classTest.getClass(); System.out.println(&quot;clazz2:&quot; + clazz2); // 4. 判断Class对象是否相同 System.out.println(&quot;Class对象是否相同:&quot; + ((clazz0.equals(clazz1)) &amp;&amp; (clazz1.equals(clazz2)))); 注意：三种方式获取的 Class 对象相同的前提是使用了相同的类加载器，比如上述代码中默认采用应用程序类加载器（sun.misc.Launcher$AppClassLoader）。不同类加载器加载的同一个类，也会获取不同的 Class 对象： // 自定义类加载器 ClassLoader myLoader = new ClassLoader() { @Override public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException { try { String fileName = name.substring(name.lastIndexOf(&quot;.&quot;) + 1) + &quot;.class&quot;; InputStream is = getClass().getResourceAsStream(fileName); if (is == null) { return super.loadClass(name); } byte[] b = new byte[is.available()]; is.read(b); return defineClass(name, b, 0, b.length); } catch (IOException e) { throw new ClassNotFoundException(name); } } }; // 采用自定义类加载器加载 Class clazz3 = Class.forName(&quot;com.yhthu.java.ClassTest&quot;, true, myLoader); // clazz0与clazz3并不相同 System.out.println(&quot;Class对象是否相同:&quot; + clazz0.equals(clazz3));查看类信息通过 Class 的 getDeclaredXxxx 和 getXxx 方法获取构造器、方法和域对象。 两者的区别在于： 前者返回的是当前 Class 对象申明的构造器、方法和域，包含修饰符为 private 的； 后者只返回修饰符为 public 的构造器、方法和域，但包含从基类中继承的。 // 返回申明为public的方法，包含从基类中继承的 for (Method method: String.class.getMethods()) { System.out.println(method.getName()); } // 返回当前类申明的所有方法，包含private的 for (Method method: String.class.getDeclaredMethods()) { System.out.println(method.getName()); } Fields、Constructors与methods类似，省略。 利用反射动态创建对象实例 通过 Class 的 newInstance 方法和 Constructor 的 newInstance 方法方法均可新建类型为 Class 的对象； 通过 Method 的 invoke 方法可以在运行时动态调用该方法； 通过 Field 的 set 方法可以在运行时动态改变域的值，但需要首先设置其为可访问（setAccessible）。 二、注解注解（Annotation）是 Java5 引入的一种代码辅助工具，它的核心作用是对类、方法、变量、参数和包进行标注，通过反射来访问这些标注信息，以此在运行时改变所注解对象的行为。Java 中的注解由内置注解和元注解组成。内置注解主要包括： @Override - 检查该方法是否是重载方法。如果发现其父类，或者是引用的接口中并没有该方法时，会报编译错误。 @Deprecated - 标记过时方法。如果使用该方法，会报编译警告。 @SuppressWarnings - 指示编译器去忽略注解中声明的警告。 @SafeVarargs - Java 7 开始支持，忽略任何使用参数为泛型变量的方法或构造函数调用产生的警告。 @FunctionalInterface - Java 8 开始支持，标识一个匿名函数或函数式接口。 这里，我们重点关注元注解，元注解位于 java.lang.annotation 包中，主要用于自定义注解。元注解包括： @Retention - 标识这个注解怎么保存，是只在代码中，还是编入 class 文件中，或者是在运行时可以通过反射访问，枚举类型分为别 SOURCE、CLASS 和 RUNTIME； @Documented - 标记这些注解是否包含在用户文档中。 @Target - 标记这个注解应该是哪种 Java 成员，枚举类型包括 TYPE、FIELD、METHOD、CONSTRUCTOR 等； @Inherited - 标记这个注解可以继承超类注解，即子类 Class 对象可使用 getAnnotations() 方法获取父类被 @Inherited 修饰的注解，这个注解只能用来申明类。 @Repeatable - Java 8 开始支持，标识某注解可以在同一个声明上使用多次。 自定义元注解需重点关注两点：1）注解的数据类型；2）反射获取注解的方法。首先，注解中的方法并不支持所有的数据类型，仅支持八种基本数据类型、String、Class、enum、Annotation 和它们的数组。比如以下代码会产生编译时错误： @Documented @Inherited @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) public @interface AnnotationTest { // 1. 注解数据类型不能是Object；2. 默认值不能为null Object value() default null; // 支持的定义方式 String value() default &quot;&quot;; }其次，上节中提到的反射相关类（Class、Constructor、Method 和 Field）和 Package 均实现了 AnnotatedElement 接口，该接口定义了访问反射信息的方法，主要如下： // 获取指定注解类型 getAnnotation(Class&lt;T&gt;):T; // 获取所有注解，包括从父类继承的 getAnnotations():Annotation[]; // 获取指定注解类型，不包括从父类继承的 getDeclaredAnnotation(Class&lt;T&gt;):T // 获取所有注解，不包括从父类继承的 getDeclaredAnnotations():Annotation[]; // 判断是否存在指定注解 isAnnotationPresent(Class&lt;? extends Annotation&gt;:boolean当使用上例中的 AnnotationTest 标注某个类后，便可在运行时通过该类的反射方法访问注解信息了。 @AnnotationTest(&quot;yhthu&quot;) public class AnnotationReflection { public static void main(String[] args) { AnnotationReflection ar = new AnnotationReflection(); Class clazz = ar.getClass(); // 判断是否存在指定注解 if (clazz.isAnnotationPresent(AnnotationTest.class)) { // 获取指定注解类型 Annotation annotation = clazz.getAnnotation(AnnotationTest.class); // 获取该注解的值 System.out.println(((AnnotationTest) annotation).value()); } } } 当自定义注解只有一个方法 value() 时，使用注解可只写值，例如：@AnnotationTest(“yhthu”) 三、动态代理参考上一篇：动态代理 代理是一种结构型设计模式，当无法或不想直接访问某个对象，或者访问某个对象比较复杂的时候，可以通过一个代理对象来间接访问，代理对象向客户端提供和真实对象同样的接口功能。经典设计模式中，代理模式有四种角色： Subject 抽象主题类——申明代理对象和真实对象共同的接口方法； RealSubject 真实主题类——实现了 Subject 接口，真实执行业务逻辑的地方； ProxySubject 代理类——实现了 Subject 接口，持有对 RealSubject 的引用，在实现的接口方法中调用 RealSubject 中相应的方法执行； Cliect 客户端类——使用代理对象的类。 在实现上，代理模式分为静态代理和动态代理，静态代理的代理类二进制文件是在编译时生成的，而动态代理的代理类二进制文件是在运行时生成并加载到虚拟机环境的。JDK 提供了对动态代理接口的支持，开源的动态代理库（Cglib、Javassist 和 Byte Buddy）提供了对接口和类的代理支持，本节将简单比较 JDK 和 Cglib 实现动态代理的异同，后续章节会对 Java 字节码编程做详细分析。 3.1 JDK 动态代理接口JDK 实现动态代理是通过 Proxy 类的 newProxyInstance 方法实现的，该方法的三个入参分别表示： public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) ClassLoader loader，定义代理生成的类的加载器，可以自定义类加载器，也可以复用当前 Class 的类加载器； Class&lt;?&gt;[] interfaces，定义代理对象需要实现的接口； InvocationHandler h，定义代理对象调用方法的处理，其 invoke 方法中的 Object proxy 表示生成的代理对象，Method 表示代理方法， Object[] 表示方法的参数。 通常的使用方法如下： private Object getProxy() { return Proxy.newProxyInstance(JDKProxyTest.class.getClassLoader(), new Class&lt;?&gt;[]{Subject.class}, new MyInvocationHandler(new RealSubject())); } private static class MyInvocationHandler implements InvocationHandler { private Object realSubject; public MyInvocationHandler(Object realSubject) { this.realSubject = realSubject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;Some thing before method invoke&quot;); Object result = method.invoke(realSubject, args); System.out.println(&quot;Some thing after method invoke&quot;); return result; } }类加载器采用当前类的加载器，默认为应用程序类加载器（sun.misc.Launcher$AppClassLoader）；接口数组以 Subject.class 为例，调用方法处理类 MyInvocationHandler 实现 InvocationHandler 接口，并在构造器中传入 Subject 的真正的业务功能服务类 RealSubject，在执行 invoke 方法时，可以在实际方法调用前后织入自定义的处理逻辑，这也就是 AOP（面向切面编程）的原理。关于 JDK 动态代理，有两个问题需要清楚： Proxy.newProxyInstance 的代理类是如何生成的？Proxy.newProxyInstance 生成代理类的核心分成两步： // 1. 获取代理类的Class对象 Class&lt;?&gt; cl = getProxyClass0(loader, intfs); // 2. 利用Class获取Constructor，通过反射生成对象 cons.newInstance(new Object[]{h});与反射获取 Class 对象时搜索 classpath 路径的. class 文件不同的是，这里的 Class 对象完全是 “无中生有” 的。getProxyClass0 根据类加载器和接口集合返回了 Class 对象，这里采用了缓存的处理。 // 缓存(key, sub-key) -&gt; value，其中key为类加载器，sub-key为代理的接口，value为Class对象 private static final WeakCache&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(), new ProxyClassFactory()); // 如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成 private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) { if (interfaces.length &gt; 65535) { throw new IllegalArgumentException(&quot;interface limit exceeded&quot;); } return proxyClassCache.get(loader, interfaces); }如果实现了代理接口的类已存在就返回缓存对象，否则就通过 ProxyClassFactory 生成。ProxyClassFactory 又是通过下面的代码生成 Class 对象的。 // 生成代理类字节码文件 byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags); try { // defineClass0为native方法，生成Class对象 return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); } catch (ClassFormatError e) { throw new IllegalArgumentException(e.toString()); }generateProxyClass 方法是用来生成字节码文件的，根据生成的字节码文件，再在 native 层生成 Class 对象。 InvocationHandler 的 invoke 方法是怎样调用的？回答这个问题得先看下上面生成的 Class 对象究竟是什么样的，将 ProxyGenerator 生成的字节码保存成文件，然后反编译打开（IDEA 直接打开），可见生成的 Proxy.class 主要包含 equals、toString、hashCode 和代理接口的 request 方法实现。 public final class $Proxy extends Proxy implements Subject { // m1 = Object的equals方法 private static Method m1; // m2 = Object的toString方法 private static Method m2; // Subject的request方法 private static Method m3; // Object的hashCode方法 private static Method m0; // 省略m1/m2/m0，此处只列出request方法实现 public final void request() throws { try { super.h.invoke(this, m3, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } }由于生成的代理类继承自 Proxy，super.h 即是 Prxoy 的 InvocationHandler，即代理类的 request 方法直接调用了 InvocationHandler 的实现，这就回答了 InvocationHandler 的 invoke 方法是如何被调用的了。 3.2 Cglib 动态代理接口和类Cglib 的动态代理是通过 Enhancer 类实现的，其 create 方法生成动态代理的对象，有五个重载方法： create():Object create(Class, Callback):Object create(Class, Class[], Callback):Object create(Class, Class[], CallbackFilter, Callback):Object create(Class[], Object):Object常用的是第二个和第三个方法，分别用于动态代理类和动态代理接口，其使用方法如下： private Object getProxy() { // 1. 动态代理类 return Enhancer.create(RealSubject.class, new MyMethodInterceptor()); // 2. 动态代理接口 return Enhancer.create(Object.class, new Class&lt;?&gt;[]{Subject.class}, new MyMethodInterceptor()); } private static class MyMethodInterceptor implements MethodInterceptor { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(&quot;Some thing before method invoke&quot;); Object result = proxy.invokeSuper(obj, args); System.out.println(&quot;Some thing after method invoke&quot;); return result; } }从上小节可知，JDK 只能代理接口，代理生成的类实现了接口的方法；而 Cglib 是通过继承被代理的类、重写其方法来实现的，如：create 方法入参的第一个参数就是被代理类的类型。当然，Cglib 也能代理接口，比如 getProxy() 方法中的第二种方式。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"动态代理","slug":"动态代理","date":"2020-07-20T07:51:44.059Z","updated":"2023-02-06T11:43:55.134Z","comments":true,"path":"2020/07/20/动态代理/","link":"","permalink":"https://topone233.github.io/2020/07/20/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","excerpt":"","text":"原文地址 代理模式为其他对象提供一个代理以控制对某个对象的访问。代理类主要负责为委托了（真实对象）预处理：消息、过滤消息、传递消息给委托类，代理类不现实具体服务，而是利用委托类来完成服务，并将执行结果封装处理。类似于生活中的中介。 有一个打印机的类 public class Printer { public void print(){ System.out.println(&quot;打印！&quot;); } }我想在打印之前先记录一下日志怎么做？ 最简单的方法：在打印的功能前面直接加上记录日志的功能。 public class Printer { public void print(){ System.out.println(&quot;记录日志！&quot;); System.out.println(&quot;打印！&quot;); } }看上去好像没有问题，但是我们修改了打印机的源代码，破坏了面向对象的开闭原则，有可能影响到其它功能。怎么解决呢？很容易可以想到，既然不能修改原来的代码，那我新建一个类吧。 public class LogPrinter extends Printer { public void print(){ System.out.println(&quot;记录日志！&quot;); System.out.println(&quot;打印！&quot;); } }这个类继承了打印机的类，重写了打印机的 print 方法，提供了记录日志的功能，以后需要打印机的时候使用这个类就好。问题似乎得到了解决，我们可以在这个解决方案的基础上进一步的优化： 静态代理先抽象出一个接口: public interface IPrinter { void print(); }打印机类（被代理类）实现这个接口: public class Printer implements IPrinter { @Override public void print(){ System.out.println(&quot;打印！&quot;); } }创建打印机代理类也实现该接口 被代理类被传递给了代理类PrinterProxy，代理类在执行具体方法时通过所持用的被代理类完成调用。 在构造函数中将打印机对象传进去，实现接口的打印方法时调用打印机对象的打印方法并在前面加上记录日志的功能: public class PrinterProxy implements IPrinter { private Printer printer; public PrinterProxy(){ this.printer = new printer(); } @Override public void print() { System.out.println(&quot;记录日志&quot;); printer.print(); } }试一把： public class Test { public static void main(String[] args) { PrinterProxy proxy = new PrinterProxy(); proxy.print(); } }结果出来了： 记录日志 打印以后我们就可以直接实例化 PrinterProxy 对象调用它的打印方法了，这就是静态代理模式，通过抽象出接口让程序的扩展性和灵活性更高了。 静态代理的缺点静态代理是完美无缺的吗？ 考虑一下，如果我的打印机类中还有别的方法，也需要加上记录日志的功能，但是静态代理只能为一个类服务，就不得不将记录日志的功能写 n 遍。进一步如果我还有电视机，电冰箱的类里面的所有方法也需要加上记录日志的功能，那要重复的地方就更多了。 怎么办？ 动态代理闪亮登场： 动态代理要想不重复写记录日志的功能，针对每一个接口实现一个代理类的做法肯定不可行了，可不可以让这些代理类的对象自动生成呢？ 利用反射机制在运行时创建代理类。 Jdk 提供了 invocationHandler 接口和 Proxy 类，借助这两个工具可以达到我们想要的效果。 invocationHandler 接口上场： //Object proxy:被代理的对象 //Method method:要调用的方法 //Object[] args:方法调用时所需要参数 public interface InvocationHandler { public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; }接口里只有一个方法 invoke，这个方法非常重要，先混个脸熟，稍后解释。 Proxy 类上场，它里面有一个很重要的方法 newProxyInstance： //CLassLoader loader:被代理对象的类加载器 //Class&lt;?&gt; interfaces:被代理类全部的接口 //InvocationHandler h:实现InvocationHandler接口的对象 public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException 调用 Proxy 的 newProxyInstance 方法可以生成代理对象 一切准备就绪动态代理模式千呼万唤始出来： 接口 IPrinter 和 该接口的实现类 Printer 的代码同前。 实现一个类，该类用来创建代理对象，_它实现了_InvocationHandler 接口： public class ProxyHandler implements InvocationHandler { private Object targetObject;//被代理的对象 //将被代理的对象传入获得它的类加载器和实现接口作为Proxy.newProxyInstance方法的参数。 public Object newProxyInstance(Object targetObject){ this.targetObject = targetObject; //targetObject.getClass().getClassLoader()：被代理对象的类加载器 //targetObject.getClass().getInterfaces()：被代理对象的实现接口 //this 当前对象，该对象实现了InvocationHandler接口所以有invoke方法，通过invoke方法可以调用被代理对象的方法 return Proxy.newProxyInstance(targetObject.getClass().getClassLoader(),targetObject.getClass().getInterfaces(),this); } //该方法在代理对象调用方法时调用 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;记录日志&quot;); return method.invoke(targetObject,args); } }被代理的对象 targetObject 可以通过方法参数传进来： public Object newProxyInstance(Object targetObject){ this.targetObject=targetObject;我们重点来分析一下这段代码： return Proxy.newProxyInstance(targetObject.getClass().getClassLoader(),targetObject.getClass().getInterfaces(),this);动态代理对象就是通过调用这段代码被创建并返回的。 方法有三个参数： 第一个参数： targetObject.getClass().getClassLoader()：targetObject 对象的类加载器。 第二个参数: targetObject.getClass().getInterfaces()：targetObject 对象的所有接口 第三个参数: this：也就是当前对象即实现了 InvocationHandler 接口的类的对象，在调用方法时会调用它的 invoke 方法。 再来看一下这段代码： public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { //在这里可以通过判断方法名来决定执行什么功能 System.out.println(&quot;记录日志&quot;); //调用被代理对象的方法 return method.invoke(targetObject, args); }这个方法就是生成的代理类中的方法被调用时会去自动调用的方法，可以看到在这个方法中调用了被代理对象的方法: method.invoke(targetObject, args); 我们可以在这里加上需要的业务逻辑，比如调用方法前记录日志功能. 见证奇迹的时刻到了： public class Test { public static void main(String[] args){ ProxyHandler proxyHandler=new ProxyHandler(); IPrinter printer = (IPrinter) proxyHandler.newProxyInstance(new Printer()); printer.print(); } }打印结果： 记录日志 打印当执行 printer.print(); 时会自动调用 invoke 方法，很多初学者不理解为什么能调用这个方法，回忆一下创建代理对象的时候是通过 return Proxy.newProxyInstance(targetObject.getClass().getClassLoader(),targetObject.getClass().getInterfaces(),this);来创建的，方法的第三个参数 this 是实现了 InvocationHandler 接口的对象， InvocationHandler 接口有 invoke 方法。现在有点思路了吧~ 将被代理的对象作为参数传入就可以执行里面的任意方法，所有的方法调用都通过 invoke 来完成。不用对每个方法进行处理，动态代理是不是很简洁。 动态代理的优势 Proxy 类的代码量被固定下来，不会因为业务的逐渐庞大而庞大； 可以实现 AOP 编程，实际上静态代理也可以实现，总的来说，AOP 可以算作是代理模式的一个典型应用； 解耦，通过参数就可以判断真实类，不需要事先实例化，更加灵活多变。 复习对象的创建很多初学 Java 的朋友眼中创建对象的过程 实际上可以换个角度，也说得通 所谓的 Class 对象，是 Class 类的实例，而 Class 类是描述所有类的，比如 Person 类，Student 类 可以看出，要创建一个实例，最关键的就是得到对应的 Class 对象。只不过对于初学者来说，new 这个关键字配合构造方法，实在太好用了，底层隐藏了太多细节，一句 Person p = new Person(); 直接把对象返回给你了。我自己刚开始学 Java 时，也没意识到 Class 对象的存在。 分析到这里，貌似有了思路： 能否不写代理类，而直接得到代理 Class 对象，然后根据它创建代理实例（反射) ? Class 对象包含了一个类的所有信息，比如构造器、方法、字段等。如果我们不写代理类，这些信息从哪获取呢？苦思冥想，突然灵光一现：代理类和目标类理应实现同一组接口。之所以实现相同接口，是为了尽可能保证代理对象的内部结构和目标对象一致，这样我们对代理对象的操作最终都可以转移到目标对象身上，代理对象只需专注于增强代码的编写。还是上面这幅图： 所以，可以这样说：接口拥有代理对象和目标对象共同的类信息。所以，我们可以从接口那得到理应由代理类提供的信息。但是别忘了，接口是无法创建对象的，怎么办？ JDK 提供了 java.lang.reflect.InvocationHandler 接口和 java.lang.reflect.Proxy 类，这两个类相互配合，入口是 Proxy，所以我们先聊它。 ProxyProxy 有个静态方法：getProxyClass(ClassLoader, interfaces)，只要你给它传入类加载器和一组接口，它就给你返回代理 Class 对象。 用通俗的话说，getProxyClass() 这个方法，会从你传入的接口 Class 中，“拷贝” 类结构信息到一个新的 Class 对象中，但新的 Class 对象带有构造器，是可以创建对象的。打个比方，一个大内太监（接口 Class），空有一身武艺（类信息），但是无法传给后人。现在江湖上有个妙手神医（Proxy 类），发明了克隆大法（getProxyClass），不仅能克隆太监的一身武艺，还保留了小 DD（构造器）…（这到底是道德の沦丧，还是人性的扭曲，欢迎走进动态代理） 所以，一旦我们明确接口，完全可以通过接口的 Class 对象，创建一个代理 Class，通过代理 Class 即可创建代理对象。 所以，按我理解，Proxy.getProxyClass() 这个方法的本质就是：以 Class 造 Class。 有了 Class 对象，就很好办了，具体看代码： 完美。 根据代理 Class 的构造器创建对象时，需要传入 InvocationHandler。每次调用代理对象的方法，最终都会调用 InvocationHandler 的 invoke() 方法： 怎么做到的呢？ 上面不是说了吗，根据代理 Class 的构造器创建对象时，需要传入 InvocationHandler。通过构造器传入一个引用，那么必然有个成员变量去接收。没错，代理对象的内部确实有个成员变量 invocationHandler，而且代理对象的每个方法内部都会调用 handler.invoke()！InvocationHandler 对象成了代理对象和目标对象的桥梁，不像静态代理这么直接。 大家仔细看上图右侧的动态代理，我在 invocationHandler 的 invoke() 方法中并没有写目标对象。因为一开始 invocationHandler 的 invoke() 里确实没有目标对象，需要我们手动 new。 但这种写法不够优雅，属于硬编码。我这次代理 A 对象，下次想代理 B 对象还要进来改 invoke() 方法，太差劲了。改进一下，让调用者把目标对象作为参数传进来： public class ProxyTest { public static void main(String[] args) throws Throwable { CalculatorImpl target = new CalculatorImpl(); //传入目标对象 //目的：1.根据它实现的接口生成代理对象 2.代理对象调用目标对象方法 Calculator calculatorProxy = (Calculator) getProxy(target); calculatorProxy.add(1, 2); calculatorProxy.subtract(2, 1); } private static Object getProxy(final Object target) throws Exception { //参数1：随便找个类加载器给它， 参数2：目标对象实现的接口，让代理对象实现相同接口 Class proxyClazz = Proxy.getProxyClass(target.getClass().getClassLoader(), target.getClass().getInterfaces()); Constructor constructor = proxyClazz.getConstructor(InvocationHandler.class); Object proxy = constructor.newInstance(new InvocationHandler() { @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(method.getName() + &quot;方法开始执行...&quot;); Object result = method.invoke(target, args); System.out.println(result); System.out.println(method.getName() + &quot;方法执行结束...&quot;); return result; } }); return proxy; } } 这样就非常灵活，非常优雅了。无论现在系统有多少类，只要你把实例传进来，getProxy() 都能给你返回对应的代理对象。就这样，我们完美地跳过了代理类，直接创建了代理对象！ 不过实际编程中，一般不用 getProxyClass()，而是使用 Proxy 类的另一个静态方法：Proxy.newProxyInstance()，直接返回代理实例，连中间得到代理 Class 对象的过程都帮你隐藏： public class ProxyTest { public static void main(String[] args) throws Throwable { CalculatorImpl target = new CalculatorImpl(); Calculator calculatorProxy = (Calculator) getProxy(target); calculatorProxy.add(1, 2); calculatorProxy.subtract(2, 1); } private static Object getProxy(final Object target) throws Exception { Object proxy = Proxy.newProxyInstance( target.getClass().getClassLoader(),/*类加载器*/ target.getClass().getInterfaces(),/*让代理对象和目标对象实现相同接口*/ new InvocationHandler(){/*代理对象的方法最终都会被JVM导向它的invoke方法*/ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(method.getName() + &quot;方法开始执行...&quot;); Object result = method.invoke(target, args); System.out.println(result); System.out.println(method.getName() + &quot;方法执行结束...&quot;); return result; } } ); return proxy; } } 现在，我想应该能看懂动态代理了。 最后讨论一下代理对象是什么类型。 首先，请区分两个概念：代理 Class 对象和代理对象。 单从名字看，代理 Class 和 Calculator 的接口确实相去甚远，但是我们却能将代理对象赋值给接口类型： 千万别觉得名字奇怪，就怀疑它不能用接口接收，只要实现该接口就是该类型。 代理对象的本质就是：和目标对象实现相同接口的实例。代理 Class 可以叫任何名字，whatever，只要它实现某个接口，就能成为该接口类型。 我写了一个 MyProxy 类，那么它的 Class 名字必然叫 MyProxy。但这和能否赋值给接口没有任何关系。由于它实现了 Serializable 和 Collection，所以 myProxy（代理实例）同时是这两个接口的类型。 小结我想了个很骚的比喻，希望能解释清楚： 接口 Class 对象是大内太监，里面的方法和字段比做他的一身武艺，但是他没有小 DD（构造器），所以不能 new 实例。一身武艺后继无人。 那怎么办呢？ 正常途径（implements）： 写一个类，实现该接口。这个就相当于大街上拉了一个人，认他做干爹。一身武艺传给他，只是比他干爹多了小 DD，可以 new 实例。 非正常途径（动态代理）： 通过妙手圣医 Proxy 的克隆大法（Proxy.getProxyClass()），克隆一个 Class，但是有小 DD。所以这个克隆人 Class 可以创建实例，也就是代理对象。 代理 Class 其实就是附有构造器的接口 Class，一样的类结构信息，却能创建实例。 JDK 动态代理生成的实例 CGLib 动态代理生成的实例 如果说继承的父类是亲爹（只有一个），那么实现的接口是干爹（可以有多个）。 实现接口是一个类认干爹的过程。接口无法创建对象，但实现该接口的类可以。 比如 class Student extends Person implements A, B这个类 new 一个实例出来，你问它：你爸爸是谁啊？它会告诉你：我只有一个爸爸 Person。 但是 student instanceof A interface，或者 student instanceof B interface，它会告诉你两个都是它干爹（true），都可以用来接收它。 然而，凡是有利必有弊。 也就是说，动态代理生成的代理对象，最终都可以用接口接收，和目标对象一起形成了多态，可以随意切换展示不同的功能。但是切换的同时，只能使用该接口定义的方法。 类加载器初学者可能对诸如 “字节码文件”、Class 对象比较陌生。所以这里花一点点篇幅介绍一下类加载器的部分原理。如果我们要定义类加载器，需要继承 ClassLoader 类，并覆盖 findClass() 方法： @Override public Class&lt;?&gt; findClass(String name) throws ClassNotFoundException { try { /*自己另外写一个getClassData() 通过IO流从指定位置读取xxx.class文件得到字节数组*/ byte[] datas = getClassData(name); if(datas == null) { throw new ClassNotFoundException(&quot;类没有找到：&quot; + name); } //调用类加载器本身的defineClass()方法，由字节码得到Class对象 return this.defineClass(name, datas, 0, datas.length); } catch (IOException e) { e.printStackTrace(); throw new ClassNotFoundException(&quot;类找不到：&quot; + name); } } 所以，这就是类加载之所以能把 xxx.class 文件加载进内存，并创建对应 Class 对象的深层原因。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"动态代理","slug":"动态代理","permalink":"https://topone233.github.io/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"}]},{"title":"MYSQL","slug":"MYSQL","date":"2020-07-15T16:00:00.000Z","updated":"2023-02-06T11:48:38.833Z","comments":true,"path":"2020/07/16/MYSQL/","link":"","permalink":"https://topone233.github.io/2020/07/16/MYSQL/","excerpt":"","text":"原文地址 [www.cnblogs.com\\](https://www.cnblogs.com/Young111/p/9598728.html) 数据库 数据库概述1. 什么是数据库 数据库就是存储数据的仓库，其本质是一个文件系统，数据按照特定的格式将数据存储起来，用户可以对数据库中的数据进行增加，修改，删除及查询操作。 2. 什么是数据库管理系统 数据库管理系统（DataBase Management System，DBMS）：指一种操作和管理数据库的大型软件，用于建立、使用和维护数据库，对数据库进行统一管理和控制，以保证数据库的安全性和完整性。用户通过数据库管理系统访问数据库中表内的数据。 常见的数据库管理系统 MYSQL ：开源免费的数据库，小型的数据库. 已经被 Oracle 收购了. MySQL6.x 版本也开始收费。 Oracle ：收费的大型数据库，Oracle 公司的产品。Oracle 收购 SUN 公司，收购 MYSQL。 DB2 ：IBM 公司的数据库产品, 收费的。常应用在银行系统中. SQLServer：MicroSoft 公司收费的中型的数据库。C#、.net 等语言常使用。 SyBase ：已经淡出历史舞台。提供了一个非常专业数据建模的工具 PowerDesigner。 SQLite : 嵌入式的小型数据库，应用在手机端。 Java 相关的数据库：MYSQL，Oracle． 这里使用 MySQL 数据库。MySQL 中可以有多个数据库，数据库是真正存储数据的地方。 数据库与数据库管理系统的关系 mysqlmysql 的管理安装linux: \\--yum -y install mariadb mariadb-server OR --yum -y install mysql mysql-server window \\--http://dev.mysql.com/downloads/mysql/ 启动\\--service mysqld start #开启 --chkconfig mysqld on #设置开机自启 OR --systemctl start mariadb --systemctl enable mariadb 查看\\-- ps aux |grep mysqld #查看进程 -- netstat -an |grep 3306 #查看端口 设置密码\\-- mysqladmin -uroot password &#39;123&#39; #设置初始密码，初始密码为空因此-p选项没有用 -- mysqladmin -u root -p123 password &#39;1234&#39; #修改root用户密码 登录\\-- mysql #本地登录，默认用户root，空密码，用户为root@127.0.0.1 -- mysql -uroot -p1234 #本地登录，指定用户名和密码，用户为root@127.0.0.1 -- mysql -uroot -p1234 -h 192.168.31.95 #远程登录，用户为root@192.168.31.95 mysql 的常用命令 \\-- -- 启动mysql服务与停止mysql服务命令： -- -- net start mysql -- net stop mysql -- -- -- 登陆与退出命令： -- -- mysql －h 服务器IP -P 端口号 -u 用户名 -p 密码 －－prompt 命令提示符 －－delimiter 指定分隔符 -- mysql －h 127.0.0.1 -P 3306 -uroot -p123 -- quit------exit----\\\\q; -- -- -- \\\\s; ------my.ini文件：\\[mysql\\] default-character-set=gbk \\[mysqld\\] character-set-server=gbk -- -- prompt 命令提示符（\\\\D:当前日期 \\\\d:当前数据库 \\\\u:当前用户） -- -- \\\\T(开始日志) \\\\t(结束日志) -- -- show warnings; -- -- help() ? \\\\h -- -- \\\\G； -- -- select now(); -- select version(); -- select user; -- -- \\\\c 取消命令 -- -- delimiter 指定分隔符 忘记密码??? 方法一: 启动 mysql 时, 跳过授权表 \\[root@controller ~\\]# service mysqld stop \\[root@controller ~\\]# mysqld\\_safe --skip-grant-table &amp; \\[root@controller ~\\]# mysql mysql&gt; select user,host,password from mysql.user; +----------+-----------------------+-------------------------------------------+ | user | host | password | +----------+-----------------------+-------------------------------------------+ | root | localhost | \\*A4B6157319038724E3560894F7F932C8886EBFCF | | root | localhost.localdomain | | | root | 127.0.0.1 | | | root | ::1 | | | | localhost | | | | localhost.localdomain | | | root | % | \\*23AE809DDACAF96AF0FD78ED04B6A265E05AA257 | +----------+-----------------------+-------------------------------------------+ mysql&gt; update mysql.user set password=password(&quot;123&quot;) where user=&quot;root&quot; and host=&quot;localhost&quot;; mysql&gt; flush privileges; mysql&gt; exit \\[root@controller ~\\]# service mysqld restart \\[root@controller ~\\]# mysql -uroot -p123 方法二: 删库 删除与权限相关的库mysql，所有的授权信息都丢失，主要用于测试数据库或者刚刚建库不久没有授权数据的情况（从删库到跑路） \\[root@controller ~\\]# rm -rf /var/lib/mysql/mysql \\[root@controller ~\\]# service mysqld restart \\[root@controller ~\\]# mysql SQL 语句数据库是不认识 python 语言的，但是我们同样要与数据库交互，这时需要使用到数据库认识的语言 SQL 语句，它是数据库的代码。 结构化查询语言 (Structured Query Language) 简称 SQL，是一种数据库查询和程序设计语言，用于存取数据以及查询、更新和管理关系数据库系统。 创建数据库、创建数据表、向数据表中添加一条条数据信息均需要使用 SQL 语句。 SQL 分类： 数据定义语言：简称 DDL(Data Definition Language)，用来定义数据库对象：数据库，表，列等。关键字：create，alter，drop 等 数据操作语言：简称 DML(Data Manipulation Language)，用来对数据库中表的记录进行更新。关键字：insert，delete，update 等 数据控制语言：简称 DCL(Data Control Language)，用来定义数据库的访问权限和安全级别，及创建用户。 数据查询语言：简称 DQL(Data Query Language)，用来查询数据库中表的记录。关键字：select，from，where 等 SQL 通用语法 l SQL 语句可以单行或多行书写，以分号结尾 l 可使用空格和缩进来增强语句的可读性 l MySQL 数据库的 SQL 语句不区分大小写，建议使用大写，例如：SELECT * FROM user。 l 同样可以使用 /**/ 的方式完成注释 l MySQL 中的我们常使用的数据类型如下 ) 详细的数据类型如下 (不建议详细阅读！) | 分类 | 类型名称 | 说明 || 整数类型 | tinyInt | 很小的整数 || smallint | 小的整数 || mediumint | 中等大小的整数 || int(integer) | 普通大小的整数 || 小数类型 | float | 单精度浮点数 || double | 双精度浮点数 || decimal（m,d） | 压缩严格的定点数 || 日期类型 | year | YYYY 1901~2155 || time | HH:MM:SS -838:59:59~838:59:59 || date | YYYY-MM-DD 1000-01-01~9999-12-3 || datetime | YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00~ 9999-12-31 23:59:59 || timestamp | YYYY-MM-DD HH:MM:SS 19700101 00:00:01 UTC~2038-01-19 03:14:07UTC || 文本、二进制类型 | CHAR(M) | M 为 0~255 之间的整数 || VARCHAR(M) | M 为 0~65535 之间的整数 || TINYBLOB | 允许长度 0~255 字节 || BLOB | 允许长度 0~65535 字节 || MEDIUMBLOB | 允许长度 0~167772150 字节 || LONGBLOB | 允许长度 0~4294967295 字节 || TINYTEXT | 允许长度 0~255 字节 || TEXT | 允许长度 0~65535 字节 || MEDIUMTEXT | 允许长度 0~167772150 字节 || LONGTEXT | 允许长度 0~4294967295 字节 || VARBINARY(M) | 允许长度 0~M 个字节的变长字节字符串 || BINARY(M) | 允许长度 0~M 个字节的定长字节字符串 | 数据库操作基础操作: \\-- 1.创建表（类似于一个excel表） create table tab\\_name( field1 type\\[完整性约束条件\\], field2 type, ... fieldn type )\\[character set xxx\\]; -- 创建一个员工表employee create table employee( id int primary key auto\\_increment , name varchar(20), gender bit default 1, -- gender char(1) default 1 ----- 或者 TINYINT(1) birthday date, entry\\_date date, job varchar(20), salary double(4,2) unsigned, resume text -- 注意，这里作为最后一个字段不加逗号 ); /\\* 约束: primary key (非空且唯一) :能够唯一区分出当前记录的字段称为主键！ unique not null auto\\_increment 主键字段必须是数字类型。 外键约束 foreign key \\*/ -- 2.查看表信息 desc tab\\_name 查看表结构 show columns from tab\\_name 查看表结构 show tables 查看当前数据库中的所有的表 show create table tab\\_name 查看当前数据库表建表语句 -- 3.修改表结构 -- (1)增加列(字段) alter table tab\\_name add \\[column\\] 列名 类型［完整性约束条件］［first｜after 字段名］; alter table user add addr varchar(20) not null unique first/after username; #添加多个字段 alter table users2 add addr varchar(20), add age int first, add birth varchar(20) after name; -- (2)修改一列类型 alter table tab\\_name modify 列名 类型 \\[完整性约束条件\\]［first｜after 字段名］; alter table users2 modify age tinyint default 20; alter table users2 modify age int after id; -- (3)修改列名 alter table tab\\_name change \\[column\\] 列名 新列名 类型 \\[完整性约束条件\\]［first｜after 字段名］; alter table users2 change age Age int default 28 first; -- (4)删除一列 alter table tab\\_name drop \\[column\\] 列名; -- 思考：删除多列呢？删一个填一个呢？ alter table users2 add salary float(6,2) unsigned not null after name, drop addr; -- (5)修改表名 rename table 表名 to 新表名; -- (6)修该表所用的字符集 alter table student character set utf8; -- 4.删除表 drop table tab\\_name; ---5 添加主键，删除主键 alter table tab\\_name add primary key(字段名称,...) alter table users drop primary key; eg: mysql&gt; create table test5(num int auto\\_increment); ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key create table test(num int primary key auto\\_increment); -- 思考，如何删除主键？ alter table test modify id int; -- auto\\_increment没了，但这样写主键依然存在，所以还要加上下面这句 alter table test drop primary key;-- 仅仅用这句也无法直接删除主键 -- 唯一索引 alter table tab\\_name add unique \\[index|key\\] \\[索引名称\\](字段名称,...) alter table users add unique(name)-- 索引值默认为字段名show create table users; alter table users add unique key user\\_name(name);-- 索引值为user\\_name -- 添加联合索引 alter table users add unique index name\\_age(name,age);#show create table users; -- 删除唯一索引 alter table tab\\_name drop {index|key} index\\_name 完整性约束条件之主键约束单字段主键 主键字段特点：非空且唯一 ) create table users( id INT primary key, name varchar(20), city varchar(20) ); View Code 多字段联合主键 ) create table users2( id INT, name varchar(20), city varchar(20), primary key(name,id) ); View Code &lt;1&gt; 一张表只能有一个主键 &lt;2&gt; 主键类型不一定非是整型 表纪录操作表纪录之增，删，改) \\-- 1.增加一条记录insert /\\*insert ［into］ tab\\_name (field1,filed2,.......) values (value1,value2,.......);\\*/ create table employee\\_new( id int primary key auto\\_increment, name varchar(20) not null unique, birthday varchar(20), salary float(7,2) ); insert into employee\\_new (id,name,birthday,salary) values (1,&#39;yuan&#39;,&#39;1990-09-09&#39;,9000); insert into employee\\_new values (2,&#39;alex&#39;,&#39;1989-08-08&#39;,3000); insert into employee\\_new (name,salary) values (&#39;xialv&#39;,1000); -- 插入多条数据 insert into employee\\_new values (4,&#39;alvin1&#39;,&#39;1993-04-20&#39;,3000), (5,&#39;alvin2&#39;,&#39;1995-05-12&#39;,5000); -- set插入: insert ［into］ tab\\_name set 字段名=值 insert into employee\\_new set id=12,; -- 2.修改表记录 update tab\\_name set field1=value1,field2=value2,......\\[where 语句\\] /\\* UPDATE语法可以用新值更新原有表行中的各列。 SET子句指示要修改哪些列和要给予哪些值。 WHERE子句指定应更新哪些行。如没有WHERE子句，则更新所有的行。\\*/ update employee\\_new set birthday=&quot;1989-10-24&quot; WHERE id=1; --- 将yuan的薪水在原有基础上增加1000元。 update employee\\_new set salary=salary+4000 where name=&#39;yuan&#39;; -- 3.删除表纪录 delete from tab\\_name \\[where ....\\] /\\* 如果不跟where语句则删除整张表中的数据 delete只能用来删除一行记录 delete语句只能删除表中的内容，不能删除表本身，想要删除表，用drop TRUNCATE TABLE也可以删除表中的所有数据，词语句首先摧毁表，再新建表。此种方式删除的数据不能在 事务中恢复。\\*/ -- 删除表中名称为’alex’的记录。 delete from employee\\_new where name=&#39;alex&#39;; -- 删除表中所有记录。 delete from employee\\_new;-- 注意auto\\_increment没有被重置:alter table employee auto\\_increment=1; -- 使用truncate删除表中记录。 truncate table emp\\_new; View Code 表纪录之查 (单表查询)) \\-- 查询表达式 SELECT \\*|field1,filed2 ... FROM tab\\_name WHERE 条件 GROUP BY field HAVING 筛选 ORDER BY field LIMIT 限制条数 ---准备表 CREATE TABLE ExamResult( id INT PRIMARY KEY auto\\_increment, name VARCHAR (20), JS DOUBLE , Django DOUBLE , OpenStack DOUBLE ); INSERT INTO ExamResult VALUES (1,&quot;yuan&quot;,98,98,98), (2,&quot;tom&quot;,35,98,67), (3,&quot;jerry&quot;,59,59,62), (4,&quot;jean&quot;,88,89,82), (5,&quot;lilei&quot;,88,98,67), (6,&quot;kivn&quot;,86,100,55); -- （1）select \\[distinct\\] \\*|field1，field2，...... from tab\\_name -- 其中from指定从哪张表筛选，\\*表示查找所有列，也可以指定一个列 -- 表明确指定要查找的列，distinct用来剔除重复行。 -- 查询表中所有学生的信息。 select \\* from ExamResult; -- 查询表中所有学生的姓名和对应的英语成绩。 select name,JS from ExamResult; -- 过滤表中重复数据。 select distinct JS ,name from ExamResult; -- （2）select 也可以使用表达式，并且可以使用: 字段 as 别名或者:字段 别名 -- 在所有学生分数上加10分特长分显示。 select name,JS+10,Django+10,OpenStack+10 from ExamResult; -- 统计每个学生的总分。 select name,JS+Django+OpenStack from ExamResult; -- 使用别名表示学生总分。 select name as 姓名,JS+Django+OpenStack as 总成绩 from ExamResult; select name,JS+Django+OpenStack 总成绩 from ExamResult; select name JS from ExamResult; -- what will happen?----&gt;记得加逗号 -- （3）使用where子句，进行过滤查询。 -- 查询姓名为XXX的学生成绩 select \\* from ExamResult where name=&#39;jerry&#39;; -- 查询英语成绩大于90分的同学 select id,name,JS from ExamResult where JS&gt;90; -- 查询总分大于200分的所有同学 select name,JS+Django+OpenStack as 总成绩 from ExamResult where JS+Django+OpenStack&gt;200 ; -- where字句中可以使用： -- 比较运算符： &gt; &lt; &gt;= &lt;= &lt;&gt; != between 80 and 100 值在10到20之间 in(80,90,100) 值是10或20或30 like &#39;jerry%&#39; /\\* pattern可以是%或者\\_， 如果是%则表示任意多字符，此例如唐僧,唐国强 如果是\\_则表示一个字符唐\\_，只有唐僧符合。两个\\_则表示两个字符：\\_\\_ \\*/ -- 逻辑运算符 在多个条件直接可以使用逻辑运算符 and or not -- 练习 -- 查询JS分数在 70－100之间的同学。 select name ,JS from ExamResult where JS between 80 and 100; -- 查询Django分数为75,76,77的同学。 select name ,Django from ExamResult where Django in (75,98,77); -- 查询所有姓王的学生成绩。 select \\* from ExamResult where name like &#39;王%&#39;; -- 查询JS分&gt;90，Django分&gt;90的同学。 select id,name from ExamResult where JS&gt;90 and Django &gt;90; -- 查找缺考数学的学生的姓名 select name from ExamResult where Database is null; -- （4）Order by 指定排序的列，排序的列即可是表中的列名，也可以是select 语句后指定的别名。 -- select \\*|field1,field2... from tab\\_name order by field \\[Asc|Desc\\] -- Asc 升序、Desc 降序，其中asc为默认值 ORDER BY 子句应位于SELECT语句的结尾。 -- 练习： -- 对JS成绩排序后输出。 select \\* from ExamResult order by JS; -- 对总分排序按从高到低的顺序输出 select name ,(ifnull(JS,0)+ifnull(Django,0)+ifnull(Database,0)) 总成绩 from ExamResult order by 总成绩 desc; -- 对姓李的学生成绩排序输出 select name ,(ifnull(JS,0)+ifnull(Django,0)+ifnull(OpenStack,0)) 总成绩 from ExamResult where name like &#39;a%&#39; order by 总成绩 desc; -- （5）group by 分组查询： CREATE TABLE order\\_menu( id INT PRIMARY KEY auto\\_increment, product\\_name VARCHAR (20), price FLOAT(6,2), born\\_date DATE, class VARCHAR (20) ); INSERT INTO order\\_menu (product\\_name,price,born\\_date,class) VALUES (&quot;苹果&quot;,20,20170612,&quot;水果&quot;), (&quot;香蕉&quot;,80,20170602,&quot;水果&quot;), (&quot;水壶&quot;,120,20170612,&quot;电器&quot;), (&quot;被罩&quot;,70,20170612,&quot;床上用品&quot;), (&quot;音响&quot;,420,20170612,&quot;电器&quot;), (&quot;床单&quot;,55,20170612,&quot;床上用品&quot;), (&quot;草莓&quot;,34,20170612,&quot;水果&quot;); -- 注意,按分组条件分组后每一组只会显示第一条记录 -- group by字句，其后可以接多个列名，也可以跟having子句,对group by 的结果进行筛选。 -- 按位置字段筛选 select \\* from order\\_menu group by 5; -- 练习：对购物表按类名分组后显示每一组商品的价格总和 select class,SUM(price)from order\\_menu group by class; -- 练习：对购物表按类名分组后显示每一组商品价格总和超过150的商品 select class,SUM(price)from order\\_menu group by class HAVING SUM(price)&gt;150; /\\* having 和 where两者都可以对查询结果进行进一步的过滤，差别有： &lt;1&gt;where语句只能用在分组之前的筛选，having可以用在分组之后的筛选； &lt;2&gt;使用where语句的地方都可以用having进行替换 &lt;3&gt;having中可以用聚合函数，where中就不行。 \\*/ -- GROUP\\_CONCAT() 函数 SELECT id,GROUP\\_CONCAT(name),GROUP\\_CONCAT(JS) from ExamResult GROUP BY id; -- （6）聚合函数： 先不要管聚合函数要干嘛，先把要求的内容查出来再包上聚合函数即可。 -- (一般和分组查询配合使用) --&lt;1&gt; 统计表中所有记录 -- COUNT(列名)：统计行的个数 -- 统计一个班级共有多少学生？先查出所有的学生，再用count包上 select count(\\*) from ExamResult; -- 统计JS成绩大于70的学生有多少个？ select count(JS) from ExamResult where JS&gt;70; -- 统计总分大于280的人数有多少？ select count(name) from ExamResult where (ifnull(JS,0)+ifnull(Django,0)+ifnull(OpenStack,0))&gt;280; -- 注意:count(\\*)统计所有行; count(字段)不统计null值. -- SUM(列名)：统计满足条件的行的内容和 -- 统计一个班级JS总成绩？先查出所有的JS成绩，再用sum包上 select JS as JS总成绩 from ExamResult; select sum(JS) as JS总成绩 from ExamResult; -- 统计一个班级各科分别的总成绩 select sum(JS) as JS总成绩, sum(Django) as Django总成绩, sum(OpenStack) as OpenStack from ExamResult; -- 统计一个班级各科的成绩总和 select sum(ifnull(JS,0)+ifnull(Django,0)+ifnull(Database,0)) as 总成绩 from ExamResult; -- 统计一个班级JS成绩平均分 select sum(JS)/count(\\*) from ExamResult ; -- 注意：sum仅对数值起作用，否则会报错。 -- AVG(列名)： -- 求一个班级JS平均分？先查出所有的JS分，然后用avg包上。 select avg(ifnull(JS,0)) from ExamResult; -- 求一个班级总分平均分 select avg((ifnull(JS,0)+ifnull(Django,0)+ifnull(Database,0))) from ExamResult ; -- Max、Min -- 求班级最高分和最低分（数值范围在统计中特别有用） select Max((ifnull(JS,0)+ifnull(Django,0)+ifnull(OpenStack,0))) 最高分 from ExamResult; select Min((ifnull(JS,0)+ifnull(Django,0)+ifnull(OpenStack,0))) 最低分 from ExamResult; -- 求购物表中单价最高的商品名称及价格 ---SELECT id, MAX(price) FROM order\\_menu;--id和最高价商品是一个商品吗? SELECT MAX(price) FROM order\\_menu; -- 注意：null 和所有的数计算都是null，所以需要用ifnull将null转换为0！ -- -----ifnull(JS,0) -- with rollup的使用 --&lt;2&gt; 统计分组后的组记录 -- （7） 重点：Select from where group by having order by -- Mysql在执行sql语句时的执行顺序： -- from where select group by having order by -- 分析: select JS as JS成绩 from ExamResult where JS成绩 &gt;70; ---- 不成功 select JS as JS成绩 from ExamResult having JS成绩 &gt;90; --- 成功 -- (8) limit SELECT \\* from ExamResult limit 1; SELECT \\* from ExamResult limit 2,5;--跳过前两条显示接下来的五条纪录 SELECT \\* from ExamResult limit 2,2; --- (9) 使用正则表达式查询 SELECT \\* FROM employee WHERE emp\\_name REGEXP &#39;^yu&#39;; SELECT \\* FROM employee WHERE emp\\_name REGEXP &#39;yun$&#39;; SELECT \\* FROM employee WHERE emp\\_name REGEXP &#39;m{2}&#39;; View Code 外键约束创建外键) \\--- 每一个班主任会对应多个学生 , 而每个学生只能对应一个班主任 ----主表 CREATE TABLE ClassCharger( id TINYINT PRIMARY KEY auto\\_increment, name VARCHAR (20), age INT , is\\_marriged boolean -- show create table ClassCharger: tinyint(1) ); INSERT INTO ClassCharger (name,age,is\\_marriged) VALUES (&quot;冰冰&quot;,12,0), (&quot;丹丹&quot;,14,0), (&quot;歪歪&quot;,22,0), (&quot;姗姗&quot;,20,0), (&quot;小雨&quot;,21,0); ----子表 CREATE TABLE Student( id INT PRIMARY KEY auto\\_increment, name VARCHAR (20), charger\\_id TINYINT, --切记:作为外键一定要和关联主键的数据类型保持一致 -- \\[ADD CONSTRAINT charger\\_fk\\_stu\\]FOREIGN KEY (charger\\_id) REFERENCES ClassCharger(id) ) ENGINE=INNODB; INSERT INTO Student(name,charger\\_id) VALUES (&quot;alvin1&quot;,2), (&quot;alvin2&quot;,4), (&quot;alvin3&quot;,1), (&quot;alvin4&quot;,3), (&quot;alvin5&quot;,1), (&quot;alvin6&quot;,3), (&quot;alvin7&quot;,2); DELETE FROM ClassCharger WHERE ; INSERT student (name,charger\\_id) VALUES (&quot;yuan&quot;,1); -- 删除居然成功,可是 alvin3显示还是有班主任id=1的冰冰的; -----------增加外键和删除外键--------- ALTER TABLE student ADD CONSTRAINT abc FOREIGN KEY(charger\\_id) REFERENCES classcharger(id); ALTER TABLE student DROP FOREIGN KEY abc; View Code INNODB 支持的 ON 语句) \\--外键约束对子表的含义: 如果在父表中找不到候选键,则不允许在子表上进行insert/update --外键约束对父表的含义: 在父表上进行update/delete以更新或删除在子表中有一条或多条对 -- 应匹配行的候选键时,父表的行为取决于：在定义子表的外键时指定的 -- on update/on delete子句 -----------------innodb支持的四种方式--------------------------------------- -----cascade方式 在父表上update/delete记录时，同步update/delete掉子表的匹配记录 -----外键的级联删除：如果父表中的记录被删除，则子表中对应的记录自动被删除-------- FOREIGN KEY (charger\\_id) REFERENCES ClassCharger(id) ON DELETE CASCADE ------set null方式 在父表上update/delete记录时，将子表上匹配记录的列设为null -- 要注意子表的外键列不能为not null FOREIGN KEY (charger\\_id) REFERENCES ClassCharger(id) ON DELETE SET NULL ------Restrict方式 :拒绝对父表进行删除更新操作(了解) ------No action方式 在mysql中同Restrict,如果子表中有匹配的记录,则不允许对父表对应候选键 -- 进行update/delete操作（了解） View Code 多表查询) \\-- 准备两张表 -- company.employee -- company.department create table employee( emp\\_id int auto\\_increment primary key not null, emp\\_name varchar(50), age int, dept\\_id int ); insert into employee(emp\\_name,age,dept\\_id) values (&#39;A&#39;,19,200), (&#39;B&#39;,26,201), (&#39;C&#39;,30,201), (&#39;D&#39;,24,202), (&#39;E&#39;,20,200), (&#39;F&#39;,38,204); create table department( dept\\_id int, dept\\_name varchar(100) ); insert into department values (200,&#39;人事部&#39;), (201,&#39;技术部&#39;), (202,&#39;销售部&#39;), (203,&#39;财政部&#39;); mysql&gt; select \\* from employee; +--------+----------+------+---------+ | emp\\_id | emp\\_name | age | dept\\_id | +--------+----------+------+---------+ | 1 | A | 19 | 200 | | 2 | B | 26 | 201 | | 3 | C | 30 | 201 | | 4 | D | 24 | 202 | | 5 | E | 20 | 200 | | 6 | F | 38 | 204 | +--------+----------+------+---------+ 6 rows in set (0.00 sec) mysql&gt; select \\* from department; +---------+-----------+ | dept\\_id | dept\\_name | +---------+-----------+ | 200 | 人事部 | | 201 | 技术部 | | 202 | 销售部 | | 203 | 财政部 | +---------+-----------+ 4 rows in set (0.01 sec) 准备表 多表查询之连接查询) mysql&gt; SELECT \\* FROM employee,department; -- select employee.emp\\_id,employee.emp\\_name,employee.age, -- department.dept\\_name from employee,department; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 1 | A | 19 | 200 | 201 | 技术部 | | 1 | A | 19 | 200 | 202 | 销售部 | | 1 | A | 19 | 200 | 203 | 财政部 | | 2 | B | 26 | 201 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 2 | B | 26 | 201 | 202 | 销售部 | | 2 | B | 26 | 201 | 203 | 财政部 | | 3 | C | 30 | 201 | 200 | 人事部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 202 | 销售部 | | 3 | C | 30 | 201 | 203 | 财政部 | | 4 | D | 24 | 202 | 200 | 人事部 | | 4 | D | 24 | 202 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 4 | D | 24 | 202 | 203 | 财政部 | | 5 | E | 20 | 200 | 200 | 人事部 | | 5 | E | 20 | 200 | 201 | 技术部 | | 5 | E | 20 | 200 | 202 | 销售部 | | 5 | E | 20 | 200 | 203 | 财政部 | | 6 | F | 38 | 204 | 200 | 人事部 | | 6 | F | 38 | 204 | 201 | 技术部 | | 6 | F | 38 | 204 | 202 | 销售部 | | 6 | F | 38 | 204 | 203 | 财政部 | +--------+----------+------+---------+---------+-----------+ 1. 笛卡尔积查询 ) \\-- 查询两张表中都有的关联数据,相当于利用条件从笛卡尔积结果中筛选出了正确的结果。 select \\* from employee,department where employee.dept\\_id = department.dept\\_id; --select \\* from employee inner join department on employee.dept\\_id = department.dept\\_id; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 5 | E | 20 | 200 | 200 | 人事部 | +--------+----------+------+---------+---------+-----------+ 2. 内连接 ) \\--（1）左外连接：在内连接的基础上增加左边有右边没有的结果 select \\* from employee left join department on employee.dept\\_id = department.dept\\_id; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 5 | E | 20 | 200 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 6 | F | 38 | 204 | NULL | NULL | +--------+----------+------+---------+---------+-----------+ --（2）右外连接：在内连接的基础上增加右边有左边没有的结果 select \\* from employee RIGHT JOIN department on employee.dept\\_id = department.dept\\_id; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 5 | E | 20 | 200 | 200 | 人事部 | | NULL | NULL | NULL | NULL | 203 | 财政部 | +--------+----------+------+---------+---------+-----------+ --（3）全外连接：在内连接的基础上增加左边有右边没有的和右边有左边没有的结果 -- mysql不支持全外连接 full JOIN -- mysql可以使用此种方式间接实现全外连接 select \\* from employee RIGHT JOIN department on employee.dept\\_id = department.dept\\_id UNION select \\* from employee LEFT JOIN department on employee.dept\\_id = department.dept\\_id; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 5 | E | 20 | 200 | 200 | 人事部 | | NULL | NULL | NULL | NULL | 203 | 财政部 | | 6 | F | 38 | 204 | NULL | NULL | +--------+----------+------+---------+---------+-----------+ -- 注意 union与union all的区别：union会去掉相同的纪录 3. 外连接 多表查询之复合条件连接查询) \\-- 查询员工年龄大于等于25岁的部门 SELECT DISTINCT department.dept\\_name FROM employee,department WHERE employee.dept\\_id = department.dept\\_id AND age&gt;25; --以内连接的方式查询employee和department表，并且以age字段的升序方式显示 select employee.emp\\_id,employee.emp\\_name,employee.age,department.dept\\_name from employee,department where employee.dept\\_id = department.dept\\_id order by age asc; View Code 多表查询之子查询) \\-- 子查询是将一个查询语句嵌套在另一个查询语句中。 -- 内层查询语句的查询结果，可以为外层查询语句提供查询条件。 -- 子查询中可以包含：IN、NOT IN、ANY、ALL、EXISTS 和 NOT EXISTS等关键字 -- 还可以包含比较运算符：= 、 !=、&gt; 、&lt;等 -- 1. 带IN关键字的子查询 ---查询employee表，但dept\\_id必须在department表中出现过 select \\* from employee where dept\\_id IN (select dept\\_id from department); +--------+----------+------+---------+ | emp\\_id | emp\\_name | age | dept\\_id | +--------+----------+------+---------+ | 1 | A | 19 | 200 | | 2 | B | 26 | 201 | | 3 | C | 30 | 201 | | 4 | D | 24 | 202 | | 5 | E | 20 | 200 | +--------+----------+------+---------+ 5 rows in set (0.01 sec) -- 2. 带比较运算符的子查询 -- =、!=、&gt;、&gt;=、&lt;、&lt;=、&lt;&gt; -- 查询员工年龄大于等于25岁的部门 select dept\\_id,dept\\_name from department where dept\\_id IN (select DISTINCT dept\\_id from employee where age&gt;=25); -- 3. 带EXISTS关键字的子查询 -- EXISTS关字键字表示存在。在使用EXISTS关键字时，内层查询语句不返回查询的记录。 -- 而是返回一个真假值。Ture或False -- 当返回Ture时，外层查询语句将进行查询；当返回值为False时，外层查询语句不进行查询 select \\* from employee WHERE EXISTS (SELECT dept\\_name from department where dept\\_id=203); --department表中存在dept\\_id=203，Ture select \\* from employee WHERE EXISTS (SELECT dept\\_name from department where dept\\_id=205); -- Empty set (0.00 sec) ps: create table t1(select \\* from t2); View Code","categories":[{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/categories/sql/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/tags/sql/"}]},{"title":"HashMap 面试必问总结","slug":"HashMap 面试必问总结","date":"2020-07-10T09:07:59.400Z","updated":"2023-05-09T09:20:18.083Z","comments":true,"path":"2020/07/10/HashMap 面试必问总结/","link":"","permalink":"https://topone233.github.io/2020/07/10/HashMap%20%E9%9D%A2%E8%AF%95%E5%BF%85%E9%97%AE%E6%80%BB%E7%BB%93/","excerpt":"","text":"原文地址 www.cnblogs.com 1.HashMap 的数据结构？哈希表结构（链表散列：数组 + 链表）实现，结合数组和链表的优点。当链表长度超过 8 时，链表转换为红黑树。 2.HashMap 的工作原理？HashMap 底层是 hash 数组和单向链表实现，数组中的每个元素都是链表，由 Node 内部类（实现 Map.Entry&lt;K,V&gt; 接口）实现，HashMap 通过 put &amp; get 方法存储和获取。 put 调用 hash(K) 方法计算 K 的 hash 值，然后结合数组长度，计算得数组下标； 调整数组大小（当容器中的元素个数大于 capacity * loadfactor 时，容器会进行扩容 resize 为 2n）； 如果 K 的 hash 值在 HashMap 中 不存在，则执行插入，若存在，则发生碰撞； 存在 且它们两者 equals 返回 true，则更新键值对； 且它们两者 equals 返回 false，则插入链表的尾部（尾插法）或者红黑树中。 注意： JDK 1.7 之前使用头插法、JDK 1.8 使用尾插法 当碰撞导致链表大于 TREEIFY_THRESHOLD = 8 时，就把链表转换成红黑树。链表长度低于 6，就把红黑树转回链表 get 调用 hash(K) 方法（计算 K 的 hash 值）从而获取该键值所在链表的数组下标； 顺序遍历链表，equals() 方法查找相同 Node 链表中 K 值对应的 V 值。 hashCode 是定位的，找到存储位置；equals 是定性的，比较两者是否相等。 3. 当两个对象的 hashCode 相同会发生什么？hashCode 相同，不一定就是相等的（equals 方法比较），如果两个对象所在数组的下标相同，”碰撞” 就此发生。又因为 HashMap 使用链表存储对象，这个 Node 会存储到链表中。 4. 你知道 hash 的实现吗？为什么要这样实现？JDK 1.8 中，是通过 hashCode() 的高 16 位异或低 16 位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度，功效和质量来考虑的，减少系统的开销，也不会造成因为高位没有参与下标的计算，从而引起的碰撞。 5. 为什么要用异或运算符？保证了对象的 hashCode 的 32 位值只要有一位发生改变，整个 hash() 返回值就会改变。尽可能的减少碰撞。 6. HashMap 的 table 的容量如何确定？loadFactor 是什么？ 该容量如何变化？这种变化会带来什么问题？ table 数组大小是由capacity这个参数确定的，默认是 16，也可以构造时传入，最大限制是 1&lt;&lt;30； loadFactor 是装载因子，主要目的是用来确认 table 数组是否需要动态扩展，默认值是 0.75，比如 table 数组大小为 16，装载因子为 0.75 时，threshold 就是 12，当 table 的实际大小超过 12 时，table 就需要动态扩容； 扩容时，调用 resize() 方法，将 table 长度变为原来的两倍（注意是 table 长度，而不是 threshold） 如果数据很大的情况下，扩展时将会带来性能的损失，在性能要求很高的地方，这种损失很可能很致命。 7.HashMap 中 put 方法的过程？调用哈希函数获取 Key 对应的 hash 值，再计算其数组下标； 如果没有出现哈希冲突，则直接放入数组；如果出现哈希冲突，则以链表的方式放在链表后面； 如果链表长度超过阀值 (TREEIFY THRESHOLD==8)，就把链表转成红黑树，链表长度低于 6，就把红黑树转回链表; 如果结点的 key 已经存在，则替换其 value 即可； 如果集合中的键值对大于 12，调用 resize 方法进行数组扩容。” 8. 数组扩容的过程？创建一个新的数组，其容量为旧数组的两倍，并重新计算旧数组中结点的存储位置。结点在新数组中的位置只有两种，原下标位置或原下标 + 旧数组的大小。 9. 拉链法导致的链表过深问题为什么不用二叉查找树代替，而选择红黑树？为什么不一直使用红黑树？之所以选择红黑树是为了解决二叉查找树的缺陷，二叉查找树在特殊情况下会变成一条线性结构（这就跟原来使用链表结构一样了，造成很深的问题），遍历查找会非常慢。而红黑树在插入新数据后可能需要通过左旋，右旋、变色这些操作来保持平衡，引入红黑树就是为了查找数据快，解决链表查询深度的问题，我们知道红黑树属于平衡二叉树，但是为了保持 “平衡” 是需要付出代价的，但是该代价所损耗的资源要比遍历线性链表要少，所以当长度大于 8 的时候，会使用红黑树，如果链表长度很短的话，根本不需要引入红黑树，引入反而会慢。 10. 说说你对红黑树的见解？ 每个节点非红即黑 根节点总是黑色的 如果节点是红色的，则它的子节点必须是黑色的（反之不一定） 每个叶子节点都是黑色的空节点（NIL 节点） 从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度） 11.jdk8 中对 HashMap 做了哪些改变？ 在 java 1.8 中，如果链表的长度超过了 8，那么链表将转换为红黑树。（桶的数量必须大于 64，小于 64 的时候只会扩容） 发生 hash 碰撞时，java 1.7 会在链表的头部插入，而 java 1.8 会在链表的尾部插入 在 java 1.8 中，Entry 被 Node 替代 (换了一个马甲)。 12.HashMap，LinkedHashMap，TreeMap 有什么区别？ LinkedHashMap 保存了记录的插入顺序，在用 Iterator 遍历时，先取到的记录肯定是先插入的；遍历比 HashMap 慢； TreeMap 实现 SortMap 接口，能够把它保存的记录根据键排序（默认按键值升序排序，也可以指定排序的比较器） 13.HashMap &amp; TreeMap &amp; LinkedHashMap 使用场景？ 一般情况下，使用最多的是 HashMap。 HashMap：在 Map 中插入、删除和定位元素时； TreeMap：在需要按自然顺序或自定义顺序遍历键的情况下； LinkedHashMap：在需要输出的顺序和输入的顺序相同的情况下。 14.HashMap 和 HashTable 有什么区别？ HashMap 是线程不安全的， HashTable 是线程安全的； 由于线程安全，所以 HashTable 的效率比不上 HashMap； HashMap 最多只允许一条记录的键为 null，允许多条记录的值为 null; HashTable 不允许； HashMap 默认初始化数组的大小为 16，扩容时，扩大两倍； HashTable 默认初始化数组的大小为 11，扩容时，扩大两倍 + 1； HashMap 需要重新计算 hash 值； HashTable 直接使用对象的 hashCode 15.同样是线程安全，ConcurrentHashMap 与 HashTable 在线程同步上有什么不同？ConcurrentHashMap 类（是 Java 并发包 java.util.concurrent 中提供的一个线程安全且高效的 HashMap 实现）。 HashTable 是使用 synchronize 关键字加锁的原理（就是对对象加锁）； 而针对 ConcurrentHashMap，在 JDK 1.7 中采用 分段锁的方式；JDK 1.8 中直接采用了 CAS（无锁算法）+ synchronized。 16.HashMap &amp; ConcurrentHashMap 的区别？除了加锁，原理上无太大区别。另外，HashMap 的键值对允许有 null，但是ConCurrentHashMap都不允许。 17. 为什么 ConcurrentHashMap 比 HashTable 效率要高？HashTable 使用一把锁（锁住整个链表结构）处理并发问题，多个线程竞争一把锁，容易阻塞； ConcurrentHashMap JDK 1.7 中使用分段锁（ReentrantLock + Segment + HashEntry），相当于把一个 HashMap 分成多个段，每段分配一把锁，这样支持多线程访问。锁粒度：基于 Segment，包含多个 HashEntry。 JDK 1.8 中使用 CAS + synchronized + Node + 红黑树。锁粒度：Node（首结点）（实现 Map.Entry&lt;K,V&gt;）。锁粒度降低了。 18. 针对 ConcurrentHashMap 锁机制具体分析（JDK 1.7 VS JDK 1.8）？JDK 1.7 中，采用分段锁的机制，实现并发的更新操作，底层采用数组 + 链表的存储结构，包括两个核心静态内部类 Segment 和 HashEntry。 Segment 继承 ReentrantLock（重入锁） 用来充当锁的角色，每个 Segment 对象守护每个散列映射表的若干个桶； HashEntry 用来封装映射表的键 - 值对； 每个桶是由若干个 HashEntry 对象链接起来的链表 JDK 1.8 中，采用 Node + CAS + Synchronized 来保证并发安全。取消类 Segment，直接用 table 数组存储键值对；当 HashEntry 对象组成的链表长度超过 TREEIFY_THRESHOLD 时，链表转换为红黑树，提升性能。底层变更为数组 + 链表 + 红黑树。 19.ConcurrentHashMap 在 JDK 1.8 中，为什么要使用内置锁 synchronized 来代替重入锁 ReentrantLock？ 粒度降低了； JVM 开发团队没有放弃 synchronized，而且基于 JVM 的 synchronized 优化空间更大，更加自然。 在大量的数据操作下，对于 JVM 的内存压力，基于 API 的 ReentrantLock 会开销更多的内存。 20.ConcurrentHashMap 简单介绍？ 重要的常量： private transient volatile int sizeCtl; 当为负数时，-1 表示正在初始化，-N 表示 N - 1 个线程正在进行扩容； 当为 0 时，表示 table 还没有初始化； 当为其他正数时，表示初始化或者下一次进行扩容的大小。 数据结构： - Node 是存储结构的基本单元，继承 HashMap 中的 Entry，用于存储数据； TreeNode 继承 Node，但是数据结构换成了二叉树结构，是红黑树的存储结构，用于红黑树中存储数据； TreeBin 是封装 TreeNode 的容器，提供转换红黑树的一些条件和锁的控制。 存储对象时（put() 方法）： 如果没有初始化，就调用 initTable() 方法来进行初始化； 如果没有 hash 冲突就直接 CAS 无锁插入； 如果需要扩容，就先进行扩容； 如果存在 hash 冲突，就加锁来保证线程安全，两种情况：一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入； 如果该链表的数量大于阀值 8，就要先转换成红黑树的结构，break 再一次进入循环 如果添加成功就调用 addCount() 方法统计 size，并且检查是否需要扩容。 扩容方法 transfer()：默认容量为 16，扩容时，容量变为原来的两倍。 helpTransfer()：调用多个工作线程一起帮助进行扩容，这样的效率就会更高。 获取对象时（get() 方法）： 计算 hash 值，定位到该 table 索引位置，如果是首结点符合就返回； 如果遇到扩容时，会调用标记正在扩容结点 ForwardingNode.find() 方法，查找该结点，匹配就返回； 以上都不符合的话，就往下遍历结点，匹配就返回，否则最后就返回 null。 21.ConcurrentHashMap 的并发度是什么？程序运行时能够同时更新 ConccurentHashMap 且不产生锁竞争的最大线程数。默认为 16，且可以在构造函数中设置。当用户设置并发度时，ConcurrentHashMap 会使用大于等于该值的最小 2 幂指数作为实际并发度（假如用户设置并发度为 17，实际并发度则为 32） 有时间会对 HashTable，ConcurrentHashmap 解析。 22.为什么要重写hashcode和equals方法？用HashMap存入自定义的类时，如果不重写这个自定义类的hashcode和equals方法，得到的结果会和预期的不一样。 重写hashcode和equals方法，来覆盖Object里的同名方法。Object的固有方法是根据两个对象的内存地址来判断，两个不同的对象，内存地址一定不会相同，所以无论值是否相等，结果都一定不会相等 23. HashMap的数组长度为什么一定是2的次幂？void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); } } 如果数组进行扩容，数组长度发生变化，而存储位置 index = h&amp;(length-1),index也可能会发生变化，需要重新计算index，我们先来看看transfer这个方法: void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; //for循环中的代码，逐个遍历链表，重新计算索引位置，将老数组数据复制到新数组中去（数组不存储实际数据，所以仅仅是拷贝引用而已） for (Entry&lt;K,V> e : table) { while(null != e) { Entry&lt;K,V> next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); //将当前entry的next链指向新的索引位置,newTable[i]有可能为空，有可能也是个entry链，如果是entry链，直接在链表头部插入。 e.next = newTable[i]; newTable[i] = e; e = next; } } } } 这个方法将老数组中的数据逐个链表地遍历，扔到新的扩容后的数组中，我们的数组索引位置的计算是通过 对key值的hashcode进行hash扰乱运算后，再通过和 length-1进行位运算得到最终数组索引位置。 HashMap的数组长度一定保持2的次幂，比如16的二进制表示为 10000，那么length-1就是15，二进制为01111，同理扩容后的数组长度为32，二进制表示为100000，length-1为31，二进制表示为011111。从下图可以我们也能看到这样会保证低位全为1，而扩容后只有一位差异，也就是多出了最左位的1，这样在通过 h&amp;(length-1)的时候，只要h对应的最左边的那一个差异位为0，就能保证得到的新的数组索引和老数组索引一致(大大减少了之前已经散列良好的老数组的数据位置重新调换)，个人理解。 还有，数组长度保持2的次幂，length-1的低位都为1，会使得获得的数组索引index更加均匀 我们看到，上面的&amp;运算，高位是不会对结果产生影响的（hash函数采用各种位运算可能也是为了使得低位更加散列），我们只关注低位bit，如果低位全部为1，那么对于h低位部分来说，任何一位的变化都会对结果产生影响，也就是说，要得到index=21这个存储位置，h的低位只有这一种组合。这也是数组长度设计为必须为2的次幂的原因。 如果不是2的次幂，也就是低位不是全为1此时，要使得index=21，h的低位部分不再具有唯一性了，哈希冲突的几率会变的更大，同时，index对应的这个bit位无论如何不会等于1了，而对应的那些数组位置也就被白白浪费了。 24. 什么是Hash冲突？两个元素通过 hash 函数计算出的值是一样的，是同一个存储地址。当后面的元素要插入到这个地址时，发现已经被占用了，这时候就产生了 hash 冲突，也叫哈希碰撞。 好的哈希函数会尽可能使计算简单和散列地址分布均匀。但是，数组是一块连续的固定长度的内存空间，再好的哈希函数也不能保证得到的存储地址绝对不发生冲突。 哈希冲突的解决方案有多种： 开放定址法(发生冲突，继续寻找下一块未被占用的存储地址) 再散列函数法 链地址法。HashMap采用的即是链地址法，也就是数组+链表的方式。 开放定址法 (查询产生冲突的地址的下一个地址是否被占用，直到寻找到空的地址)，再散列法，链地址法等。hashmap 采用的就是链地址法，jdk1.7 中，当冲突时，在冲突的地址上生成一个链表，将冲突的元素的 key，通过 equals 进行比较，相同即覆盖，不同则添加到链表上，此时如果链表过长，效率就会大大降低，查找和添加操作的时间复杂度都为 O(n)；但是在 jdk1.8 中如果链表长度大于 8，链表就会转化为红黑树，下图就是 1.8 版本的（图片来源 https://segmentfault.com/a/1190000012926722），时间复杂度也降为了 O(logn)，性能得到了很大的优化。 25. HashMap有哪些参数？//默认初始容量为16，0000 0001 左移4位 0001 0000为16，主干数组的初始容量为16，而且这个数组 //必须是2的倍数(后面说为什么是2的倍数) static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 //最大容量为int的最大值除2 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //默认加载因子为0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; //阈值，如果主干数组上的链表的长度大于8，链表转化为红黑树 static final int TREEIFY_THRESHOLD = 8; //hash表扩容后，如果发现某一个红黑树的长度小于6，则会重新退化为链表 static final int UNTREEIFY_THRESHOLD = 6; //当hashmap容量大于64时，链表才能转成红黑树 static final int MIN_TREEIFY_CAPACITY = 64; //临界值=主干数组容量*负载因子 int threshold； 参考博客：https://www.cnblogs.com/heqiyoujing/p/11143298.html https://www.jianshu.com/p/75adf47958a7","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"HashMap","slug":"hashmap","permalink":"https://topone233.github.io/tags/hashmap/"}]},{"title":"九大常见数据结构","slug":"九大常见数据结构","date":"2020-07-10T09:07:59.386Z","updated":"2023-02-06T11:44:48.257Z","comments":true,"path":"2020/07/10/九大常见数据结构/","link":"","permalink":"https://topone233.github.io/2020/07/10/%E4%B9%9D%E5%A4%A7%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"参考：https://mp.weixin.qq.com/s/lnMvB3zgWZTmCfCvnNwTbA 数据结构想必大家都不会陌生，对于一个成熟的程序员而言，熟悉和掌握数据结构和算法也是基本功之一。数据结构本身其实不过是数据按照特点关系进行存储或者组织的集合，特殊的结构在不同的应用场景中往往会带来不一样的处理效率。 常用的数据结构可根据数据访问的特点分为线性结构和非线性结构。线性结构包括常见的链表、栈、队列等，非线性结构包括树、图等。数据结构种类繁多，本文将通过图解的方式对常用的数据结构进行理论上的介绍和讲解，以方便大家掌握常用数据结构的基本知识。 1 数组数组可以说是最基本最常见的数据结构。数组一般用来存储相同类型的数据，可通过数组名和下标进行数据的访问和更新。数组中元素的存储是按照先后顺序进行的，同时在内存中也是按照这个顺序进行连续存放。数组相邻元素之间的内存地址的间隔一般就是数组数据类型的大小。 2 链表链表相较于数组，除了数据域，还增加了指针域用于构建链式的存储数据。链表中每一个节点都包含此节点的数据和指向下一节点地址的指针。由于是通过指针进行下一个数据元素的查找和访问，使得链表的自由度更高。 这表现在对节点进行增加和删除时，只需要对上一节点的指针地址进行修改，而无需变动其它的节点。不过事物皆有两极，指针带来高自由度的同时，自然会牺牲数据查找的效率和多余空间的使用。 一般常见的是有头有尾的单链表，对指针域进行反向链接，还可以形成双向链表或者循环链表。 链表和数组对比链表和数组在实际的使用过程中需要根据自身的优劣势进行选择。链表和数组的异同点也是面试中高频的考察点之一。这里对单链表和数组的区别进行了对比和总结。 3 跳表从上面的对比中可以看出，链表虽然通过增加指针域提升了自由度，但是却导致数据的查询效率恶化。特别是当链表长度很长的时候，对数据的查询还得从头依次查询，这样的效率会更低。跳表的产生就是为了解决链表过长的问题，通过增加链表的多级索引来加快原始链表的查询效率。这样的方式可以让查询的时间复杂度从 O(n) 提升至 O(logn)。 跳表通过增加的多级索引能够实现高效的动态插入和删除，其效率和红黑树和平衡二叉树不相上下。目前 redis 和 levelDB 都有用到跳表。 从上图可以看出，索引级的指针域除了指向下一个索引位置的指针，还有一个 down 指针指向低一级的链表位置，这样才能实现跳跃查询的目的。 4 栈栈是一种比较简单的数据结构，常用一句话描述其特性，后进先出。栈本身是一种线性结构，但是在这个结构中只有一个口子允许数据的进出。这种模式可以参考腔肠动物… 即进食和排泄都用一个口… 栈的常用操作包括入栈 push 和出栈 pop，对应于数据的压入和压出。还有访问栈顶数据、判断栈是否为空和判断栈的大小等。由于栈后进先出的特性，常可以作为数据操作的临时容器，对数据的顺序进行调控，与其它数据结构相结合可获得许多灵活的处理。 5 队列队列是栈的兄弟结构，与栈的后进先出相对应，队列是一种先进先出的数据结构。顾名思义，队列的数据存储是如同排队一般，先存入的数据先被压出。常与栈一同配合，可发挥最大的实力。 6 树树作为一种树状的数据结构，其数据节点之间的关系也如大树一样，将有限个节点根据不同层次关系进行排列，从而形成数据与数据之间的父子关系。常见的数的表示形式更接近 “倒挂的树”，因为它将根朝上，叶朝下。 树是图的一种，树与图的区别在于：树是没有环的，而图是可以有环的。 树的数据存储在结点中，每个结点有零个或者多个子结点。没有父结点的结点在最顶端，成为根节点；没有非根结点有且只有一个父节点；每个非根节点又可以分为多个不相交的子树。 这意味着树是具备层次关系的，父子关系清晰，家庭血缘关系明朗；这也是树与图之间最主要的区别。 别看树好像很高级，其实可看作是链表的高配版。树的实现就是对链表的指针域进行了扩充，增加了多个地址指向子结点。同时将 “链表” 竖起来，从而凸显了结点之间的层次关系，更便于分析和理解。 树可以衍生出许多的结构，若将指针域设置为双指针，那么即可形成最常见的二叉树，即每个结点最多有两个子树的树结构。二叉树根据结点的排列和数量还可进一度划分为完全二叉树、满二叉树、平衡二叉树、红黑树等。 完全二叉树：除了最后一层结点，其它层的结点数都达到了最大值；同时最后一层的结点都是按照从左到右依次排布。 满二叉树：除了最后一层，其它层的结点都有两个子结点。 平衡二叉树平衡二叉树又被称为 AVL 树，它是一棵二叉排序树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过 1，并且左右两个子树都是一棵平衡二叉树。 二叉排序树：是一棵空树，或者：若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值；若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值；它的左、右子树也分别为二叉排序树。 树的高度：结点层次的最大值 平衡因子：左子树高度 - 右子树高度 二叉排序树意味着二叉树中的数据是排好序的，顺序为左结点 &lt;根节点&lt;右结点，这表明二叉排序树的中序遍历结果是有序的。 （二叉树四种遍历方式[前序遍历、中序遍历、后序遍历、层序遍历] ） 平衡二叉树的产生是为了解决二叉排序树在插入时发生线性排列的现象。由于二叉排序树本身为有序，当插入一个有序程度十分高的序列时，生成的二叉排序树会持续在某个方向的字数上插入数据，导致最终的二叉排序树会退化为链表，从而使得二叉树的查询和插入效率恶化。 平衡二叉树的出现能够解决上述问题，但是在构造平衡二叉树时，却需要采用不同的调整方式，使得二叉树在插入数据后保持平衡。主要的四种调整方式有 LL（左旋）、RR（右旋）、LR（先左旋再右旋）、RL（先右旋再左旋）。这里先给大家介绍下简单的单旋转操作，左旋和右旋。LR 和 RL 本质上只是 LL 和 RR 的组合。 在插入一个结点后应该沿搜索路径将路径上的结点平衡因子进行修改，当平衡因子大于 1 时，就需要进行平衡化处理。从发生不平衡的结点起，沿刚才回溯的路径取直接下两层的结点，如果这三个结点在一条直线上，则采用单旋转进行平衡化，如果这三个结点位于一条折线上，则采用双旋转进行平衡化。 左旋：S 为当前需要左旋的结点，E 为当前结点的父节点。 左旋的操作可以用一句话简单表示：将当前结点 S 的左孩子旋转为当前结点父结点 E 的右孩子，同时将父结点 E 旋转为当前结点 S 的左孩子。 右旋：S 为当前需要左旋的结点，E 为当前结点的父节点。右单旋是左单旋的镜像旋转。 左旋的操作同样可以用一句话简单表示：将当前结点 S 的左孩子 E 的右孩子旋转为当前结点 S 的左孩子，同时将当前结点 S 旋转为左孩子 E 的右孩子 红黑树平衡二叉树（AVL）为了追求高度平衡，需要通过平衡处理使得左右子树的高度差必须小于等于 1。高度平衡带来的好处是能够提供更高的搜索效率，其最坏的查找时间复杂度都是 O(logN)。但是由于需要维持这份高度平衡，所付出的代价就是当对树种结点进行插入和删除时，需要经过多次旋转实现复衡。这导致 AVL 的插入和删除效率并不高。 为了解决这样的问题，能不能找一种结构能够兼顾搜索和插入删除的效率呢？这时候红黑树便申请出战了。 红黑树具有五个特性： 每个结点要么是红的要么是黑的。 根结点是黑的。 每个叶结点（叶结点即指树尾端 NIL 指针或 NULL 结点）都是黑的。 如果一个结点是红的，那么它的两个儿子都是黑的。 对于任意结点而言，其到叶结点树尾端 NIL 指针的每条路径都包含相同数目的黑结点。 红黑树通过将结点进行红黑着色，使得原本高度平衡的树结构被稍微打乱，平衡程度降低。红黑树不追求完全平衡，只要求达到部分平衡。这是一种折中的方案，大大提高了结点删除和插入的效率。C++ 中的 STL 就常用到红黑树作为底层的数据结构。 除了上面所提及的树结构，还有许多广泛应用在数据库、磁盘存储等场景下的树结构。比如 B 树、B + 树等。这里就先不介绍了诶，下次在讲述相关存储原理的时候将会着重介绍。（其实是因为懒） B树（B-tree）B树和平衡二叉树稍有不同的是B树属于多叉树又名平衡多路查找树（查找路径不只两个），数据库索引技术里大量使用者B树和B+树的数据结构。B树相对于平衡二叉树，每个节点包含的关键字增多了，特别是在B树应用到数据库中的时候，数据库充分利用了磁盘块的原理（磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次IO进行数据读取时，同一个磁盘块的数据可以一次性读取出来）把节点大小限制和充分使用在磁盘快大小范围；把树的节点关键字增多后树的层级比原来的二叉树少了，减少数据查找的次数和复杂度; 规则： 排序方式：所有节点关键字是按递增次序排列，并遵循左小右大原则； 子节点数：非叶节点的子节点数&gt;1，且&lt;=M ，且M&gt;=2，空树除外（注：M阶代表一个树节点最多有多少个查找路径，M=M路,当M=2则是2叉树,M=3则是3叉）； 关键字数：枝节点的关键字数量大于等于ceil(m/2)-1个且小于等于M-1个（注：ceil()是个朝正无穷方向取整的函数 如ceil(1.1)结果为2)； 所有叶子节点均在同一层、叶子节点除了包含了关键字和关键字记录的指针外也有指向其子节点的指针只不过其指针地址都为null对应下图最后一层节点的空格子; 查询： 如上图我要从上图中找到E字母，查找流程如下 获取根节点的关键字进行比较，当前根节点关键字为M，E&lt;M（26个字母顺序），所以往找到指向左边的子节点（二分法规则，左小右大，左边放小于当前节点值的子节点、右边放大于当前节点值的子节点）； 拿到关键字D和G，D&lt;E&lt;G 所以直接找到D和G中间的节点； 拿到E和F，因为E=E 所以直接返回关键字和指针信息（如果树结构里面没有包含所要查找的节点则返回null）； 插入： 节点拆分规则：当前是要组成一个5路查找树，那么此时m=5,关键字数必须&lt;=5-1（这里关键字数&gt;4就要进行节点拆分）； 排序规则：满足节点本身比左边节点大，比右边节点小的排序规则; 删除： 节点合并规则：当前是要组成一个5路查找树，那么此时m=5,关键字数必须大于等于ceil（5/2）（这里关键字数&lt;2就要进行节点合并）； 满足节点本身比左边节点大，比右边节点小的排序规则; 关键字数小于二时先从子节点取，子节点没有符合条件时就向向父节点取，取中间值往父节点放； B+树B+树是B树的一个升级版，相对于B树来说B+树更充分的利用了节点的空间，让查询速度更加稳定，其速度完全接近于二分法查找。查找的效率要比B树更高、更稳定。 规则： B+跟B树不同B+树的非叶子节点不保存关键字记录的指针，只进行数据索引，这样使得B+树每个非叶子节点所能保存的关键字大大增加； B+树叶子节点保存了父节点的所有关键字记录的指针，所有数据地址必须要到叶子节点才能获取到。所以每次数据查询的次数都一样； B+树叶子节点的关键字从小到大有序排列，左边结尾数据都会保存右边节点开始数据的指针。 非叶子节点的子节点数=关键字数（来源百度百科）（根据各种资料 这里有两种算法的实现方式，另一种为非叶节点的关键字数=子节点数-1（来源维基百科)，虽然他们数据排列结构不一样，但其原理还是一样的Mysql 的B+树是用第一种方式实现）; 特点： B+树的层级更少：相较于B树B+每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快； B+树查询速度更稳定：B+所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定; B+树天然具备排序功能：B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。 B+树全节点遍历更快：B+树遍历整棵树只需要遍历所有的叶子节点即可，，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描。 B树相对于B+树的优点是，如果经常访问的数据离根节点很近，而B树的非叶子节点本身存有关键字其数据的地址，所以这种数据检索的时候会要比B+树快。 7 堆了解完二叉树，再来理解堆就不是什么难事了。堆通常是一个可以被看做一棵树的数组对象。堆的具体实现一般不通过指针域，而是通过构建一个一维数组与二叉树的父子结点进行对应，因此堆总是一颗完全二叉树。 对于任意一个父节点的序号 n 来说（这里 n 从 0 算），它的子节点的序号一定是 2n+1，2n+2，因此可以直接用数组来表示一个堆。 不仅如此，堆还有一个性质：堆中某个节点的值总是不大于或不小于其父节点的值。将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。 堆常用来实现优先队列，在面试中经常考的问题都是与排序有关，比如堆排序、topK 问题等。由于堆的根节点是序列中最大或者最小值，因而可以在建堆以及重建堆的过程中，筛选出数据序列中的极值，从而达到排序或者挑选 topK 值的目的。 8 散列表散列表也叫哈希表，是一种通过键值对直接访问数据的机构。在初中，我们就学过一种能够将一个 x 值通过一个函数获得对应的一个 y 值的操作，叫做映射。散列表的实现原理正是映射的原理，通过设定的一个关键字和一个映射函数，就可以直接获得访问数据的地址，实现 O(1) 的数据访问效率。在映射的过程中，事先设定的函数就是一个映射表，也可以称作散列函数或者哈希函数。 散列表的实现最关键的就是散列函数的定义和选择。一般常用的有以下几种散列函数： 直接寻址法：取关键字或关键字的某个线性函数值为散列地址。 数字分析法：通过对数据的分析，发现数据中冲突较少的部分，并构造散列地址。例如同学们的学号，通常同一届学生的学号，其中前面的部分差别不太大，所以用后面的部分来构造散列地址。 平方取中**法**：当无法确定关键字里哪几位的分布相对比较均匀时，可以先求出关键字的平方值，然后按需要取平方值的中间几位作为散列地址。这是因为：计算平方之后的中间几位和关键字中的每一位都相关，所以不同的关键字会以较高的概率产生不同的散列地址。 取随机数法：使用一个随机函数，取关键字的随机值作为散列地址，这种方式通常用于关键字长度不同的场合。 除留取余法：取关键字被某个不大于散列表的表长 n 的数 m 除后所得的余数 p 为散列地址。这种方式也可以在用过其他方法后再使用。该函数对 m 的选择很重要，一般取素数或者直接用 n。 确定好散列函数之后，通过某个key值的确会得到一个唯一的value地址。但是却会出现一些特殊情况。即通过不同的key值可能会访问到同一个地址，这个现象称之为冲突。 冲突在发生之后，当在对不同的key值进行操作时会使得造成相同地址的数据发生覆盖或者丢失，是非常危险的。所以在设计散列表往往还需要采用冲突解决的办法。 常用的冲突处理方式有很多，常用的包括以下几种： 开放地址法（也叫开放寻址法）：实际上就是当需要存储值时，对 Key 哈希之后，发现这个地址已经有值了，这时该怎么办？不能放在这个地址，不然之前的映射会被覆盖。这时对计算出来的地址进行一个探测再哈希，比如往后移动一个地址，如果没人占用，就用这个地址。如果超过最大长度，则可以对总长度取余。这里移动的地址是产生冲突时的增列序量。 再哈希法：在产生冲突之后，使用关键字的其他部分继续计算地址，如果还是有冲突，则继续使用其他部分再计算地址。这种方式的缺点是时间增加了。 链地址法：链地址法其实就是对 Key 通过哈希之后落在同一个地址上的值，做一个链表。其实在很多高级语言的实现当中，也是使用这种方式处理冲突的。 公共溢出区：这种方式是建立一个公共溢出区，当地址存在冲突时，把新的地址放在公共溢出区里。 目前比较常用的冲突解决方法是链地址法，一般可以通过数组和链表的结合达到冲突数据缓存的目的。 左侧数组的每个成员包括一个指针，指向一个链表的头。每发生一个冲突的数据，就将该数据作为链表的节点链接到链表尾部。这样一来，就可以保证冲突的数据能够区分并顺利访问。 考虑到链表过长造成的问题，还可以使用红黑树替换链表进行冲突数据的处理操作，来提高散列表的查询稳定性。 9 图图相较于上文的几个结构可能接触的不多，但是在实际的应用场景中却经常出现。比方说交通中的线路图，常见的思维导图都可以看作是图的具体表现形式。 图结构一般包括顶点和边，顶点通常用圆圈来表示，边就是这些圆圈之间的连线。边还可以根据顶点之间的关系设置不同的权重，默认权重相同皆为 1。此外根据边的方向性，还可将图分为有向图和无向图。 图结构用抽象的图线来表示十分简单，顶点和边之间的关系非常清晰明了。但是在具体的代码实现中，为了将各个顶点和边的关系存储下来，却不是一件易事。 邻接矩阵目前常用的图存储方式为邻接矩阵，通过所有顶点的二维矩阵来存储两个顶点之间是否相连，或者存储两顶点间的边权重。 无向图的邻接矩阵是一个对称矩阵，是因为边不具有方向性，若能从此顶点能够到达彼顶点，那么彼顶点自然也能够达到此顶点。此外，由于顶点本身与本身相连没有意义，所以在邻接矩阵中对角线上皆为 0。 有向图由于边具有方向性，因此彼此顶点之间并不能相互达到，所以其邻接矩阵的对称性不再。 用邻接矩阵可以直接从二维关系中获得任意两个顶点的关系，可直接判断是否相连。但是在对矩阵进行存储时，却需要完整的一个二维数组。若图中顶点数过多，会导致二维数组的大小剧增，从而占用大量的内存空间。 而根据实际情况可以分析得，图中的顶点并不是任意两个顶点间都会相连，不是都需要对其边上权重进行存储。那么存储的邻接矩阵实际上会存在大量的 0。虽然可以通过稀疏表示等方式对稀疏性高的矩阵进行关键信息的存储，但是却增加了图存储的复杂性。 因此，为了解决上述问题，一种可以只存储相连顶点关系的邻接表应运而生。 邻接表在邻接表中，图的每一个顶点都是一个链表的头节点，其后连接着该顶点能够直接达到的相邻顶点。相较于无向图，有向图的情况更为复杂，因此这里采用有向图进行实例分析。 在邻接表中，每一个顶点都对应着一条链表，链表中存储的是顶点能够达到的相邻顶点。存储的顺序可以按照顶点的编号顺序进行。比如上图中对于顶点 B 来说，其通过有向边可以到达顶点 A 和顶点 E，那么其对应的邻接表中的顺序即 B-&gt;A-&gt;E，其它顶点亦如此。 通过邻接表可以获得从某个顶点出发能够到达的顶点，从而省去了对不相连顶点的存储空间。然而，这还不够。对于有向图而言，图中有效信息除了从顶点 “指出去” 的信息，还包括从别的顶点 “指进来” 的信息。这里的 “指出去” 和“指进来”可以用出度和入度来表示。 入度：有向图的某个顶点作为终点的次数和。 出度：有向图的某个顶点作为起点的次数和。 由此看出，在对有向图进行表示时，邻接表只能求出图的出度，而无法求出入度。这个问题很好解决，那就是增加一个表用来存储能够到达某个顶点的相邻顶点。这个表称作逆邻接表。 逆邻接表逆邻接表与邻接表结构类似，只不过图的顶点链接着能够到达该顶点的相邻顶点。也就是说，邻接表时顺着图中的箭头寻找相邻顶点，而逆邻接表时逆着图中的箭头寻找相邻顶点。 邻接表和逆邻接表的共同使用下，就能够把一个完整的有向图结构进行表示。可以发现，邻接表和逆邻接表实际上有一部分数据时重合的，因此可以将两个表合二为一，从而得到了所谓的十字链表。 十字链表十字链表似乎很简单，只需要通过相同的顶点分别链向以该顶点为终点和起点的相邻顶点即可。 但这并不是最优的表示方式。虽然这样的方式共用了中间的顶点存储空间，但是邻接表和逆邻接表的链表节点中重复出现的顶点并没有得到重复利用，反而是进行了再次存储。因此，上图的表示方式还可以进行进一步优化。 十字链表优化后，可通过扩展的顶点结构和边结构来进行正逆邻接表的存储：（下面的弧头可看作是边的箭头那端，弧尾可看作是边的圆点那端） data：用于存储该顶点中的数据； firstin 指针：用于连接以当前顶点为弧头的其他顶点构成的链表，即从别的顶点指进来的顶点； firstout 指针：用于连接以当前顶点为弧尾的其他顶点构成的链表，即从该顶点指出去的顶点； 边结构通过存储两个顶点来确定一条边，同时通过分别代表这两个顶点的指针来与相邻顶点进行链接： tailvex：用于存储作为弧尾的顶点的编号； headvex：用于存储作为弧头的顶点的编号； headlink 指针：用于链接下一个存储作为弧头的顶点的节点； taillink 指针：用于链接下一个存储作为弧尾的顶点的节点； 以上图为例子，对于顶点 A 而言，其作为起点能够到达顶点 E。因此在邻接表中顶点 A 要通过边AE（即边 04）指向顶点 E，顶点 A 的firstout指针需要指向边 04 的tailvex。同时，从 B 出发能够到达 A，所以在逆邻接表中顶点 A 要通过边AB（即边 10）指向 B，顶点 A 的firstin指针需要指向边 10 的弧头，即headlink指针。依次类推。 十字链表采用了一种看起来比较繁乱的方式对边的方向性进行了表示，能够在尽可能降低存储空间的情况下增加指针保留顶点之间的方向性。具体的操作可能一时间不好弄懂，建议多看几次上图，弄清指针指向的意义，明白正向和逆向邻接表的表示。 10 总结数据结构博大精深，没有高等数学的讳莫如深，也没有量子力学的玄乎其神，但是其在计算机科学的各个领域都具有强大的力量。本文试图采用图解的方式对九种数据结构进行理论上的介绍，但是其实这都是不够的。 即便是简单的数组、栈、队列等结构，在实际使用以及底层实现上都会有许多优化设计以及使用技巧，这意味着还需要真正把它们灵活的用起来，才能够算是真正意义上的熟悉和精通。但是本文可以作为常见数据结构的一个总结，当你对某些结构有些淡忘的时候，不妨重新回来看看。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"二叉树的四种遍历方式","slug":"二叉树的四种遍历方式","date":"2020-07-10T09:07:59.381Z","updated":"2023-02-06T11:44:06.493Z","comments":true,"path":"2020/07/10/二叉树的四种遍历方式/","link":"","permalink":"https://topone233.github.io/2020/07/10/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%9B%9B%E7%A7%8D%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F/","excerpt":"","text":"原文地址 www.cnblogs.com 二叉树的四种遍历方式： 二叉树的遍历（traversing binary tree）是指从根结点出发，按照某种次序依次访问二叉树中所有的结点，使得每个结点被访问依次且仅被访问一次。四种遍历方式分别为：先序遍历、中序遍历、后序遍历、层序遍历。 树的相关术语：节点的度：一个节点含有的子树的个数称为该节点的度； 叶节点：度为0的节点； 树的度：一棵树中，最大的节点的度； 森林：由m（m&gt;=0）棵互不相交的树的集合 树的符号表现法：（1（2（4（5，6）），3） 解读：祖先1的子节点2（子节点4（叶节点5，6）），3。同层子树间用逗号隔开。 如何创建二叉树遍历之前，我们首先介绍一下，如何创建一个二叉树，在这里我们用的是先建左树在建右树的方法， 首先要声明结点 TreeNode 类，代码如下： public class TreeNode { public int data; public TreeNode leftChild; public TreeNode rightChild; public TreeNode(int data){ this.data = data; } } 再来创建一颗二叉树： /** * 构建二叉树 * @param list 输入序列 * @return */ public static TreeNode createBinaryTree(LinkedList&lt;Integer&gt; list){ TreeNode node = null; if(list == null || list.isEmpty()){ return null; } Integer data = list.removeFirst(); if(data!=null){ node = new TreeNode(data); node.leftChild = createBinaryTree(list); node.rightChild = createBinaryTree(list); } return node; } 接下来我们按照上面列的顺序一一讲解， 先序遍历首先来看先序遍历，所谓的先序遍历就是先访问根节点，在访问左节点，最后访问右节点， 如上图所示，前序遍历结果为：ABDFECGHI 实现代码如下： /** * 二叉树前序遍历 根-&gt; 左-&gt; 右 * @param node 二叉树节点 */ public static void preOrderTraveral(TreeNode node){ if(node == null){ return; } System.out.print(node.data+&quot; &quot;); preOrderTraveral(node.leftChild); preOrderTraveral(node.rightChild); } 中序遍历再者就是中序遍历，所谓的中序遍历就是先访问左节点，再访问根节点，最后访问右节点， 如上图所示，中序遍历结果为：DBEFAGHCI（G没有左子树，所以直接访问G，而不是访问H） 实现代码如下： /** * 二叉树中序遍历 左-&gt; 根-&gt; 右 * @param node 二叉树节点 */ public static void inOrderTraveral(TreeNode node){ if(node == null){ return; } inOrderTraveral(node.leftChild); System.out.print(node.data+&quot; &quot;); inOrderTraveral(node.rightChild); } 后序遍历最后就是后序遍历，所谓的后序遍历就是先访问左节点，再访问右节点，最后访问根节点。 如上图所示，前序遍历结果为：DEFBHGICA 实现代码如下： /** * 二叉树后序遍历 左-&gt; 右-&gt; 根 * @param node 二叉树节点 */ public static void postOrderTraveral(TreeNode node){ if(node == null){ return; } postOrderTraveral(node.leftChild); postOrderTraveral(node.rightChild); System.out.print(node.data+&quot; &quot;); } 非递归的前中后序遍历讲完上面三种非递归的方法，下面再给大家讲讲非递归是如何实现前中后序遍历的 非递归前序遍历还是一样，先看非递归前序遍历 首先申请一个新的栈，记为 stack； 声明一个结点 treeNode，让其指向 node 结点； 如果 treeNode 的不为空，将 treeNode 的值打印，并将 treeNode 入栈，然后让 treeNode 指向 treeNode 的右结点， 重复步骤 3，直到 treenode 为空； 然后出栈，让 treeNode 指向 treeNode 的右孩子 重复步骤 3，直到 stack 为空. 实现代码如下： public static void preOrderTraveralWithStack(TreeNode node){ Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode treeNode = node; while(treeNode!=null || !stack.isEmpty()){ //迭代访问节点的左孩子，并入栈 while(treeNode != null){ System.out.print(treeNode.data+&quot; &quot;); stack.push(treeNode); treeNode = treeNode.leftChild; } //如果节点没有左孩子，则弹出栈顶节点，访问节点右孩子 if(!stack.isEmpty()){ treeNode = stack.pop(); treeNode = treeNode.rightChild; } } } 非递归中序遍历中序遍历非递归，在此不过多叙述具体步骤了， 具体过程： 申请一个新栈，记为 stack，申请一个变量 cur，初始时令 treeNode 为头节点； 先把 treeNode 节点压入栈中，对以 treeNode 节点为头的整棵子树来说，依次把整棵树的左子树压入栈中，即不断令 treeNode=treeNode.leftChild，然后重复步骤 2； 不断重复步骤 2，直到发现 cur 为空，此时从 stack 中弹出一个节点记为 treeNode，打印 node 的值，并让 treeNode= treeNode.right，然后继续重复步骤 2； 当 stack 为空并且 cur 为空时结束。 public static void inOrderTraveralWithStack(TreeNode node){ Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode treeNode = node; while(treeNode!=null || !stack.isEmpty()){ while(treeNode != null){ stack.push(treeNode); treeNode = treeNode.leftChild; } if(!stack.isEmpty()){ treeNode = stack.pop(); System.out.print(treeNode.data+&quot; &quot;); treeNode = treeNode.rightChild; } } } 非递归后序遍历后序遍历这里较前两者实现复杂一点，我们需要一个标记为来记忆我们此时节点上一个节点，具体看代码注释 public static void postOrderTraveralWithStack(TreeNode node){ Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode treeNode = node; TreeNode lastVisit = null; //标记每次遍历最后一次访问的节点 while(treeNode!=null || !stack.isEmpty()){//节点不为空，结点入栈，并且指向下一个左孩子 while(treeNode!=null){ stack.push(treeNode); treeNode = treeNode.leftChild; } //栈不为空 if(!stack.isEmpty()){ //出栈 treeNode = stack.pop(); /** * 这块就是判断treeNode是否有右孩子， * 如果没有输出treeNode.data，让lastVisit指向treeNode，并让treeNode为空 * 如果有右孩子，将当前节点继续入栈，treeNode指向它的右孩子,继续重复循环 */ if(treeNode.rightChild == null || treeNode.rightChild == lastVisit) { System.out.print(treeNode.data + &quot; &quot;); lastVisit = treeNode; treeNode = null; }else{ stack.push(treeNode); treeNode = treeNode.rightChild; } } } } 层序遍历最后再介绍一下层序遍历 具体步骤如下： 首先申请一个新的队列，记为 queue； 将头结点 head 压入 queue 中； 每次从 queue 中出队，记为 node，然后打印 node 值，如果 node 左孩子不为空，则将左孩子入队；如果 node 的右孩子不为空，则将右孩子入队； 重复步骤 3，直到 queue 为空。 实现代码如下： public static void levelOrder(TreeNode root){ LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()){ root = queue.pop(); System.out.print(root.data+&quot; &quot;); if(root.leftChild!=null) queue.add(root.leftChild); if(root.rightChild!=null) queue.add(root.rightChild); } }","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"二叉树","slug":"二叉树","permalink":"https://topone233.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"时间复杂度","slug":"时间复杂度","date":"2020-07-06T16:00:00.000Z","updated":"2023-02-06T11:45:42.478Z","comments":true,"path":"2020/07/07/时间复杂度/","link":"","permalink":"https://topone233.github.io/2020/07/07/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/","excerpt":"","text":"时间复杂度 若存在函数 f(n)，使得当n趋近于无穷大时，T(n) / f(n) 的极限值为不等于零的常数，则称 f(n) 是 T(n) 的同数量级函数。记作 T(n) = O( f(n) )，称 O( f(n) ) 为算法的渐进时间复杂度(asymptotic time complexity)，简称时间复杂度。 OK，我们先引出官方的定义先混个眼熟。接下来看几个通俗易懂的案例： 场景1：问：给李华一个长n寸的面包，每3天吃掉1寸，几天吃完？ 答：3 X n = 3n 天。 如果用函数表达这个相对时间：T(n) = 3n 场景2：问：给李华一个长16寸的面包，每5天吃掉面包剩余长度的一半。第一次吃掉8寸，第二次吃4寸，…… 几天可以吃得只剩1寸？ 答：5 X log16 = 20天。 T(n) = 5log n 场景3：问：给李华一个长10寸的面包和一个鸡腿，每2天吃掉1个鸡腿，几天吃完鸡腿？ 答：2天 （与面包一点关系没有）。 T(n) = 2 场景4：问：给李华一个长10寸的面包，吃掉第一个一寸需要1天时间，吃掉第二个一寸需要2天时间，吃掉第三个一寸需要3天时间…..每多吃一寸，所花的时间也多一天，几天吃完？ 答：1到10的和 = 55天 1+2+3+……+ n-1 + n = (1+n)*n/2 = 0.5n^2 + 0.5n 。 T(n) = 0.5n^2 + 0.5n 相信通过以上4个案例已经了解了： T(n) 基本操作执行次数的函数。 那么如何推导出时间复杂度呢？有如下几个原则： 如果运行时间是常数量级，用常数1表示； 只保留时间函数中的最高阶项； 如果最高阶项存在，则省去最高阶项前面的系数。 渐进分析（asymptotic analysis）：忽略掉那些依赖于机器的常量，不去检查实际的运行时间，而是关注运行时间的增长。 在看刚才的四个场景： 场景1： T(n) = 3n 最高阶项为3n，省去系数3，时间复杂度为：O(n) 场景2： T(n) = 5log n 最高阶项为5log n，省去系数5，时间复杂度为：O(log n) 场景3： T(n) = 2 只有常数量级，时间复杂度为：O(1) 场景4： T(n) = 0.5n^2 + 0.5n 最高阶项为0.5n^2，省去系数0.5，时间复杂度为：O(n^2) O（1）&lt; O（log n）&lt; O（n）&lt; O（n^2） 空间复杂度空间复杂度：即程序中变量的个数 位运算位运算在算法中很有用，速度可以比四则运算快很多。 左移 &lt;&lt;10 &lt;&lt; 1 结果： 20 左移就是将二进制全部往左移动。10的二进制：1010，左移一位：10100，十进制就是20。 基本可以把左移看作：a * (2 ^ b) 右移 &gt;&gt;10 &gt;&gt; 1 结果： 5 右移就是将二进制全部往右移动，并去除多余的右边。右移一位：101，十进制就是5 基本可以把右移看作：a / (2 ^ b) 右移很好用，比如可以用在二分算法中取中间值","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Hello World！（hexo配置记录）","slug":"Hello World！（hexo配置记录）","date":"2020-07-01T02:25:00.000Z","updated":"2023-02-06T11:47:13.960Z","comments":true,"path":"2020/07/01/Hello World！（hexo配置记录）/","link":"","permalink":"https://topone233.github.io/2020/07/01/Hello%20World%EF%BC%81%EF%BC%88hexo%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95%EF%BC%89/","excerpt":"","text":"Hello World！（hexo配置记录）以前在网上冲浪的时候只是不经意间发现许多让人眼前一亮的blog，由此萌生了create my blog的想法。 目前的blog搭建： CSDN/博客园平台 但我个人不是很喜欢这种，首先是太丑了（没错，就是你，CSDN）。 其次依托于平台，虽然只需要创作就行，但是感觉不是属于自己的，没有归属感 独立blog 租云服务器、买域名，还要管理维护，个人感觉略微有些费事，精力有限，还是简单点好 hexo 依托于Github，也是我目前选择的，配置相对简单快捷，主题丰富 hexo配置步骤： 安装Git 安装Node.js 安装hexo 生成ssh并添加到GitHub 部署项目 上传到GitHub 修改主题 1.安装Git下载地址 安装步骤：双击下载的exe文件，一路next就行 2.安装Node.jsHexo是基于nodeJS环境的静态博客，npm是必备的 下载地址 安装步骤：下载好msi文件后，双击打开安装，也是一路next，不过在Custom Setup这一步记得选 Add to PATH ,这样你就不用自己去配置电脑上环境变量了 3.安装hexo 创建一个源文件夹，然后cd到该文件夹下 安装hexo： npm i -g hexo hexo -v 查看版本，检查是否安装成功 hexo init 初始化，初始化完成后可在文件夹下看到文件 这里要说下，npm install出现一直停留在”fetchMetadata: sill resolveWithNewModule find-cache-dir@”解决方法，更换成淘宝的源（反正我是解决了） //修改为淘宝源 npm config set registry https://registry.npm.taobao.org //配置后可通过下面方式来验证是否成功 npm config get registry //或 npm info express4.生成ssh并添加到GitHubSSH密钥可以防止其他人恶意部署文件到你的仓库 首先要有GitHub账号，没有的自行注册 创建一个仓库repository，名称为youname.github.io 在gitbash中，配置GitHub账号信息 //配置你的GitHub账号信息 git config --global user.name &quot;YourName&quot; git config --global user.email &quot;YourEmail&quot;创建ssh //创建ssh ssh-keygen -t rsa -C &quot;youremail@xx.com&quot;生成ssh，在gitbash中切换到文件目录cat读取 //读取ssh文件内容 cat id_rsa.pub 全部复制（包括开头的ssh-rsa，和尾部的email）到GitHub 配置ssh，title随便起 5.部署项目修改hexo的_config.yml文件配置信息（直接复制，只需要修改url即可） deploy: type: git repo: https://github.com/YourgithubName/YourgithubName.github.io.git branch: master回到gitbash，进入hexo目录 hexo clean hexo generate hexo server 也可简写成：hexo clean &amp;&amp; hexo g &amp;&amp; hexo s这时，在http://localhost:4000就可以看到默认页面 6.上传到GitHubnpm install hexo-deployer-git --save将写好的文章部署到GitHub服务器，执行命令 hexo clean hexo generate hexo deploy 第一次deploy要输入GitHubusername和password完成后https://yourgithubname.github.io，查看即可 7.修改主题hexo官网有推荐很多很多主题，自行选择 8.更新hexo clean &amp;&amp; hexo g &amp;&amp; hexo s hexo deploy","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://topone233.github.io/tags/hexo/"}]}],"categories":[{"name":"Windows","slug":"windows","permalink":"https://topone233.github.io/categories/windows/"},{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/categories/%E8%B5%84%E6%BA%90/"},{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"},{"name":"MySql","slug":"mysql","permalink":"https://topone233.github.io/categories/mysql/"},{"name":"算法","slug":"算法","permalink":"https://topone233.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/categories/nosql/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/categories/sql/"},{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Windows 10","slug":"windows-10","permalink":"https://topone233.github.io/tags/windows-10/"},{"name":"bat","slug":"bat","permalink":"https://topone233.github.io/tags/bat/"},{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/tags/%E8%B5%84%E6%BA%90/"},{"name":"python","slug":"python","permalink":"https://topone233.github.io/tags/python/"},{"name":"GitHub","slug":"github","permalink":"https://topone233.github.io/tags/github/"},{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"SpringBoot","slug":"springboot","permalink":"https://topone233.github.io/tags/springboot/"},{"name":"WebSocket","slug":"websocket","permalink":"https://topone233.github.io/tags/websocket/"},{"name":"MySql","slug":"mysql","permalink":"https://topone233.github.io/tags/mysql/"},{"name":"算法","slug":"算法","permalink":"https://topone233.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"动态规划","slug":"动态规划","permalink":"https://topone233.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"背包问题","slug":"背包问题","permalink":"https://topone233.github.io/tags/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/tags/nosql/"},{"name":"Redis","slug":"redis","permalink":"https://topone233.github.io/tags/redis/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/tags/sql/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"},{"name":"Maven","slug":"maven","permalink":"https://topone233.github.io/tags/maven/"},{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"SSL","slug":"ssl","permalink":"https://topone233.github.io/tags/ssl/"},{"name":"安全","slug":"安全","permalink":"https://topone233.github.io/tags/%E5%AE%89%E5%85%A8/"},{"name":"TCP","slug":"tcp","permalink":"https://topone233.github.io/tags/tcp/"},{"name":"滑动窗口","slug":"滑动窗口","permalink":"https://topone233.github.io/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"},{"name":"JVM","slug":"jvm","permalink":"https://topone233.github.io/tags/jvm/"},{"name":"集合","slug":"集合","permalink":"https://topone233.github.io/tags/%E9%9B%86%E5%90%88/"},{"name":"多线程","slug":"多线程","permalink":"https://topone233.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"IO","slug":"io","permalink":"https://topone233.github.io/tags/io/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"排序算法","slug":"排序算法","permalink":"https://topone233.github.io/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"name":"动态代理","slug":"动态代理","permalink":"https://topone233.github.io/tags/%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"},{"name":"HashMap","slug":"hashmap","permalink":"https://topone233.github.io/tags/hashmap/"},{"name":"二叉树","slug":"二叉树","permalink":"https://topone233.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"hexo","slug":"hexo","permalink":"https://topone233.github.io/tags/hexo/"}]}