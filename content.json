{"meta":{"title":"QSX1C","subtitle":"  ","description":"","author":"QSX1C","url":"https://topone233.github.io","root":"/"},"pages":[{"title":"about","date":"2020-07-12T08:05:04.000Z","updated":"2020-07-12T08:05:18.584Z","comments":true,"path":"about/index.html","permalink":"https://topone233.github.io/about/","excerpt":"","text":""},{"title":"tags","date":"2020-07-12T08:04:26.000Z","updated":"2020-07-12T08:04:53.027Z","comments":true,"path":"tags/index.html","permalink":"https://topone233.github.io/tags/","excerpt":"","text":""},{"title":"categories","date":"2020-07-12T08:02:56.000Z","updated":"2020-07-12T08:04:10.280Z","comments":true,"path":"categories/index.html","permalink":"https://topone233.github.io/categories/","excerpt":"","text":""},{"title":"contact","date":"2020-07-12T08:05:31.000Z","updated":"2020-07-12T08:05:48.424Z","comments":true,"path":"contact/index.html","permalink":"https://topone233.github.io/contact/","excerpt":"","text":""},{"title":"friends","date":"2020-09-21T13:29:00.000Z","updated":"2020-09-21T13:29:54.435Z","comments":true,"path":"friends/index.html","permalink":"https://topone233.github.io/friends/","excerpt":"","text":""}],"posts":[{"title":"Innodb 中的事务隔离级别和锁的关系 - 美团技术团队","slug":"Innodb 中的事务隔离级别和锁的关系 - 美团技术团队","date":"2020-09-20T01:49:35.432Z","updated":"2020-09-21T04:26:26.319Z","comments":true,"path":"2020/09/20/Innodb 中的事务隔离级别和锁的关系 - 美团技术团队/","link":"","permalink":"https://topone233.github.io/2020/09/20/Innodb%20%E4%B8%AD%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8C%E9%94%81%E7%9A%84%E5%85%B3%E7%B3%BB%20-%20%E7%BE%8E%E5%9B%A2%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F/","excerpt":"","text":"原文地址 [tech.meituan.com](https://tech.meituan.com/2014/08/20/innodb-lock.html) 前言 我们都知道事务的几种性质，数据库为了维护这些性质，尤其是一致性和隔离性，一般使用加锁这种方式。同时数据库又是个高并发的应用，同一时间会有大量的并发访问，如果加锁过度，会极大的降低并发处理能力。所以对于加锁的处理，可以说就是数据库对于事务处理的精髓所在。这里通过分析 MySQL 中 InnoDB 引擎的加锁机制，来抛砖引玉，让读者更好的理解，在事务处理中数据库到底做了什么。 一次封锁 or 两段锁？因为有大量的并发访问，为了预防死锁，一般应用中推荐使用一次封锁法，就是在方法的开始阶段，已经预先知道会用到哪些数据，然后全部锁住，在方法运行之后，再全部解锁。这种方式可以有效的避免循环死锁，但在数据库中却不适用，因为在事务开始阶段，数据库并不知道会用到哪些数据。 数据库遵循的是两段锁协议，将事务分成两个阶段，加锁阶段和解锁阶段（所以叫两段锁） 加锁阶段：在该阶段可以进行加锁操作。在对任何数据进行读操作之前要申请并获得 S 锁（共享锁，其它事务可以继续加共享锁，但不能加排它锁），在进行写操作之前要申请并获得 X 锁（排它锁，其它事务不能再获得任何锁）。加锁不成功，则事务进入等待状态，直到加锁成功才继续执行。 解锁阶段：当事务释放了一个封锁以后，事务进入解锁阶段，在该阶段只能进行解锁操作不能再进行加锁操作。 事务加锁 / 解锁处理begin；insert into test …..加 insert 对应的锁update test set…加 update 对应的锁delete from test ….加 delete 对应的锁commit;事务提交时，同时释放 insert、update、delete 对应的锁 这种方式虽然无法避免死锁，但是两段锁协议可以保证事务的并发调度是串行化（串行化很重要，尤其是在数据恢复和备份的时候）的。 事务中的加锁方式事务的四种隔离级别脏读（Dirty Read）：一个事务读取到另一个事务未提交的数据，如果B事务回滚，那么A事务读到的数据根本不是合法的，称为脏读。比如A在B超市买鞋，A汇钱给B，汇钱这个操作还没有提交，A告诉B我打钱了，B查了一下，发现了A的汇的钱。给了A鞋子，A立刻回滚，B发现自己账户上面没有钱。 不可重复读（NonRepeatable Read）：A事务读取了B事务已经提交的更改（或删除）数据，多次读取结果不同— 行级别的问题。A去超市购物，结账时系统读到卡里有100元，而此时A的老婆正在网上转账，把A卡里的100元转到了另一账户，并在A前提交了事务，此时系统检查到A的工资卡里已经没有钱了，A非常纳闷，明明卡里有钱… 幻读（Phantom Read）：一个事务内读取到了别的事务插入的数据，导致前后读取不一致 — 表级别的问题。A平时还挺节俭，A的老婆在银行部门，她经常通过银行系统查看A的消费记录。有一天，她查到A的卡消费是80元，但是A此时正在外面胡吃海喝，消费了1000元，A的老婆打印账单时显示A的消费记录是1080元，老婆很诧异，以为出现了幻觉，幻读就这样产生了。 在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。我们的数据库锁，也是为了构建这些隔离级别存在的。 隔离级别脏读（Dirty Read）不可重复读（NonRepeatable Read）幻读（Phantom Read）未提交读（Read uncommitted）可能可能可能已提交读（Read committed）不可能可能可能可重复读（Repeatable read）不可能不可能可能可串行化（Serializable ）不可能不可能不可能 未提交读 (Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 提交读 (Read Committed)：只能读取到已经提交的数据。Oracle 等多数数据库默认都是该级别 (不重复读) 可重复读 (Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB 默认级别。在 SQL 标准中，该隔离级别消除了不可重复读，但是还存在幻象读 串行读 (Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞 Read Uncommitted 这种级别，数据库一般都不会用，而且任何操作都不会加锁，这里就不讨论了。 MySQL 中锁的种类MySQL 中锁的种类很多，有常见的表锁和行锁，也有新加入的 Metadata Lock 等等, 表锁是对一整张表加锁，虽然可分为读锁和写锁，但毕竟是锁住整张表，会导致并发能力下降，一般是做 ddl 处理时使用。 行锁则是锁住数据行，这种加锁方法比较复杂，但是由于只锁住有限的数据，对于其它数据不加限制，所以并发能力强，MySQL 一般都是用行锁来处理并发事务。这里主要讨论的也就是行锁。 Read Committed（读取提交内容）在 RC 级别中，数据的读取都是不加锁的，但是数据的写入、修改和删除是需要加锁的。效果如下 MySQL&gt; show create table class\\_teacher \\\\G\\\\ Table: class\\_teacher Create Table: CREATE TABLE \\`class\\_teacher\\` ( \\`id\\` int(11) NOT NULL AUTO\\_INCREMENT, \\`class\\_name\\` varchar(100) COLLATE utf8mb4\\_unicode\\_ci NOT NULL, \\`teacher\\_id\\` int(11) NOT NULL, PRIMARY KEY (\\`id\\`), KEY \\`idx\\_teacher\\_id\\` (\\`teacher\\_id\\`) ) ENGINE=InnoDB AUTO\\_INCREMENT=5 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4\\_unicode\\_ci 1 row in set (0.02 sec) MySQL&gt; select \\* from class\\_teacher; + | id | class\\_name | teacher\\_id | + | 1 | 初三一班 | 1 | | 3 | 初二一班 | 2 | | 4 | 初二二班 | 2 | + 由于 MySQL 的 InnoDB 默认是使用的 RR 级别，所以我们先要将该 session 开启成 RC 级别，并且设置 binlog 的模式 SET session transaction isolation level read committed; SET SESSION binlog\\_format = &#39;ROW&#39;;（或者是MIXED）事务 A事务 Bbegin;begin;update class_teacher set class_name=‘初三二班’ where teacher_id=1;update class_teacher set class_name=‘初三三班’ where teacher_id=1;commit; 为了防止并发过程中的修改冲突，事务 A 中 MySQL 给 teacher_id=1 的数据行加锁，并一直不 commit（释放锁），那么事务 B 也就一直拿不到该行锁，wait 直到超时。 这时我们要注意到，teacher_id 是有索引的，如果是没有索引的 class_name 呢？update class_teacher set teacher_id=3 where class_name = ‘初三一班’; 那么 MySQL 会给整张表的所有数据行的加行锁。这里听起来有点不可思议，但是当 sql 运行的过程中，MySQL 并不知道哪些数据行是 class_name = ‘初三一班’的（没有索引嘛），如果一个条件无法通过索引快速过滤，存储引擎层面就会将所有记录加锁后返回，再由 MySQL Server 层进行过滤。 但在实际使用过程当中，MySQL 做了一些改进，在 MySQL Server 过滤条件，发现不满足后，会调用 unlock_row 方法，把不满足条件的记录释放锁 (违背了二段锁协议的约束)。这样做，保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。可见即使是 MySQL，为了效率也是会违反规范的。（参见《高性能 MySQL》中文第三版 p181） 这种情况同样适用于 MySQL 的默认隔离级别 RR。所以对一个数据量很大的表做批量修改的时候，如果无法使用相应的索引，MySQL Server 过滤数据的的时候特别慢，就会出现虽然没有修改某些行的数据，但是它们还是被锁住了的现象。 Repeatable Read（可重读）这是 MySQL 中 InnoDB 默认的隔离级别。我们姑且分 “读” 和“写”两个模块来讲解。 读读就是可重读，可重读这个概念是一事务的多个实例在并发读取数据时，会看到同样的数据行，有点抽象，我们来看一下效果。 RC（不可重读）模式下的展现 事务 A事务 Bbegin;begin;select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三二班12初三一班1&nbsp;&nbsp;update class_teacher set class_name='初三三班' where id=1;&nbsp;&nbsp;commit;select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三三班12初三一班1读到了事务 B 修改的数据，和第一次查询的结果不一样，是不可重读的。&nbsp;commit;&nbsp; 事务 B 修改 id=1 的数据提交之后，事务 A 同样的查询，后一次和前一次的结果不一样，这就是不可重读（重新读取产生的结果不一样）。这就很可能带来一些问题，那么我们来看看在 RR 级别中 MySQL 的表现： 事务 A事务 B事务 Cbegin;begin;begin;select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三二班12初三一班1&nbsp;update class_teacher set class_name='初三三班' where id=1;commit;&nbsp;&nbsp;&nbsp;insert into class_teacher values (null,'初三三班',1);commit;select id,class_name,teacher_id from class_teacher where teacher_id=1;idclass_nameteacher_id1初三二班12初三一班1没有读到事务 B 修改的数据，和第一次 sql 读取的一样，是可重复读的。没有读到事务 C 新添加的数据。&nbsp;&nbsp;commit;&nbsp;&nbsp; 我们注意到，当 teacher_id=1 时，事务 A 先做了一次读取，事务 B 中间修改了 id=1 的数据，并 commit 之后，事务 A 第二次读到的数据和第一次完全相同。所以说它是可重读的。那么 MySQL 是怎么做到的呢？这里姑且卖个关子，我们往下看。 不可重复读和幻读的区别很多人容易搞混不可重复读和幻读，确实这两者有些相似。但不可重复读重点在于 update 和 delete，而幻读的重点在于 insert。 如果使用锁机制来实现这两种隔离级别，在可重复读中，该 sql 第一次读取到数据后，就将这些数据加锁，其它事务无法修改这些数据，就可以实现可重复读了。但这种方法却无法锁住 insert 的数据，所以当事务 A 先前读取了数据，或者修改了全部数据，事务 B 还是可以 insert 数据提交，这时事务 A 就会发现莫名其妙多了一条之前没有的数据，这就是幻读，不能通过行锁来避免。需要 Serializable 隔离级别 ，读用读锁，写用写锁，读锁和写锁互斥，这么做可以有效的避免幻读、不可重复读、脏读等问题，但会极大的降低数据库的并发能力。 所以说不可重复读和幻读最大的区别，就在于如何通过锁机制来解决他们产生的问题。 上文说的，是使用悲观锁机制来处理这两种问题，但是 MySQL、ORACLE、PostgreSQL 等成熟的数据库，出于性能考虑，都是使用了以乐观锁为理论基础的 MVCC（多版本并发控制）来避免这两种问题。 悲观锁和乐观锁 悲观锁 正如其名，它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。 在悲观锁的情况下，为了保证事务的隔离性，就需要一致性锁定读。读取数据时给加锁，其它事务无法修改这些数据。修改删除数据时也要加锁，其它事务无法读取这些数据。 乐观锁 相对悲观锁而言，乐观锁机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。但随之而来的就是数据库性能的大量开销，特别是对长事务而言，这样的开销往往无法承受。 而乐观锁机制在一定程度上解决了这个问题。乐观锁，大多是基于数据版本（ Version ）记录机制实现。何谓数据版本？即为数据增加一个版本标识，在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据。 要说明的是，MVCC 的实现没有固定的规范，每个数据库都会有不同的实现方式，这里讨论的是 InnoDB 的 MVCC。 MVCC 在 MySQL 的 InnoDB 中的实现在 InnoDB 中，会在每行数据后添加两个额外的隐藏的值来实现 MVCC，这两个值一个记录这行数据何时被创建，另外一个记录这行数据何时过期（或者被删除）。 在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 在可重读 Repeatable reads 事务隔离级别下： SELECT 时，读取创建版本号 &lt;= 当前事务版本号，删除版本号为空或&gt; 当前事务版本号。 INSERT 时，保存当前事务版本号为行的创建版本号 DELETE 时，保存当前事务版本号为行的删除版本号 UPDATE 时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行 通过 MVCC，虽然每行记录都需要额外的存储空间，更多的行检查工作以及一些额外的维护工作，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，也只锁住必要行。 我们不管从数据库方面的教课书中学到，还是从网络上看到，大都是上文中事务的四种隔离级别这一模块列出的意思，RR 级别是可重复读的，但无法解决幻读，而只有在 Serializable 级别才能解决幻读。于是我就加了一个事务 C 来展示效果。在事务 C 中添加了一条 teacher_id=1 的数据 commit，RR 级别中应该会有幻读现象，事务 A 在查询 teacher_id=1 的数据时会读到事务 C 新加的数据。但是测试后发现，在 MySQL 中是不存在这种情况的，在事务 C 提交后，事务 A 还是不会读到这条数据。可见在 MySQL 的 RR 级别中，是解决了幻读的读问题的。参见下图 innodb_lock_1 读问题解决了，根据 MVCC 的定义，并发提交数据时会出现冲突，那么冲突时如何解决呢？我们再来看看 InnoDB 中 RR 级别对于写数据的处理。 “读”与 “读” 的区别可能有读者会疑惑，事务的隔离级别其实都是对于读数据的定义，但到了这里，就被拆成了读和写两个模块来讲解。这主要是因为 MySQL 中的读，和事务隔离级别中的读，是不一样的。 我们且看，在 RR 级别中，通过 MVCC 机制，虽然让数据变得可重复读，但我们读到的数据可能是历史数据，是不及时的数据，不是数据库当前的数据！这在一些对于数据的时效特别敏感的业务中，就很可能出问题。 对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，而读取数据库当前版本数据的方式，叫当前读 (current read)。很显然，在 MVCC 中： 快照读：就是 select select * from table ….; 当前读：特殊的读操作，插入 / 更新 / 删除操作，属于当前读，处理的都是当前的数据，需要加锁。 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete; 事务的隔离级别实际上都是定义了当前读的级别，MySQL 为了减少锁处理（包括等待其它锁）的时间，提升并发能力，引入了快照读的概念，使得 select 不用加锁。而 update、insert 这些 “当前读”，就需要另外的模块来解决了。 写（” 当前读”）事务的隔离级别中虽然只定义了读数据的要求，实际上这也可以说是写数据的要求。上文的 “读”，实际是讲的快照读；而这里说的“写” 就是当前读了。 为了解决当前读中的幻读问题，MySQL 事务使用了 Next-Key 锁。 Next-Key 锁Next-Key 锁是行锁和 GAP（间隙锁）的合并，行锁上文已经介绍了，接下来说下 GAP 间隙锁。 行锁可以防止不同事务版本的数据修改提交时造成数据冲突的情况。但如何避免别的事务插入数据就成了问题。我们可以看看 RR 级别和 RC 级别的对比 RC 级别： 事务 A事务 Bbegin;begin;select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三二班30&nbsp;update class_teacher set class_name='初三四班' where teacher_id=30;insert into class_teacher values (null,'初三二班',30);commit;select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三四班3010初三二班30&nbsp; RR 级别： 事务 A事务 Bbegin;begin;select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三二班30&nbsp;update class_teacher set class_name='初三四班' where teacher_id=30;insert into class_teacher values (null,'初三二班',30);waiting....select id,class_name,teacher_id from class_teacher where teacher_id=30;idclass_nameteacher_id2初三四班30&nbsp;commit;事务 Acommit 后，事务 B 的 insert 执行。 通过对比我们可以发现，在 RC 级别中，事务 A 修改了所有 teacher_id=30 的数据，但是当事务 Binsert 进新数据后，事务 A 发现莫名其妙多了一行 teacher_id=30 的数据，而且没有被之前的 update 语句所修改，这就是 “当前读” 的幻读。 RR 级别中，事务 A 在 update 后加锁，事务 B 无法插入新数据，这样事务 A 在 update 前后读的数据保持一致，避免了幻读。这个锁，就是 Gap 锁。 MySQL 是这么实现的： 在 class_teacher 这张表中，teacher_id 是个索引，那么它就会维护一套 B + 树的数据关系，为了简化，我们用链表结构来表达（实际上是个树形结构，但原理相同） innodb_lock_2 如图所示，InnoDB 使用的是聚集索引，teacher_id 身为二级索引，就要维护一个索引字段和主键 id 的树状结构（这里用链表形式表现），并保持顺序排列。 Innodb 将这段数据分成几个个区间 (negative infinity, 5], (5,30], (30,positive infinity)； update class_teacher set class_name=‘初三四班’ where teacher_id=30; 不仅用行锁，锁住了相应的数据行；同时也在两边的区间，（5,30] 和（30，positive infinity），都加入了 gap 锁。这样事务 B 就无法在这个两个区间 insert 进新数据。 受限于这种实现方式，Innodb 很多时候会锁住不需要锁的区间。如下所示： 事务 A事务 B事务 Cbegin;begin;begin;select id,class_name,teacher_id from class_teacher;idclass_nameteacher_id1初三一班52初三二班30&nbsp;&nbsp;update class_teacher set class_name='初一一班' where teacher_id=20;&nbsp;&nbsp;&nbsp;insert into class_teacher values (null,'初三五班',10);waiting .....insert into class_teacher values (null,'初三五班',40);commit;事务 A commit 之后，这条语句才插入成功commit;&nbsp;commit;&nbsp; update 的 teacher_id=20 是在 (5，30] 区间，即使没有修改任何数据，Innodb 也会在这个区间加 gap 锁，而其它区间不会影响，事务 C 正常插入。 如果使用的是没有索引的字段，比如 update class_teacher set teacher_id=7 where class_name=‘初三八班（即使没有匹配到任何数据）’, 那么会给全表加入 gap 锁。同时，它不能像上文中行锁一样经过 MySQL Server 过滤自动解除不满足条件的锁，因为没有索引，则这些字段也就没有排序，也就没有区间。除非该事务提交，否则其它事务无法插入任何数据。 行锁防止别的事务修改或删除，GAP 锁防止别的事务新增，行锁和 GAP 锁结合形成的的 Next-Key 锁共同解决了 RR 级别在写数据时的幻读问题。 Serializable这个级别很简单，读加共享锁，写加排他锁，读写互斥。使用的悲观锁的理论，实现简单，数据更加安全，但是并发能力非常差。如果你的业务并发的特别少或者没有并发，同时又要求数据及时可靠的话，可以使用这种模式。 这里要吐槽一句，不要看到 select 就说不会加锁了，在 Serializable 这个级别，还是会加锁的！ 参考资料 MySQL 参考手册 《高性能 MySQL》第三版","categories":[{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/categories/sql/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/tags/sql/"}]},{"title":"赛码-Java面试题笔记","slug":"赛码-Java面试题笔记","date":"2020-09-18T14:52:12.219Z","updated":"2020-09-21T04:30:56.959Z","comments":true,"path":"2020/09/18/赛码-Java面试题笔记/","link":"","permalink":"https://topone233.github.io/2020/09/18/%E8%B5%9B%E7%A0%81-Java%E9%9D%A2%E8%AF%95%E9%A2%98%E7%AC%94%E8%AE%B0/","excerpt":"","text":"1.有关接口说法正确的是 A. 接口中的数据成员为final static B. 接口中的数据成员为public abstract C. 接口同样存在构造方法 D. 实现接口的类必须实现该接口的所有抽象方法 AD C 接口是无法被实例化的，没有构造方法 D 如果是抽象类的话，就不需要。 2.下列程序的输出结果是 public class Test { public static void main(String[] args) { String [][]s={{\"helloworld\",\"hello world\"},{\"this is\",\"a java program\"}}; System.out.println((new StringTokenizer(s[1][1])).countTokens()&gt;2); } } A. 3&gt;2 B. 2&gt;2 C. false D. true D 3.使用折半查找算法对含有20个元素的有序表查找的平均查找长度 A. 2.3 B. 4.3 C. 5.1 D. 3 B 折半查找时间复杂度是 log2(n)，所以 log2(20)=4.3 4.对含有31个元素的序列采用直接选择排序算法排序，在最坏情况下需要进行多少次移动才能完成排序 A. 31 B. 30 C. 60 D. 90 D （n-1）次交换，3(n-1)次移动。一次交换，需要三次移动，因为交换需要借助辅助变量。 5.使用二分法在序列1,4,6,7,15,33,39,50,64,78,75,81,89,96中查找元素81时,需要经过（ ）次比较 A. 4 B. 3 C. 2 D. 12 B 编号：0-13，第一次查找编号6的，即39 39 -&gt; 75 -&gt; 89 6.已知存在8阶对称矩阵，采用压缩存储方式按行序为主序存储，每个元素占一个地址空间。若a22为元素的存储地址为1，每个元素占一个地址空间，则a74的地址为 A. 11 B. 23 C. 32 D. 33 B (1,1) (2,1) (2,2) 1 (3,1) (3,2) (3,3) 3(4,1) (4,2) (4,3) (4,4) 4(5,1) (5,2) (5,3) (5,4) (5,5) 5(6,1) (6,2) (6,3) (6,4) (6,5) (6,6) 6(7,1) (7,2) (7,3) (7,4) 4 1+3+4+5+6+4=23 7.已知主串S=“ababcabcacbab”，模式T=“abcac”。利用KMP算法进行匹配时，需要进行几次才可以匹配成功 A. 3 B. 4 C. 5 D. 6 A 关于 KMP算法 参考：http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html 首先计算部分匹配值：abcac中：a的前缀为0，后缀为0，前缀和后缀重合的部分为0ab的前缀为a，后缀为b，前缀和后缀重合的部分为0abc的前缀为[a,ab]，后缀为[bc,c],前缀和后缀重合的部分为0abca的前缀为[a,ab,abc]，后缀为[bca,ca,a],前缀和后缀重合的部分为1abcac的前缀为[a,ab,abc,abca]，后缀为[bcac,cac,ac,c],前缀和后缀重合的部分为0所以部分匹配值也就是 abcac 对应的next数组为：0 0 0 1 0 即：a b c a c 0 0 0 1 0移动位数 = 已匹配的字符数 - 对应的部分匹配值第一次 :a b a b c a b c a c b a ba b c a c匹配个数为2，最后一个匹配的字符为 ‘b’，其next值是0 ， 移动位数=2-0=2第二次： a b a b c a b c a c b a b a b c a c匹配个数为4，最后一个匹配的字符为 ‘a’，其next值是1， 移动位数=4-1=3第三次： a b a b c a b c a c b a b a b c a c匹配成功共需要三次 8.广度优先遍历二叉树的操作可以用哪种数据结构模拟 A. 栈 B. 单链表 C. 队列 D. 数组 C 深度用栈，广度用队列 9.下列关于二叉排序树说法正确的是 A. 二叉排序树的查找性能取决于二叉树的形状 B. 二叉排序树的查找性能取决于序列的大小 C. 二叉排序树复杂度介于 O(log2n) 和 O(n) 之间 D. 对二叉排序树进行层序遍历可得到有序序列 AC 10.单C PU系统中通常采用两级处理器调度，以下相关描述正确的是 A. 作业调度是从慢速存储设备中的后备队列中挑选作业加载到主存中。 B. 作业调度是从慢速存储设备中的就绪队列中挑选作业加载到主存中。 C. 进程调度是从主存中中的后备队列中挑选进程占用处理器运行。 D. 进程调度是从主存中中的就绪队列中挑选进程占用处理器运行。 AD 11.一级封锁协议可以 A. 能够避免不可重复读取问题 B. 能够避免不读“脏”数据 C. 不能避免不可重复读取和不读“脏”数据的问题 D. 可避免更新丢失的问题 CD 12.当需要控制一个类的实例只能有一个，而且客户端只能从一个全局访问点访问它，应该选择何种设计模式: A. 观察者模式 B. 单例模式 C. 迭代器模式 D. 享元模式 B 13.原型模式的本质是 A. 根据状态来分离和选择行为 B. 封装请求 C. 克隆生成对象 D. 触发联动 C 14.TCP协议的拥塞控制就是防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。常用的方法有 A. 慢启动、窗口滑动 B. 慢开始、拥塞控制 C. 快重传、快恢复 D. 快开始、快恢复 BC 15. import java.util.ArrayList; import java.util.List; public class Main { public static void main(String[] args) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); for(int i=0;i&lt;100;i++){ list.add(&quot;a&quot;); } } }JDK1.8中，执行以上程序后，该list进行了几次扩容？ A. 4 B. 5 C. 6 D. 7 C jdk 1.8，arrayList默认大小为10，每次扩容1.5倍。 int newCapacity = oldCapacity + (oldCapcity &gt;&gt; 1); 16. public class Main { public static void main(String[] args) { System.out.print(fun1()); } public static String fun1() { try { System.out.print(&quot;A&quot;); return fun2(); } finally { System.out.print(&quot;B&quot;); } } public static String fun2() { System.out.print(&quot;C&quot;); return &quot;D&quot;; } }执行以上程序后，输出结果正确的是 A. ABCD B. ACDB C. ACBD D. 不确定 C 如果有finally和return，那就在return前进行finally。 本题代码，先执行fun2()方法，获取返回值后，会存到fun1()的局部变量表中，执行finally语句后，再返回这个值。 17. public class Test { public static void main(String[] args) throws Exception{ ClassLoader classLoader=ClassLoader.getSystemClassLoader(); Class clazz=classLoader.loadClass(&quot;A&quot;); System.out.print(&quot;Test&quot;); clazz.forName(&quot;A&quot;); } } class A{ static { System.out.print(&quot;A&quot;); } } A. TestA B. ATestA C. ATestA D. Test A Class clazz=classLoader.loadClass(“A”); 加载class文件到内存中，并没有对类进行首次主动使用，所以没有初始化。 clazz.forName(“A”); 通过反射获取到A的内存中的数据结构对象，对类进行了首次使用，才会触发初始化。 static 静态代码块只在初始化时候执行一次。 对类的主动使用：创建类实例、访问类或接口的静态变量、静态方法、反射、初始化类的子类、被标明为启动类的类 18.针对类的继承，虚拟机会如何进行父类和子类的初始化加载呢 public class Test { public static void main(String[] args) { System.out.print(B.c); } } class A { public static String c = &quot;C&quot;; static { System.out.print(&quot;A&quot;); } } class B extends A{ static { System.out.print(&quot;B&quot;); } } A. AC B. ABC C. C D. BC A 子类 B引用父类A的静态字段，不会导致子类初始化，只会引发父类初始化。 19. public class Test { public static void main(String[] args) { System.out.print(B.c); } } class A { static { System.out.print(&quot;A&quot;); } } class B extends A{ static { System.out.print(&quot;B&quot;); } public final static String c = &quot;C&quot;; } A. AB B. ABC C. C D. BC C 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用定义常量的类，因此不会触发定义常量的类的初始化。 静态域被访问，而且它不是常量，才触发初始化。 20.JAVA的类加载期负责整个生命周期内的class的初始化和加载工作，就虚拟机的规范来说，以下代码会输出什么结果 public class Test { public static void main(String[] args) { System.out.println(Test2.a); } } class Test2{ public static final String a=&quot;JD&quot;; static { System.out.print(&quot;OK&quot;); } } 输出：JD System.out.println(Test2.a); 用类名直接调用静态属性，没有用对象，就没加载对象中的static方法。 常量在编译阶段会存入调用类的常量池中，本质并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 21. public class Main { private static int x = 10; private static Integer y = 10; public static void updateX(int value) { value = 3 * value; } public static void updateY(Integer value) { value = 3 * value; } public static void main(String[] args) { updateX(x); updateY(y); } }执行以上程序后，x和y的值分别是多少 10，10 Java中只有值传递，没有引用传递 Integer是final定义的。 Integer 是引用类型，但是每次对Integer的赋值操作，都是创建一个新的对象，并且给变量赋上新的地址值。 22. public class Main { public static void main(String[] args) { System.out.println(&quot;A&quot;); new Main(); new Main(); } public Main() { System.out.println(&quot;B&quot;); } { System.out.println(&quot;C&quot;); } static { System.out.println(&quot;D&quot;); } } 输出结果：DACBCB 静态代码块（先父类后子类）-&gt; 非静态代码块 -&gt; 构造函数 23.以下哪种设备工作在数据链路层？ A. 中继器 B. 集线器 C. 交换机 D. 路由器 C 物理层：中继器、集线器 数据链路层：交换机 网络层：路由器 24.一颗二叉树的叶子节点有5个，出度为1的结点有3个，该二叉树的结点总个数是 12 度数为0的节点数量 = 度数为2的节点数量 + 1 本题中：5 = x + 1 可得度数为2节点数量：x=4。 所以这颗二叉树的总节点为：5+3+4=12 25.下面linux命令中，可以用来检查内存使用状况的有 A. sed B. free C. top D. iostat BC free 查看内存使用情况。 free -m 以M为单位显示 top 动态命令，查看进程和内存的动态情况。如果此时输入M，按进程使用内存大小排序。如果输入P，按照进程占用CPU的大小排序。 26.下列关于JVM性能调优工具的描述，错误的是 A. jps主要是用来输出JVM中运行的进程状态信息 B. jmap是用来查看堆栈内存使用状况 C. jstack主要是用来查看某个Java进程内的线程堆栈信息 D. jstat是JVM统计监测工具 C jps：主要用来输出JVM中运行的进程状态信息 jmap：用来查看堆内存使用状况，一般结合jhat使用 jstack：堆栈跟踪工具，主要用来查看某个Java进程内的线程堆栈信息 jstat ：JVM统计监测工具，查看各个内存和GC的情况 hprof：展现CPU使用率，统计堆内存使用情况 27.某系统中有4个并发进程，多需要同类资源5个，试问该系统无论怎样都不会发生死锁的最少资源数是 17 4*(5-1) + 1 28. public class Person{ { System.out.println(&quot;P1&quot;); } static{ System.out.println(&quot;P2&quot;); } public Person(){ System.out.println(&quot;P3&quot;); } } public class Students extends Person{ static{ System.out.println(&quot;S1&quot;); } { System.out.println(&quot;S2&quot;); } public Students(){ System.out.println(&quot;S3&quot;); } public static void main(String[] args){ new Students(); } } P2S1P1P3S2S3 执行顺序：父类静态代码块 -&gt; 子类静态代码块 -&gt; 父类代码块 -&gt; 父类构造方法 -&gt; 子类代码块 -&gt;子类构造方法 首先 当JVM遇到new Students() 时，会去加载Student类，加载Student类之前要去加载父类Person，加载过程中会初始化变量和去初始化static的东西，所以先父类静态代码块后子类静态代码块，类加载完成后，就需要在内存里创建对象了，创建Student对象需要创建父类People，创建People类对象先执行代码块再执行构造器，父类执行完后执行子类的代码块和构造器。 29. public class Test { public static void main(String[] args) { Test t = new Test(); t.method(null); } public void method(Object o) { System.out.println(&quot;Object&quot;); } public void method(String s) { System.out.println(&quot;String&quot;); } } 输出结果：String Object 和 String 值都可以为null。但存在继承关系，因此会将不确定对象null当作子类型处理。 最精准原则String比Object精确，因为一个String本质也是一个Object，但不是每个Object都可以是String的 30. public class Arraytest { int a[] = new int[6]; public static void main (String arg[]) { System.out.println(a[0]); } } 编译时出错 a数组不是静态类型，不能在静态的main方法中直接使用","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"}]},{"title":"IDE常用快捷键","slug":"IDE常用快捷键","date":"2020-09-17T12:32:42.645Z","updated":"2020-09-17T12:34:48.566Z","comments":true,"path":"2020/09/17/IDE常用快捷键/","link":"","permalink":"https://topone233.github.io/2020/09/17/IDE%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/","excerpt":"","text":"eclipse常用快捷键 Ctrl+Shift+T：查找 Java 类文件 Ctrl+Shift+R：查找所有文件（包括 Java 文件） Ctrl+Shift+G：查找类、方法和属性的引用 Ctrl+Shift+F：格式化代码 Alt+/ ：提示 Ctrl+Shift+X：把当前选中的文本全部变味小写 Ctrl+Shift+Y：把当前选中的文本全部变为小写 Ctrl+Shift+O：快速地导入 import Ctrl+O：显示类中方法和属性的大纲，能快速定位类的方法和属性 Ctrl+/: 快速添加注释 Ctrl+Shift+C: 添加注释 Alt+←：前一个编辑的页面 Alt+→：下一个编辑的页面 IDEA常用快捷键 代码格式化：Ctrl + Alt + L 代码智能提示辅助：Alt + Enter 从所有项中查找：双击 Shift 从所有类中查找：Ctrl + N 从所有文件中查找：Ctrl + Shift + N 跳到上一处/下一处代码：Ctrl + Alt + ⬅ / ➡ 从当前文件中查找：Ctrl + F 从当前文件中查找并替换：Ctrl + R 运行当前Java文件：Shift + F10 调试当前Java文件：Shift + F9 单步调试：F8 进入方法调试：F7 跳到下一个短点位置：F9 查看当前项目中的打的断点：Ctrl + Shift + F8","categories":[{"name":"工具","slug":"工具","permalink":"https://topone233.github.io/categories/%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://topone233.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"IDE","slug":"ide","permalink":"https://topone233.github.io/tags/ide/"}]},{"title":"为什么要控制反转？","slug":"为什么要控制反转？","date":"2020-09-17T04:40:14.940Z","updated":"2020-09-17T04:50:10.022Z","comments":true,"path":"2020/09/17/为什么要控制反转？/","link":"","permalink":"https://topone233.github.io/2020/09/17/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%8E%A7%E5%88%B6%E5%8F%8D%E8%BD%AC%EF%BC%9F/","excerpt":"","text":"原文地址 是什么？ IOC 全称是 Inversion of Control控制反转。按照字面意思理解，将控制反转过来，这里的控制指的是什么？为什么要进行反转，ioc 可以解决什么问题？要回答这些问题，我们需要先了解一下 IOC 为什么会产生。 怎么来的？ Java 是一门面向对象的语言，我们的应用程序通过一个个对象之间的相互关联和作用来完成功能，就像手表里的机械结构。每一个齿轮代表一个对象，对象之间彼此紧密咬合形成一个系统，这样的系统对象之间的耦合度非常高，所谓的耦合度就是关系的依赖程度，高耦合度带来的问题显而易见，只要有一个齿轮发生故障，其它齿轮也无法工作，进而整个系统都无法正常工作，这种牵一发而动全身情况如何才能改善呢？ 再来一个 Service 层实际的例子： public class UserServiceImpl { private UserDao userDao = new UserDaoImpl(); private UserDao userDao = (UserDao)BeanFactory.getBean(&quot;userDao&quot;); public List&lt;User&gt; getAllUser(){ return userDao.getAllUser(); } }一个是独立控制通过 new 一个 UserDao 实现类来完成，一个是 Bean 工厂通过全限定类名找到 Bean 对象并创建多例对象，无法自主控制。第二者把控制权交给了 Bean 工厂来创建对象，带来的好处就是降低程序间的依赖关系，也叫削减计算机的耦合。 改善方法？ 上面机械齿轮的例子可以通过一个中间齿轮的方式来解决，也就是后面的中间 IOC 容器。所有的齿轮都交由中间这个齿轮管理，试着把中间这个齿轮拿掉我们可以看到这两个齿轮之间彼此毫无关系，即使一个齿轮出了故障，也不会影响到其它齿轮。中间这个齿轮就好比 ioc 容器，其它齿轮就是对象，可以看出引入了 ioc 容器，对象之间的耦合度降低了。当我们修改一个对象的时候不需要去考虑其它对象，因为它不会对其它对象造成影响。 IoC 原理？ 这里说到的 ioc 容器到底是个什么东东，又是什么让它具有如此神奇的力量？ 先来看一下没有 ioc 容器的时候，对象 A 依赖对象 B，A 在运行到某一时刻的时候会去创建 B 的对象，在这里 A 具有主动权，它控制了对象 B 的创建。 引入 ioc 以后对象 A 和对象 B 之间没有了直接联系，当 A 运行的时候由 ioc 容器创建 B 对象在适当的时候注入到 A 中，在这里，控制权由 A 对象转移到了 ioc 容器。这也就是控制反转名称的由来。 基于上述 UserDao 的例子我们可以通过反射来解耦，反射可以根据类的全限定名在程序运行时创建对象，可以这样做，将类的全限定名配置在 xml 文件中，在程序运行时通过反射读取该类的全限定名，动态的创建对象，赋值给 userDao 接口 userDaoImpl。这样做后 UserServiceImpl 和 UserDaoImpl 之间没有了直接的关系，当我们需要替换 UserDaoImpl 对象的时候只需要在配置文件中去修改类的全限定名就可以了，非常的灵活方便，ioc 容器的实现就是这个原理。 IOC 容器可以自动的帮我们完成以上一系列操作，我们需要做的就是通过配置文件告诉 ioc 需要创建哪个类以及类和类之间的关系。 控制反转和依赖注入 在这里需要提到一个概念依赖注入，很多初学者搞不清楚控制反转和依赖注入之间的关系，其实他们是对同一事物的不同角度的描述。控制反转是一种设计思想而依赖注入是这种思想的具体实现 具体说控制反转就是将创建 userDaoImpl 对象的控制权反转过来由 UserServiceImpl 交给了 ioc 容器，强调的是一种能力和思想，ioc 容器具有了控制权。 依赖注入就是 ioc 容器将 UserServiceImpl 所依赖的对象 userDaoImpl，注入给 UserServiceImpl，强调的是一个过程和实现。 IOC 很好的体现了面向对象设计法则之一—— 好莱坞法则：“别找我们，我们找你”。 优缺点 软件系统中由于引入了第三方 IOC 容器，生成对象的步骤变得有些复杂，本来是两者之间的事情，又凭空多出一道手续，所以，我们在刚开始使用 IOC 框架的时候，会感觉系统变得不太直观。所以，引入了一个全新的框架，就会增加团队成员学习和认识的培训成本，并且在以后的运行维护中，还得让新加入者具备同样的知识体系。 由于 IOC 容器生成对象是通过反射方式，在运行效率上有一定的损耗。如果你要追求运行效率的话，就必须对此进行权衡。 具体到 IOC 框架产品 (比如：Spring) 来讲，需要进行大量的配制工作，比较繁琐，对于一些小的项目而言，客观上也可能加大一些工作成本。 IOC 框架产品本身的成熟度需要进行评估，如果引入一个不成熟的 IOC 框架产品，那么会影响到整个项目，所以这也是一个隐性的风险。 我们大体可以得出这样的结论：一些工作量不大的项目或者产品，不太适合使用 IOC 框架产品。另外，如果团队成员的知识能力欠缺，对于 IOC 框架产品缺乏深入的理解，也不要贸然引入。最后，特别强调运行效率的项目或者产品，也不太适合引入 IOC 框架产品，像 WEB2.0 网站就是这种情况。 Spring 框架文档：https://yoyling.com/spring5 本文由 YOYLING. 发表， 最后编辑时间为：2020-09-08 13:02如果你觉得我的文章不错，不妨鼓励我继续写作。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"CORS 跨域资源共享安全漏洞","slug":"CORS 跨域资源共享安全漏洞","date":"2020-09-15T13:08:42.316Z","updated":"2020-09-19T06:48:17.411Z","comments":true,"path":"2020/09/15/CORS 跨域资源共享安全漏洞/","link":"","permalink":"https://topone233.github.io/2020/09/15/CORS%20%E8%B7%A8%E5%9F%9F%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E/","excerpt":"","text":"原文地址 [www.cnblogs.com\\](https://www.cnblogs.com/sevck/p/9035422.html) CORS 漏洞其中已经存在很久了，但是国内了解的人不是很多，文章更是少只有少，漏洞平台也没有此分类。 定义CORS，Cross-Origin Resource Sharing，跨源资源共享。CORS 是 W3C 出的一个标准，其思想是使用自定义的 HTTP 头部让浏览器与服务器进行沟通。因为开发者需要进行跨域进行获取资源，应用场景，在 a.com，想获取 b.com 中的数据，常用的 2 种方法进行跨域一种为 JSONP, 一种为 CORS. 前者再次不在描述，常见的为 JSONP 劫持。 CORS 请求示例： 代码如下： &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt;CORS Test&lt;/head&gt; &lt;body&gt; &lt;div&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; //XmlHttpRequest对象 function createXmlHttpRequest(){ if(window.ActiveXObject){ //如果是IE浏览器 return new ActiveXObject(&quot;Microsoft.XMLHTTP&quot;); }else if(window.XMLHttpRequest){ //非IE浏览器 return new XMLHttpRequest(); } } function getFile() { var img\\_Container = document.getElementById(&quot;img\\_Div&quot;); var xhr = createXmlHttpRequest(); xhr.open(&#39;GET&#39;, &#39;http://oss.youkouyang.com/1.jpg&#39;, true); xhr.setRequestHeader(&#39;Content-Type&#39;, &#39;image/jpeg&#39;); xhr.responseType = &quot;blob&quot;; xhr.onload = function() { if (this.status == 200) { var blob = this.response; var img = document.createElement(&quot;img&quot;); img.onload = function(e) { window.URL.revokeObjectURL(img.src); }; img.src = window.URL.createObjectURL(blob); img\\_Container.appendChild(img); } } xhr.send(null); } &lt;/script&gt; &lt;div&gt; &lt;input type=&quot;button&quot; onclick=&quot;getFile()&quot; value=&quot;Get&quot; /&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;CORS 安全机制在 CORS 中，相关的标准和浏览器厂商也推出了很多防范的相关标准。例如 SOP 等等。 简单请求 在简单请求中，如果请求符合 2 个标准，则会进行异步请求：1,GET、POST、HEAD..2,Content-Type 验证, pplication/x-www-form-urlencoded;multipart/form-data;text/plain 在简单请求中，浏览器进行跨域请求，会在请求中携带 Origin，表面这是一个跨域。服务端会在接收中，通过自己的跨域规则进行验证。通过 access-Control-Allow-Origin 和 Access-Control-Allow-Methods 如果验证成功，则会返回资源内容，如果验证失败，则返回 403 状态。 预先请求在请求中，满足下面任意一个请求中，浏览器会首先发出 OPTION 请求，再去验证是否符合。 在 OWASP TOP 10 中也有更详细的说明：http://blog.securelayer7.net/owasp-top-10-security-misconfiguration-5-cors-vulnerability-patch/在 DefConChina 中，陈建军分享的议题中解释的更清楚，有意向的可以找 PPT 或者视频 实战请求 COS 之前发过一次 CORS 的蠕虫：http://evilcos.me/?p=590 防御：在防御过程中，存在很多配置不当，或者配置错误的情况，例如使用 *, 或者使用多个域名 list, 或者使用 null, 或者 nginx 配置规则存在错误等等。 相关工具：CORScanner // github 参考： https://blog.csdn.net/saytime/article/details/51549888","categories":[{"name":"安全","slug":"安全","permalink":"https://topone233.github.io/categories/%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"安全","slug":"安全","permalink":"https://topone233.github.io/tags/%E5%AE%89%E5%85%A8/"},{"name":"CROS","slug":"cros","permalink":"https://topone233.github.io/tags/cros/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"SSL协议","slug":"SSL协议","date":"2020-09-11T08:05:13.045Z","updated":"2020-09-17T02:19:30.706Z","comments":true,"path":"2020/09/11/SSL协议/","link":"","permalink":"https://topone233.github.io/2020/09/11/SSL%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"1.基本概念安全套接层（secure sockets layer，SSL）协议是Netscape公司1994年提出的用于Web应用的传输层安全协议。 SSL协议使用非对称加密体制和数字证书技术，保护信息传输的秘密性和完整性。 2.特点2.1 主要应用于HTTP协议SSL协议尽管可以用于HTTP、FTP、TELNET等协议，但是目前主要应用于HTTP协议，为基于Web服务的各种网络应用中客户和服务器之间的用户身份认证与安全数据传输提供服务。 2.2 加密的安全通道SSL协议处于应用层和传输层之间，在TCP协议之上建立一个加密的安全通道，为TCP协议之间传输的数据提供安全保障。 2.3 加密与解密当HTTP协议使用SSL协议时，HTTP的请求、应答报文格式、处理方法不变。不同之处是：应用进程所产生的报文将通过SSL协议加密之后，再通过TCP连接传送出去。接收端TCP协议将加密的报文传送给SSL协议解密之后，再传送到应用层HTTP协议 2.4 不同的使用形式当Web系统采用SSL协议时，Web服务器的默认端口号从80 变换成 443；Web客户端使用HTTPS 取代HTTP。 2.5 握手协议、记录协议SSL协议包含两个协议：SSL握手协议（SSL Handshake Protocol）与 SSL记录协议（SSL Record Protocol）。SSL握手协议实现双方加密算法的协商与密钥传递，SSL记录协议定义SSL数据传输格式，实现对数据的加密与解密操作。","categories":[{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"安全","slug":"安全","permalink":"https://topone233.github.io/tags/%E5%AE%89%E5%85%A8/"},{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"SSL","slug":"ssl","permalink":"https://topone233.github.io/tags/ssl/"}]},{"title":"TCP协议","slug":"TCP协议","date":"2020-09-10T14:31:50.471Z","updated":"2020-09-17T13:33:16.335Z","comments":true,"path":"2020/09/10/TCP协议/","link":"","permalink":"https://topone233.github.io/2020/09/10/TCP%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"1.特点1.1 支持面向连接的传输服务如果将UDP协议提供的服务比作一封邮件的话，那么TCP协议所能提供的服务相当于语音聊天，必须要通信双方建立连接。 面向连接对提高系统数据传输的可靠性是很重要的，使用TCP传输数据之前，必须在源进程端口与目的进程端口之间建立一条TCP传输连接。用双方端口号来标识。TCP建立在不可靠的网络层 IP 协议上，IP 协议不能提供任何可靠性保障机制，因此TCP 协议的可靠性需要自己解决。 1.2 支持字节流的传输流（stream）相当于是一个管道（水管），从一端放入什么，另一端可以照原样取出。描述了一个不出现 ：丢失、重复、乱序的数据传输过程。TCP 协议将数据看成是一连串的、无结构的字节流。 如果用户是通过键盘输入数据，应用进程将字符逐个提交给发送端。如果数据是从文件得到，那么数据可能是逐行或逐块交付发送端。 为了支持字节流传输，发送端和接收端都需要使用缓存。发送端将几个写操作组合成一个报文的，提交给 IP 协议，由 IP 协议封装成 IP 分组之后传输到接收端， 1.3 支持全双工通信由于通信双方都设置有发送和接收的缓冲区，TCP 协议允许通信双方在任何时候都可以发送数据。 1.4 支持同时建立多个并发的TCP连接TCP 协议需要支持同时建立多个连接，这个特点在服务端表现的更为突出。一个服务器必须同时处理多个客户端的访问。例如，一个Web服务器的套接字为“ 141.8.22.51:80 ”，同时有三个客户端需要访问这个服务器，它们的套接字分别为“ 202.1.12.5:30001 “ “ 242.1.12.5:300022 “ “ 212.1.12.5:300023 “则服务器端需要同时建立三个 TCP 连接。 也支持一个客户端与多个服务器同时建立多个 TCP 连接。 1.5 支持可靠的传输服务TCP 协议使用确认机制检查数据是否安全和完整，并且提供拥塞控制功能。对发送和接收的数据进行跟踪、确认和重传。 但是 TCP 协议是建立在不可靠的网络层 IP 协议之上，一旦 IP 协议及以下层出现传输错误，TCP 协议只能不断进行重传，可靠性会受到底层限制。 2.报文格式窗口 窗口字段长度为16位，表示以字节（B）为单位的窗口大小。 由于接收端的接收缓冲区是受到限制的，因此需要设置一个窗口字段，表示下一次传输接收端还有多大的接收容量。窗口字段值是准备接收下一个 TCP报文段的接收端，通知即将发送报文段的发送端，下一次最多可以发送报文段的字节数。 发送端将根据接收端通知的窗口值调整自己的发送窗口值大小。 窗口字段值是动态变化的。 主机A 发给主机 B的TCP报头中确认号是 502，窗口值 1000。表示：下一次主机B要向主机A发送的 TCP，字段第一字节号应该是 502，字段最大长度是 1000，最后一个字节号最大是 1501。 序号 序号字段长度位32位 TCP 是面向字节流的，它需要为发送字节流中的每个字节都按顺序编号。 TCP连接建立时，每方都需要使用随机数产生器，生成一个初始序号。 不能为0。避免因TCP连接非正常断开而可能引起的混乱。如果在连接突然中断时，可能有一个或两个进程同时等待对方的确认应答，而这个时候有一个新连接的序号也是从0开始，那么接收进程就有可能认为是对方重传的报文，这样就可能造成连接过程的错误。 ACK（确认位） 在TCP连接建立后发送的所有报文段的ACK位都要置 1。 SYN（同步位） 在连接建立时用来同步序号。例如 SYN=1，ACK=0时，表示这是一个连接建立请求报文，同意建立连接的响应报文：SYN=1，ACK=1。 3. 连接建立与释放3.1 三次握手3.1.1 最初，客户端TCP 进程是处于 CLOSE 状态。准备发起TCP连接时，进入 SYN-SEND 状态，向处于LISTEN 状态的服务器端 TCP 进程发送第一个控制位 SYN = 1 的”连接建立请求报文“，不携带数据字段，但需要给报文一个序号 seq=x。 SYN=1 seq=x 3.1.2 服务器端接收到” 连接建立请求报文“之后，如果同意建立连接，则向客户端发送第二个控制位 SYN=1， ACK=1 的 ” 连接建立请求确认报文 “。确认号 ack = x + 1，表示是对第一个” 连接建立请求报文“（序号 seq=x）的确认。同样不携带任何数据字段，但是需要给报文一个序号 seq=y。这时服务器进入 SYN-RCVD（准备接收）状态。 SYN=1 ACK=1 seq=y ack=x+1 3.1.3 客户端发送第三个控制位 ACK=1 ” 连接建立请求确认报文 “。由于该报文是对” 连接建立请求确认报文”（seq=y）的确认，因此确认序号 ack=y+1。同样不携带数据字段，但需要给一个序号，仍为 x+1 。ACK=1 seq=x+1 ack =y+1。这时客户端进入 ESTABLISHED（已建立连接）状态。服务器端在接收到ACK报文之后也进入 ESTABLISHED （已建立连接）状态。 三次握手完成。TCP连接建立。 3.1.4 为什么需要第三次握手？主要是防止已经失效的连接请求报文段突然又传回到服务端而产生错误： 客户端发出的第一个连接请求报文段， 但是由于某些原因在某个网络节点滞留了很长时间，客户端一直等不到确认报文，于是客户端再次发出一次新的连接请求，并成功收到服务端确认，建立了连接。 之前的请求报文段并没有丢失，延误到客户端与服务端连接释放后才到达服务端，本来这个请求已经失效了，但是服务端收到此请求报文段后，误以为是客户端发出的新的请求连接，于是服务端又向客户端发出确认报文段。 假如不采用三次握手，那么只要服务端发出确认，连接就建立了。 但是客户端并没有发出连接建立的请求，因此不会理会服务端的确认，也不会向服务端发出数据。 而服务端以为却一直在等待客户端发来数据，这样服务端的许多资源就白白浪费了 常用三次握手可以防止上诉现象发生。客户端不向服务端发出确认请求，服务端就不会建立连接。 3.2 报文传输TCP传输连接建立之后，双方可以使用这个连接，进行全双工的字节流传输。 为了保证TCP工作正常、有序的进行，TCP在服务器端设置了保持计时器（keep timer），用来防止TCP连接处于长时间空闲。当服务器端收到客户端的报文时，就保持计时器复位。如果没有收到客户端的信息，他就发送探测报文。如果发送10个探测报文（每个相隔 75s）还没有响应，就假设客户端出现故障，终止该连接。 3.3 四次挥手3.3.1 客户端主动提出释放TCP连接，进入 FIN-WAIT-1（释放等待-1）状态。向服务器发送第一个控制位FIN=1，的 ” 连接释放请求报文 ”，提出连接释放请求，停止发送数据。不携带任何数据字段。但是需要给报文一个序号。 FIN=1，seq=u 。u等于客户端发送的最后一个字节的序号加1。 3.3.2 服务器端收到之后，需要向客户发送 “ 连接释放请求确认报文 ”，表示对报文的确认， ack=u+1。这个 “ 连接释放请求报文 ” 的序号 v 等于服务器发送的最后一个字节序号加1。 ACK=1，seq=v，ack=u+1。 TCP服务器进程向高层应用进程通知客户请求释放TCP连接，客户到服务器的TCP连接断开，但是服务器到客户的TCP连接还没有断开，如果服务器还有数据报文需要发送时，它还可以继续发送直至完毕。这种状态称为半关闭（helf-close）状态。这个状态需要持续一段时间。 客户在接收到服务器发送的ACK报文之后进入 FIN-WAIT-2状态；服务器进入CLOSE-WAIT状态。 3.3.3 服务器的高层应用程序已经没有数据需要发送时，它会通知TCP可以释放连接，这时服务器向客户发送 “ 连接释放请求报文 ”。报文的序号（假定为w），取决于在半关闭状态时，服务器端是否发送过数据报文。服务器端经过 LAST-ACK状态之后转回到LISTEN（收听）状态。 ACK=1，FIN=1，seq=w，ack=u+1。 3.3.4 客户接收到FIN报文之后，向服务器发送 “ 连接释放请求确认报文 ”，ACK=1，seq=u+1，ack=w+1 3.3.5 为什么TCP的连接的建立只需要三次握手，而连接的释放需要四次呢？因为服务端在LISTEN状态下，收到建立请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。 而连接关闭时，当收到对方的FIN报文时，仅仅表示客户端没有需要发送的数据了，但是还能接收数据。服务端上层的应用程序未必数据已经全部发送给对方了，此时服务端可以立即关闭，也可以将应该发送的数据全部发送完毕后，再发送FIN报文给客户端来表示同意现在关闭连接。 从这个角度而言，服务端的ACK和FIN一般都会分开发送。从而导致多了一次。 3.4 时间等待计时器为了保证TCP连接释放过程正常的进行，TCP设置了时间等待计时器（TIME-WAIT Timer）。当TCP关闭一个连接时，它并不认为这个连接马上就真正的关闭。这时，客户端进入TIME-WAIT状态，需要再等待两个最长报文寿命（maximum segment lifetime，MSL）时间之后，才真正进入CLOSE（关闭）状态。 四次挥手之后，确认双方已经同意释放连接，客户端仍需要采取延迟2MSL时间，确保服务器在最后阶段发送给客户端的数据，以及客户端发送给服务器的最后一个ACK报文都能正确的被接收，防止因个别报文传输错误导致连接释放失败。 4.滑动窗口与确认、重传机制4.1 滑动窗口TCP协议使用以字节为单位的滑动窗口协议（Sliding-Windows Protocol），来控制字节流的发送、接收、确认、重传过程。 TCP使用两个缓存和一个窗口来控制字节流的传输过程。 发送端的TCP有一个缓存，用来存储应用进程准备发送的数据。对这个缓存设置一个发送窗口。 接收端也有一个缓存，将正确接收到字节流写入缓存，等待应用进程读取。也有一个接收窗口。 只要发送窗口值不为0就可以发送报文段。发送窗口的大小取决于接收窗口的大小。发送端每一次能够连续发送字节数取决于发送窗口的大小。不能超过接收窗口值，发送端可以根据自身的需要来决定。 接收窗口值等于接收缓存还可以继续接收的字节流的大小。由接收端根据接收缓存剩余空间的大小，以及应用进程读取数据的速度决定。 接收端通过TCP报头通知发送端，已经正确接收的字节号，以及发送端还能够连续发送的字节数。 虽然TCP协议是面向字节流的，但是它不可能每次传送一个字节，就对这个字节进行确认。它是将字节流分成段，一个段的多个字节打包成一个TCP报文段一起传送、一起确认。TCP协议通过报头的“序号”来标识发送的字节，用“确认号”表示哪些字节已经被正确的接收。 4.1.2 字节流传输状态为了达到利用滑动窗口协议控制差错的目的，TCP协议引入了“字节流传输状态”的概念。为了对正确传输的字节流进行确认，必须对字节流的传输状态进行跟踪： 假设发送的第一个字节的序号是1。 第一类：已经发送，且已得到确认的字节。假设序号为19之前的字节已经被接收端正确的接收，并且发送端发送了确认信息。1-19的字节属于第一类。 第二类：已经发送，但未收到确认的字节。 第三类：尚未发送，但是接收端表示接收缓冲区已经准备好，如果发送端准备好就可以立即发送这些字节。 第四类：尚未发送，且接收端也未做好接收准备的字节。 4.1.3 发送窗口与可用窗口发送端每一次能够连续发送字节数取决于发送窗口的大小。 发送窗口的长度等于第二类与第三类字节数之和。 可用窗口长度等于第三类字节数。如果没有任何问题，发送端可以立即发送可用窗口的字节。 接收端确认发送窗口的字节，为保持发送窗口值不变，需要将窗口向左滑动（例如从序号1 移动到 20）。 4.2 容错控制TCP协议通过滑动窗口机制来跟踪和记录发送字节的状态，实现差错控制功能。 TCP协议的设计思想是让应用进程将数据作为一个字节流传送，而不是限制应用层数据的长度。应用进程不需要考虑发送数据的长度，由TCP协议来负责将这些字节分段打包。 发送端利用已建立的TCP连接，将字节流传送到接收端的应用进程，并且是顺序的，没有差错、丢失、重复的。 TCP协议发送的报文是交给 IP 协议传输的，IP协议只能提供尽力而为的服务，IP分组在传输过程中出错是不可避免的，TCP协议必须提供差错控制、确认、重传功能，以保证接收的字节流是正确的。 4.3 选择重传策略上面我们没有考虑到报文段丢失的情况，但是在Internet中，报文段丢失是不可避免的，会造成接收的字节流序号不连续的现象。 接收字节流序号不连续的处理方法有两种：拉回、选择重传 4.3.1 拉回在丢失第二个报文段时，不管之后的报文段接收是否正确，都要求从第二个报文段开始，重传后面的所有的报文段。显然，拉回方式发效率是很低的。 4.3.2 选择重传选择重传（selective ACK，SACK），如果收到字节流序号不连续时，如果这些字节的序号都在接收窗口之内，则首先完成接收窗口内字节的接收，然后将丢失的字节序号通知发送端，发送端只需要重传丢失的报文段，而不需要重传已经接收的报文段。 4.4 重传计时器TCP使用重传计时器（retransmission timer）来控制报文确认与等待重传的时间。当发送端发送一个报文时，首先将它的一个报文的副本放入重传队列，同时启动一个重传计时器。重传计时器设定一个值，例如400ms，然后开始倒计时。时间结束前收到确认，表示传输成功，否则说明传输失败，准备重传该报文。 4.4.1 影响超时重传的元素设定重传计时器的时间值时很重要的。如果设定值过低，可能出现已被接收端正确接收的报文被重传，造成接收报文重复的现象。如果设定值过高，造成一个报文已经丢失，而发送端长时间等待，造成效率降低的现象。 如果一个主机同时与其他两个主机建立两条TCP连接，那么它就需要分别为每个连接启动一个重传计时器。如果一个用于本地局域网中传输文本文件，另一个用于Internet远程访问Web服务，那么两个TCP报文的往返时间相差很大。因此，需要对不同的TCP连接设定不同的重传计时器的时间。 由于Internet在不同时间段的用户数量变化很大，流量与传输延迟变化也很大，报文传输延迟也不会相同。 正是由于这些原因，为TCP连接确定合适的重传定时器数值是很可能的。TCP不会采用简单的静态方法，必须采用动态的自适应的方法。根据端对端报文往返时间的连续测量，不断调整和设定重传定时器的超时重传时间。 4.5超时重传时间的选择略。。。（写不动了，下次再补吧） 5.滑动窗口与流量控制、拥塞控制5.1 TCP窗口与流量控制研究流量控制（flow control）算法的目的是控制发送端发送速率，使之不超过接收端的接收速率，防止由于接收端来不及接收送达的字节流，而出现报文段丢失的现象。滑动窗口协议可以利用TCP报头中窗口字段，方便的实现流量控制。","categories":[{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"}],"tags":[{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"TCP","slug":"tcp","permalink":"https://topone233.github.io/tags/tcp/"},{"name":"滑动窗口","slug":"滑动窗口","permalink":"https://topone233.github.io/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"}]},{"title":"关于 Java 中 length、length()、size() 的区别","slug":"关于 Java 中 length、length()、size() 的区别","date":"2020-09-08T08:54:24.754Z","updated":"2020-09-17T02:18:03.900Z","comments":true,"path":"2020/09/08/关于 Java 中 length、length()、size() 的区别/","link":"","permalink":"https://topone233.github.io/2020/09/08/%E5%85%B3%E4%BA%8E%20Java%20%E4%B8%AD%20length%E3%80%81length()%E3%80%81size()%20%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"以前总是觉得自己好像会了，但是某天忽然面对这个笔试题还是会恍惚一下，混淆和答错的几率也很大，不知道有没有其他人像我一样的。 所以今天把这个问题记一下，希望印象更深刻。 首先区分一下 length 和 length()； length 不是方法，是属性，数组的属性； public static void main(String\\[\\] args) { int\\[\\] intArray = {1,2,3}; System.out.println(&quot;这个数组的长度为：&quot; + intArray.length); }length() 是字符串 String 的一个方法； public static void main(String\\[\\] args) { String str = &quot;HelloWorld&quot;; System.out.println(&quot;这个字符串的长度为：&quot; + str.length()); }进入 length() 方法看一下实现 private final char value\\[\\]; public int length() { return value.length; }注释中的解释是 @return the length of the sequence of characters represented by this object. 即由该对象所代表的字符序列的长度，所以归根结底最后要找的还是 length 这个底层的属性； size() 方法，是 List 集合的一个方法； public static void main(String\\[\\] args) { List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(&quot;a&quot;); list.add(&quot;b&quot;); list.add(&quot;c&quot;); System.out.println(&quot;这个list的长度为：&quot; + list.size()); }在 List 的方法中，是没有 length() 方法的； 也看一段 ArrayList 的源码 private final E\\[\\] a; ArrayList(E\\[\\] array) { if (array==null) throw new NullPointerException(); a = array; } public int size() { return a.length; }由这段就可以看出 list 的底层实现其实就是数组，size() 方法最后要找的其实还是数组的 length 属性； 另外，除了 List，Set 和 Map 也有 size() 方法，所以准确说 size() 方法是针对集合而言。 总结： length——数组的属性； length()——String 的方法； size()——集合的方法； 谨记。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"集合","slug":"集合","permalink":"https://topone233.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"JVM 初步理解","slug":"JVM 初步理解","date":"2020-09-04T02:25:57.056Z","updated":"2020-09-20T05:47:39.177Z","comments":true,"path":"2020/09/04/JVM 初步理解/","link":"","permalink":"https://topone233.github.io/2020/09/04/JVM%20%E5%88%9D%E6%AD%A5%E7%90%86%E8%A7%A3/","excerpt":"","text":"原文地址 [www.zhihu.com\\](https://www.zhihu.com/question/20097631/answer/817071740) 如果 JAVA 开发是降龙十八掌，JVM 就是九阳神功，具备九阳神功的内力基础降龙十八掌的威力会被发挥的淋漓尽致。 JVM 的内存结构和类加载机制是玩转 JVM 的入口，如果弄通了接下来的路该怎么走你自然就知道了 下面就从这两个方面来深入解析一下 JVM 1.JVM 内存结构jvm 内存分为五大块： 标灰的是线程公有的内存区域，没有标灰的是线程私有。 1.1 程序计数器程序计数器是用来指示当前线程要执行哪条指令，并且在执行完该条指令后让程序计数器指向下一条指令，直到将程序执行完毕。指令需要靠 cpu 来执行，在多线程中，多个线程是通过轮流切换分配 cpu 的时间片而执行的，在切换时需要记录当前执行到了哪条指令以便将来继续执行，每一个线程都需要有自己的程序计数器，所以程序计数器是线程私有的内存。 1.2 虚拟机栈通常我们把 jvm 的内存粗略的分为堆和栈，其中的栈指的就是虚拟机栈, 虚拟机栈也是线程私有的。 虚拟机栈对应的是方法的内存区域，每个方法执行时都会创建一个栈帧，用来存储该方法的局部变量表，操作数栈，动态链接，方法返回地址： 1.2.1 局部变量表局部变量表中存储的是方法的参数和方法中定义的局部变量，在编译期间就为局部变量表分配好了内存空间。局部变量表中存储三种类型的数据： （1） 基本数据类型 （2） 引用类型：指向一个对象在内存中的地址 （3） returnAddress 类型：指向指令的地址（已经很少见了，指向异常处理的指令，现在已经由异常表代替） 1.2.2 操作数栈当虚拟机执行一些指令的时候会对操作数栈进行入栈或出栈的操作，比如 iadd 指令将两个数相加，会先将操作数栈中的两个数弹出来（出栈），相加后再压入栈（入栈）中。 1.2.3 动态链接在运行时常量池中存储了诸如类名，方法名，我们要找到目标类，执行相应的方法就需要用到动态链接，栈帧中有一个指向运行时常量池的引用，通过这个引用可以找到相应的类名和方法名，但是光知道名称是没法执行方法的，需要通过名称找到相应的类和方法在内存中的地址，这个过程就是动态链接。 1.2.4 方法返回地址当方法执行完以后如果有返回值，就会把这个返回值返回给该方法的调用者，方法的返回就是我们 java 中用到的 return 命令。方法返回之后调用者需要继续往下执行就需要知道要执行的地址，该地址就是方法返回地址，它被记录在了栈帧中，当然在发生异常的情况下不会有返回值，要继续执行的地址可以通过异常处理器表来确定。 虚拟机栈可能出现两种类型的异常： 1. 线程请求的栈深度大于虚拟机允许的栈深度会抛出 StackOverflowError,（虚拟机栈空间不能动态扩展的情况下） 2. 如果虚拟机栈空间可以动态扩展（目前多数的虚拟机都可以），当动态扩展无法申请到足够的空间时会抛出 OutOfMemory 异常。 1.3 本地方法栈本地方法栈与虚拟机栈的作用是一样的，区别在于虚拟机栈为虚拟机执行 java 方法服务，而本地方法栈为虚拟机执行 native 方法服务，native 方法为本地方法，不是用 java 语言写的有可能是 c 或者 c++ 写的，在 jdk 中就有很多 c 的代码，就是提供给本地方法来调用的。 1.4 堆通常我们把 jvm 的内存粗略的分为堆和栈，其中的堆就是指它，它是虚拟机中占用内存最大的一块，是被所有线程共享的一块区域，它是用来存放对象实例的。是垃圾收集器管理的主要区域。 1.5 方法区方法区也是被所有线程共享的一块区域，它存储的是类信息，常量，静态变量，编译后的字节码等信息。方法区中还有一块区域 “运行时常量池 “：运行时常量池中存储的是编译期生成的各种字面量和符号引用。字面量相当于 Java 里常量的概念，比如字符串，声明为 final 的常量值等，符号引用包括了：类和接口名，字段名，方法名。 2.类加载机制2.1 java 类的加载过程编译后的 Java 类是以字节码的形式存在的，它只有被加载到虚拟机内存中才能被使用，它是如何被加载到内存中的呢？ 下图为类加载到内存的机制： 2.1.1 加载在加载（注意和类加载是不同的概念）阶段虚拟机需要完成三件事 （1）. 通过一个类的全限定名（类名全称，带包路径的用点隔开，例如: java.lang.String）来获取其定义的二进制字节流（被编译以后的字节码文件就是二进制的）。 （2）. 将这个字节流所代表的静态存储结构（字节码文件就是其中一种）转化为方法区的运行时数据结构（能够在虚拟机中存储的结构）。 （3）. 在 Java 堆中生成一个代表这个类的 java.lang.Class 对象（用于表示这个类的信息），作为对方法区中这些数据的访问入口。 2.1.2 验证验证，准备，解析统称为连接，作为连接阶段的第一步，验证的主要作用是保证加载进来的二进制流中的信息是符合当前虚拟机要求的，并且不会对虚拟机的安全造成危害。主要包括： （1）文件格式验证：主要是验证二进制流是否符合 class 文件格式的规范，并且能被当前的虚拟机处理，例如：主次版本号是否在当前虚拟机处理范围之内，常量池的常量中是否有不被支持的常量类型。只有通过了这个阶段的验证后字节流才会进入内存的方法区中进行存储，从这里我们可以看出加载和验证阶段是交叉进行的，加载还未完成，文件格式验证就已经开始了。 （2）元数据验证：对字节码描述的信息进行语义分析以保证其描述的信息符合 java 语言规范的要求：例如：这个类是否有父类（所有类除了 Object 都应该有父类） （3）字节码验证：确定程序语义是合法的符合逻辑的，如将子类对象赋给父类类型是符合逻辑的，反之，将父类对象赋给子类类型则是不合法的。 （4）符号引用验证：可以对常量池中各种符号引用的信息进行匹配性校验。例如：符号引用中通过字符串描述的全限定名是否能找到对应的类。 2.1.3 准备准备阶段会为静态变量分配内存并设置初始值，注意：该初始值为数据类型的零值例如： Public static int num = 3; 在准备阶段会将 num 值设置为 0 而不是 3. 只有在初始化阶段才会赋值为 3. 2.1.4 解析解析阶段是把类中的符号引用转换为直接引用的过程： 符号引用：在编译的时候是不知道类所引用的类的具体地址，因此只能使用符号引用来代替，比如：com.Student 类引用了 com.Grade 类，编译时 Student 类并不知道 Grade 类在内存中的实际地址，只能用符号 com.Grade。 直接引用; 引用的实际内存地址。 2.1.5 初始化初始化阶段是类加载过程的最后一步，在这个阶段会根据程序中的赋值语句给变量赋值，当有继承关系时先初始化父类，再初始化子类。如果该类还没有被加载和连接，那么初始化之前先加加载和连接。 什么时候会进行初始化呢？ 1. 使用 new 关键字实例化对象的时候 2. 读取或设置一个类的静态字段的时候 3. 调用一个类的静态方法的时候 4. 对类进行反射调用的时候 5. 当虚拟机启动时执行一个类的 main 方法，会先初始化这个类 2.2 类加载器加载阶段中的第一步：“通过一个类的全限定名来获取其定义的二进制字节流” 是通过类加载器来完成的，类加载器分为三种： 2.2.1 启动类加载器(BootStrap ClassLoader)这个类加载器负责将 jdk\\jre\\lib 下的类库加载到内存中，启动类加载器无法被应用程序直接使用。 2.2.2 扩展类加载器(Extension ClassLoader)它负责加载 jdk\\jre\\lib\\ext 中的类库。开发者可以直接使用扩展类加载器。 2.2.3 应用程序类加载器 (Application ClassLoader)它用来加载 classpath 路径（src 路径下的文件在编译后会放到 WEB-INF/classes 路径下。默认的 classpath 是在这里）指定的类。开发者可以直接使用这个类加载器，如果如果应用程序中没有定义自己的类加载器，一般情况下这个就是程序中默认的类加载器。 我们的应用程序都是由这三种类加载器互相配合进行加载的，如果有必要还可以加入自己定义的类加载器。 2.3 双亲委派模型下图为双亲委派模型的类加载器的层次关系： 双亲委派模型的的工作过程是：如果一个类加载器收到了类加载的请求，它会把这个请求委派给父类加载器去完成，每一个层次的加载器都是如此，因此所有的加载请求最终都会传送到顶层的启动类加载器中。只有当父加载器反馈自己无法完成这个加载请求时（在自己的加载范围内没有搜索到该类）, 子加载器才会尝试自己去加载。例如： 1. 当应用程序类加载器加载一个类时，它会把类加载请求委派给扩展类加载器。 2. 扩展类加载器又把这个类加载请求委派给启动类加载器。 3. 启动类加载器如果加载失败，在 (jdk/jre/lib) 里没有找到这个类，会使用扩展类加载器进行加载。 4. 扩展类加载器如果加载失败，在（jdk/jre/lib/ext）里没有找到这个类，会使用应用程序类加载器来加载 5. 应用程序加载器加载失败则会报：ClassNotFoundException 异常。 使用双亲委派模型的意义： 例如类 java.lang.Object 存放在 rt.jar 中，无论哪个类加载器要加载这个类，最终都会委派启动类加载器来加载，如果没有双亲委派模型用户自己编写了一个 java.lang.Object 类，放到 ClassPath 中，系统中将会出现多个不同的 Object 类，应用程序也会变得混乱。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"JVM","slug":"jvm","permalink":"https://topone233.github.io/tags/jvm/"}]},{"title":"MYSQL","slug":"MYSQL","date":"2020-09-04T02:21:49.744Z","updated":"2020-09-20T05:48:24.287Z","comments":true,"path":"2020/09/04/MYSQL/","link":"","permalink":"https://topone233.github.io/2020/09/04/MYSQL/","excerpt":"","text":"原文地址 [www.cnblogs.com\\](https://www.cnblogs.com/Young111/p/9598728.html) 数据库 数据库概述1. 什么是数据库 数据库就是存储数据的仓库，其本质是一个文件系统，数据按照特定的格式将数据存储起来，用户可以对数据库中的数据进行增加，修改，删除及查询操作。 2. 什么是数据库管理系统 数据库管理系统（DataBase Management System，DBMS）：指一种操作和管理数据库的大型软件，用于建立、使用和维护数据库，对数据库进行统一管理和控制，以保证数据库的安全性和完整性。用户通过数据库管理系统访问数据库中表内的数据。 常见的数据库管理系统 MYSQL ：开源免费的数据库，小型的数据库. 已经被 Oracle 收购了. MySQL6.x 版本也开始收费。 Oracle ：收费的大型数据库，Oracle 公司的产品。Oracle 收购 SUN 公司，收购 MYSQL。 DB2 ：IBM 公司的数据库产品, 收费的。常应用在银行系统中. SQLServer：MicroSoft 公司收费的中型的数据库。C#、.net 等语言常使用。 SyBase ：已经淡出历史舞台。提供了一个非常专业数据建模的工具 PowerDesigner。 SQLite : 嵌入式的小型数据库，应用在手机端。 Java 相关的数据库：MYSQL，Oracle． 这里使用 MySQL 数据库。MySQL 中可以有多个数据库，数据库是真正存储数据的地方。 数据库与数据库管理系统的关系 mysqlmysql 的管理安装linux: \\--yum -y install mariadb mariadb-server OR --yum -y install mysql mysql-server window \\--http://dev.mysql.com/downloads/mysql/ 启动\\--service mysqld start #开启 --chkconfig mysqld on #设置开机自启 OR --systemctl start mariadb --systemctl enable mariadb 查看\\-- ps aux |grep mysqld #查看进程 -- netstat -an |grep 3306 #查看端口 设置密码\\-- mysqladmin -uroot password &#39;123&#39; #设置初始密码，初始密码为空因此-p选项没有用 -- mysqladmin -u root -p123 password &#39;1234&#39; #修改root用户密码 登录\\-- mysql #本地登录，默认用户root，空密码，用户为root@127.0.0.1 -- mysql -uroot -p1234 #本地登录，指定用户名和密码，用户为root@127.0.0.1 -- mysql -uroot -p1234 -h 192.168.31.95 #远程登录，用户为root@192.168.31.95 mysql 的常用命令 \\-- -- 启动mysql服务与停止mysql服务命令： -- -- net start mysql -- net stop mysql -- -- -- 登陆与退出命令： -- -- mysql －h 服务器IP -P 端口号 -u 用户名 -p 密码 －－prompt 命令提示符 －－delimiter 指定分隔符 -- mysql －h 127.0.0.1 -P 3306 -uroot -p123 -- quit------exit----\\\\q; -- -- -- \\\\s; ------my.ini文件：\\[mysql\\] default-character-set=gbk \\[mysqld\\] character-set-server=gbk -- -- prompt 命令提示符（\\\\D:当前日期 \\\\d:当前数据库 \\\\u:当前用户） -- -- \\\\T(开始日志) \\\\t(结束日志) -- -- show warnings; -- -- help() ? \\\\h -- -- \\\\G； -- -- select now(); -- select version(); -- select user; -- -- \\\\c 取消命令 -- -- delimiter 指定分隔符 忘记密码??? 方法一: 启动 mysql 时, 跳过授权表 \\[root@controller ~\\]# service mysqld stop \\[root@controller ~\\]# mysqld\\_safe --skip-grant-table &amp; \\[root@controller ~\\]# mysql mysql&gt; select user,host,password from mysql.user; +----------+-----------------------+-------------------------------------------+ | user | host | password | +----------+-----------------------+-------------------------------------------+ | root | localhost | \\*A4B6157319038724E3560894F7F932C8886EBFCF | | root | localhost.localdomain | | | root | 127.0.0.1 | | | root | ::1 | | | | localhost | | | | localhost.localdomain | | | root | % | \\*23AE809DDACAF96AF0FD78ED04B6A265E05AA257 | +----------+-----------------------+-------------------------------------------+ mysql&gt; update mysql.user set password=password(&quot;123&quot;) where user=&quot;root&quot; and host=&quot;localhost&quot;; mysql&gt; flush privileges; mysql&gt; exit \\[root@controller ~\\]# service mysqld restart \\[root@controller ~\\]# mysql -uroot -p123 方法二: 删库 删除与权限相关的库mysql，所有的授权信息都丢失，主要用于测试数据库或者刚刚建库不久没有授权数据的情况（从删库到跑路） \\[root@controller ~\\]# rm -rf /var/lib/mysql/mysql \\[root@controller ~\\]# service mysqld restart \\[root@controller ~\\]# mysql SQL 语句数据库是不认识 python 语言的，但是我们同样要与数据库交互，这时需要使用到数据库认识的语言 SQL 语句，它是数据库的代码。 结构化查询语言 (Structured Query Language) 简称 SQL，是一种数据库查询和程序设计语言，用于存取数据以及查询、更新和管理关系数据库系统。 创建数据库、创建数据表、向数据表中添加一条条数据信息均需要使用 SQL 语句。 SQL 分类： 数据定义语言：简称 DDL(Data Definition Language)，用来定义数据库对象：数据库，表，列等。关键字：create，alter，drop 等 数据操作语言：简称 DML(Data Manipulation Language)，用来对数据库中表的记录进行更新。关键字：insert，delete，update 等 数据控制语言：简称 DCL(Data Control Language)，用来定义数据库的访问权限和安全级别，及创建用户。 数据查询语言：简称 DQL(Data Query Language)，用来查询数据库中表的记录。关键字：select，from，where 等 SQL 通用语法 l SQL 语句可以单行或多行书写，以分号结尾 l 可使用空格和缩进来增强语句的可读性 l MySQL 数据库的 SQL 语句不区分大小写，建议使用大写，例如：SELECT * FROM user。 l 同样可以使用 /**/ 的方式完成注释 l MySQL 中的我们常使用的数据类型如下 ) 详细的数据类型如下 (不建议详细阅读！) | 分类 | 类型名称 | 说明 || 整数类型 | tinyInt | 很小的整数 || smallint | 小的整数 || mediumint | 中等大小的整数 || int(integer) | 普通大小的整数 || 小数类型 | float | 单精度浮点数 || double | 双精度浮点数 || decimal（m,d） | 压缩严格的定点数 || 日期类型 | year | YYYY 1901~2155 || time | HH:MM:SS -838:59:59~838:59:59 || date | YYYY-MM-DD 1000-01-01~9999-12-3 || datetime | YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00~ 9999-12-31 23:59:59 || timestamp | YYYY-MM-DD HH:MM:SS 19700101 00:00:01 UTC~2038-01-19 03:14:07UTC || 文本、二进制类型 | CHAR(M) | M 为 0~255 之间的整数 || VARCHAR(M) | M 为 0~65535 之间的整数 || TINYBLOB | 允许长度 0~255 字节 || BLOB | 允许长度 0~65535 字节 || MEDIUMBLOB | 允许长度 0~167772150 字节 || LONGBLOB | 允许长度 0~4294967295 字节 || TINYTEXT | 允许长度 0~255 字节 || TEXT | 允许长度 0~65535 字节 || MEDIUMTEXT | 允许长度 0~167772150 字节 || LONGTEXT | 允许长度 0~4294967295 字节 || VARBINARY(M) | 允许长度 0~M 个字节的变长字节字符串 || BINARY(M) | 允许长度 0~M 个字节的定长字节字符串 | 数据库操作基础操作: \\-- 1.创建表（类似于一个excel表） create table tab\\_name( field1 type\\[完整性约束条件\\], field2 type, ... fieldn type )\\[character set xxx\\]; -- 创建一个员工表employee create table employee( id int primary key auto\\_increment , name varchar(20), gender bit default 1, -- gender char(1) default 1 ----- 或者 TINYINT(1) birthday date, entry\\_date date, job varchar(20), salary double(4,2) unsigned, resume text -- 注意，这里作为最后一个字段不加逗号 ); /\\* 约束: primary key (非空且唯一) :能够唯一区分出当前记录的字段称为主键！ unique not null auto\\_increment 主键字段必须是数字类型。 外键约束 foreign key \\*/ -- 2.查看表信息 desc tab\\_name 查看表结构 show columns from tab\\_name 查看表结构 show tables 查看当前数据库中的所有的表 show create table tab\\_name 查看当前数据库表建表语句 -- 3.修改表结构 -- (1)增加列(字段) alter table tab\\_name add \\[column\\] 列名 类型［完整性约束条件］［first｜after 字段名］; alter table user add addr varchar(20) not null unique first/after username; #添加多个字段 alter table users2 add addr varchar(20), add age int first, add birth varchar(20) after name; -- (2)修改一列类型 alter table tab\\_name modify 列名 类型 \\[完整性约束条件\\]［first｜after 字段名］; alter table users2 modify age tinyint default 20; alter table users2 modify age int after id; -- (3)修改列名 alter table tab\\_name change \\[column\\] 列名 新列名 类型 \\[完整性约束条件\\]［first｜after 字段名］; alter table users2 change age Age int default 28 first; -- (4)删除一列 alter table tab\\_name drop \\[column\\] 列名; -- 思考：删除多列呢？删一个填一个呢？ alter table users2 add salary float(6,2) unsigned not null after name, drop addr; -- (5)修改表名 rename table 表名 to 新表名; -- (6)修该表所用的字符集 alter table student character set utf8; -- 4.删除表 drop table tab\\_name; ---5 添加主键，删除主键 alter table tab\\_name add primary key(字段名称,...) alter table users drop primary key; eg: mysql&gt; create table test5(num int auto\\_increment); ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key create table test(num int primary key auto\\_increment); -- 思考，如何删除主键？ alter table test modify id int; -- auto\\_increment没了，但这样写主键依然存在，所以还要加上下面这句 alter table test drop primary key;-- 仅仅用这句也无法直接删除主键 -- 唯一索引 alter table tab\\_name add unique \\[index|key\\] \\[索引名称\\](字段名称,...) alter table users add unique(name)-- 索引值默认为字段名show create table users; alter table users add unique key user\\_name(name);-- 索引值为user\\_name -- 添加联合索引 alter table users add unique index name\\_age(name,age);#show create table users; -- 删除唯一索引 alter table tab\\_name drop {index|key} index\\_name 完整性约束条件之主键约束单字段主键 主键字段特点：非空且唯一 ) create table users( id INT primary key, name varchar(20), city varchar(20) ); View Code 多字段联合主键 ) create table users2( id INT, name varchar(20), city varchar(20), primary key(name,id) ); View Code &lt;1&gt; 一张表只能有一个主键 &lt;2&gt; 主键类型不一定非是整型 表纪录操作表纪录之增，删，改) \\-- 1.增加一条记录insert /\\*insert ［into］ tab\\_name (field1,filed2,.......) values (value1,value2,.......);\\*/ create table employee\\_new( id int primary key auto\\_increment, name varchar(20) not null unique, birthday varchar(20), salary float(7,2) ); insert into employee\\_new (id,name,birthday,salary) values (1,&#39;yuan&#39;,&#39;1990-09-09&#39;,9000); insert into employee\\_new values (2,&#39;alex&#39;,&#39;1989-08-08&#39;,3000); insert into employee\\_new (name,salary) values (&#39;xialv&#39;,1000); -- 插入多条数据 insert into employee\\_new values (4,&#39;alvin1&#39;,&#39;1993-04-20&#39;,3000), (5,&#39;alvin2&#39;,&#39;1995-05-12&#39;,5000); -- set插入: insert ［into］ tab\\_name set 字段名=值 insert into employee\\_new set id=12,; -- 2.修改表记录 update tab\\_name set field1=value1,field2=value2,......\\[where 语句\\] /\\* UPDATE语法可以用新值更新原有表行中的各列。 SET子句指示要修改哪些列和要给予哪些值。 WHERE子句指定应更新哪些行。如没有WHERE子句，则更新所有的行。\\*/ update employee\\_new set birthday=&quot;1989-10-24&quot; WHERE id=1; --- 将yuan的薪水在原有基础上增加1000元。 update employee\\_new set salary=salary+4000 where name=&#39;yuan&#39;; -- 3.删除表纪录 delete from tab\\_name \\[where ....\\] /\\* 如果不跟where语句则删除整张表中的数据 delete只能用来删除一行记录 delete语句只能删除表中的内容，不能删除表本身，想要删除表，用drop TRUNCATE TABLE也可以删除表中的所有数据，词语句首先摧毁表，再新建表。此种方式删除的数据不能在 事务中恢复。\\*/ -- 删除表中名称为’alex’的记录。 delete from employee\\_new where name=&#39;alex&#39;; -- 删除表中所有记录。 delete from employee\\_new;-- 注意auto\\_increment没有被重置:alter table employee auto\\_increment=1; -- 使用truncate删除表中记录。 truncate table emp\\_new; View Code 表纪录之查 (单表查询)) \\-- 查询表达式 SELECT \\*|field1,filed2 ... FROM tab\\_name WHERE 条件 GROUP BY field HAVING 筛选 ORDER BY field LIMIT 限制条数 ---准备表 CREATE TABLE ExamResult( id INT PRIMARY KEY auto\\_increment, name VARCHAR (20), JS DOUBLE , Django DOUBLE , OpenStack DOUBLE ); INSERT INTO ExamResult VALUES (1,&quot;yuan&quot;,98,98,98), (2,&quot;tom&quot;,35,98,67), (3,&quot;jerry&quot;,59,59,62), (4,&quot;jean&quot;,88,89,82), (5,&quot;lilei&quot;,88,98,67), (6,&quot;kivn&quot;,86,100,55); -- （1）select \\[distinct\\] \\*|field1，field2，...... from tab\\_name -- 其中from指定从哪张表筛选，\\*表示查找所有列，也可以指定一个列 -- 表明确指定要查找的列，distinct用来剔除重复行。 -- 查询表中所有学生的信息。 select \\* from ExamResult; -- 查询表中所有学生的姓名和对应的英语成绩。 select name,JS from ExamResult; -- 过滤表中重复数据。 select distinct JS ,name from ExamResult; -- （2）select 也可以使用表达式，并且可以使用: 字段 as 别名或者:字段 别名 -- 在所有学生分数上加10分特长分显示。 select name,JS+10,Django+10,OpenStack+10 from ExamResult; -- 统计每个学生的总分。 select name,JS+Django+OpenStack from ExamResult; -- 使用别名表示学生总分。 select name as 姓名,JS+Django+OpenStack as 总成绩 from ExamResult; select name,JS+Django+OpenStack 总成绩 from ExamResult; select name JS from ExamResult; -- what will happen?----&gt;记得加逗号 -- （3）使用where子句，进行过滤查询。 -- 查询姓名为XXX的学生成绩 select \\* from ExamResult where name=&#39;jerry&#39;; -- 查询英语成绩大于90分的同学 select id,name,JS from ExamResult where JS&gt;90; -- 查询总分大于200分的所有同学 select name,JS+Django+OpenStack as 总成绩 from ExamResult where JS+Django+OpenStack&gt;200 ; -- where字句中可以使用： -- 比较运算符： &gt; &lt; &gt;= &lt;= &lt;&gt; != between 80 and 100 值在10到20之间 in(80,90,100) 值是10或20或30 like &#39;jerry%&#39; /\\* pattern可以是%或者\\_， 如果是%则表示任意多字符，此例如唐僧,唐国强 如果是\\_则表示一个字符唐\\_，只有唐僧符合。两个\\_则表示两个字符：\\_\\_ \\*/ -- 逻辑运算符 在多个条件直接可以使用逻辑运算符 and or not -- 练习 -- 查询JS分数在 70－100之间的同学。 select name ,JS from ExamResult where JS between 80 and 100; -- 查询Django分数为75,76,77的同学。 select name ,Django from ExamResult where Django in (75,98,77); -- 查询所有姓王的学生成绩。 select \\* from ExamResult where name like &#39;王%&#39;; -- 查询JS分&gt;90，Django分&gt;90的同学。 select id,name from ExamResult where JS&gt;90 and Django &gt;90; -- 查找缺考数学的学生的姓名 select name from ExamResult where Database is null; -- （4）Order by 指定排序的列，排序的列即可是表中的列名，也可以是select 语句后指定的别名。 -- select \\*|field1,field2... from tab\\_name order by field \\[Asc|Desc\\] -- Asc 升序、Desc 降序，其中asc为默认值 ORDER BY 子句应位于SELECT语句的结尾。 -- 练习： -- 对JS成绩排序后输出。 select \\* from ExamResult order by JS; -- 对总分排序按从高到低的顺序输出 select name ,(ifnull(JS,0)+ifnull(Django,0)+ifnull(Database,0)) 总成绩 from ExamResult order by 总成绩 desc; -- 对姓李的学生成绩排序输出 select name ,(ifnull(JS,0)+ifnull(Django,0)+ifnull(OpenStack,0)) 总成绩 from ExamResult where name like &#39;a%&#39; order by 总成绩 desc; -- （5）group by 分组查询： CREATE TABLE order\\_menu( id INT PRIMARY KEY auto\\_increment, product\\_name VARCHAR (20), price FLOAT(6,2), born\\_date DATE, class VARCHAR (20) ); INSERT INTO order\\_menu (product\\_name,price,born\\_date,class) VALUES (&quot;苹果&quot;,20,20170612,&quot;水果&quot;), (&quot;香蕉&quot;,80,20170602,&quot;水果&quot;), (&quot;水壶&quot;,120,20170612,&quot;电器&quot;), (&quot;被罩&quot;,70,20170612,&quot;床上用品&quot;), (&quot;音响&quot;,420,20170612,&quot;电器&quot;), (&quot;床单&quot;,55,20170612,&quot;床上用品&quot;), (&quot;草莓&quot;,34,20170612,&quot;水果&quot;); -- 注意,按分组条件分组后每一组只会显示第一条记录 -- group by字句，其后可以接多个列名，也可以跟having子句,对group by 的结果进行筛选。 -- 按位置字段筛选 select \\* from order\\_menu group by 5; -- 练习：对购物表按类名分组后显示每一组商品的价格总和 select class,SUM(price)from order\\_menu group by class; -- 练习：对购物表按类名分组后显示每一组商品价格总和超过150的商品 select class,SUM(price)from order\\_menu group by class HAVING SUM(price)&gt;150; /\\* having 和 where两者都可以对查询结果进行进一步的过滤，差别有： &lt;1&gt;where语句只能用在分组之前的筛选，having可以用在分组之后的筛选； &lt;2&gt;使用where语句的地方都可以用having进行替换 &lt;3&gt;having中可以用聚合函数，where中就不行。 \\*/ -- GROUP\\_CONCAT() 函数 SELECT id,GROUP\\_CONCAT(name),GROUP\\_CONCAT(JS) from ExamResult GROUP BY id; -- （6）聚合函数： 先不要管聚合函数要干嘛，先把要求的内容查出来再包上聚合函数即可。 -- (一般和分组查询配合使用) --&lt;1&gt; 统计表中所有记录 -- COUNT(列名)：统计行的个数 -- 统计一个班级共有多少学生？先查出所有的学生，再用count包上 select count(\\*) from ExamResult; -- 统计JS成绩大于70的学生有多少个？ select count(JS) from ExamResult where JS&gt;70; -- 统计总分大于280的人数有多少？ select count(name) from ExamResult where (ifnull(JS,0)+ifnull(Django,0)+ifnull(OpenStack,0))&gt;280; -- 注意:count(\\*)统计所有行; count(字段)不统计null值. -- SUM(列名)：统计满足条件的行的内容和 -- 统计一个班级JS总成绩？先查出所有的JS成绩，再用sum包上 select JS as JS总成绩 from ExamResult; select sum(JS) as JS总成绩 from ExamResult; -- 统计一个班级各科分别的总成绩 select sum(JS) as JS总成绩, sum(Django) as Django总成绩, sum(OpenStack) as OpenStack from ExamResult; -- 统计一个班级各科的成绩总和 select sum(ifnull(JS,0)+ifnull(Django,0)+ifnull(Database,0)) as 总成绩 from ExamResult; -- 统计一个班级JS成绩平均分 select sum(JS)/count(\\*) from ExamResult ; -- 注意：sum仅对数值起作用，否则会报错。 -- AVG(列名)： -- 求一个班级JS平均分？先查出所有的JS分，然后用avg包上。 select avg(ifnull(JS,0)) from ExamResult; -- 求一个班级总分平均分 select avg((ifnull(JS,0)+ifnull(Django,0)+ifnull(Database,0))) from ExamResult ; -- Max、Min -- 求班级最高分和最低分（数值范围在统计中特别有用） select Max((ifnull(JS,0)+ifnull(Django,0)+ifnull(OpenStack,0))) 最高分 from ExamResult; select Min((ifnull(JS,0)+ifnull(Django,0)+ifnull(OpenStack,0))) 最低分 from ExamResult; -- 求购物表中单价最高的商品名称及价格 ---SELECT id, MAX(price) FROM order\\_menu;--id和最高价商品是一个商品吗? SELECT MAX(price) FROM order\\_menu; -- 注意：null 和所有的数计算都是null，所以需要用ifnull将null转换为0！ -- -----ifnull(JS,0) -- with rollup的使用 --&lt;2&gt; 统计分组后的组记录 -- （7） 重点：Select from where group by having order by -- Mysql在执行sql语句时的执行顺序： -- from where select group by having order by -- 分析: select JS as JS成绩 from ExamResult where JS成绩 &gt;70; ---- 不成功 select JS as JS成绩 from ExamResult having JS成绩 &gt;90; --- 成功 -- (8) limit SELECT \\* from ExamResult limit 1; SELECT \\* from ExamResult limit 2,5;--跳过前两条显示接下来的五条纪录 SELECT \\* from ExamResult limit 2,2; --- (9) 使用正则表达式查询 SELECT \\* FROM employee WHERE emp\\_name REGEXP &#39;^yu&#39;; SELECT \\* FROM employee WHERE emp\\_name REGEXP &#39;yun$&#39;; SELECT \\* FROM employee WHERE emp\\_name REGEXP &#39;m{2}&#39;; View Code 外键约束创建外键) \\--- 每一个班主任会对应多个学生 , 而每个学生只能对应一个班主任 ----主表 CREATE TABLE ClassCharger( id TINYINT PRIMARY KEY auto\\_increment, name VARCHAR (20), age INT , is\\_marriged boolean -- show create table ClassCharger: tinyint(1) ); INSERT INTO ClassCharger (name,age,is\\_marriged) VALUES (&quot;冰冰&quot;,12,0), (&quot;丹丹&quot;,14,0), (&quot;歪歪&quot;,22,0), (&quot;姗姗&quot;,20,0), (&quot;小雨&quot;,21,0); ----子表 CREATE TABLE Student( id INT PRIMARY KEY auto\\_increment, name VARCHAR (20), charger\\_id TINYINT, --切记:作为外键一定要和关联主键的数据类型保持一致 -- \\[ADD CONSTRAINT charger\\_fk\\_stu\\]FOREIGN KEY (charger\\_id) REFERENCES ClassCharger(id) ) ENGINE=INNODB; INSERT INTO Student(name,charger\\_id) VALUES (&quot;alvin1&quot;,2), (&quot;alvin2&quot;,4), (&quot;alvin3&quot;,1), (&quot;alvin4&quot;,3), (&quot;alvin5&quot;,1), (&quot;alvin6&quot;,3), (&quot;alvin7&quot;,2); DELETE FROM ClassCharger WHERE ; INSERT student (name,charger\\_id) VALUES (&quot;yuan&quot;,1); -- 删除居然成功,可是 alvin3显示还是有班主任id=1的冰冰的; -----------增加外键和删除外键--------- ALTER TABLE student ADD CONSTRAINT abc FOREIGN KEY(charger\\_id) REFERENCES classcharger(id); ALTER TABLE student DROP FOREIGN KEY abc; View Code INNODB 支持的 ON 语句) \\--外键约束对子表的含义: 如果在父表中找不到候选键,则不允许在子表上进行insert/update --外键约束对父表的含义: 在父表上进行update/delete以更新或删除在子表中有一条或多条对 -- 应匹配行的候选键时,父表的行为取决于：在定义子表的外键时指定的 -- on update/on delete子句 -----------------innodb支持的四种方式--------------------------------------- -----cascade方式 在父表上update/delete记录时，同步update/delete掉子表的匹配记录 -----外键的级联删除：如果父表中的记录被删除，则子表中对应的记录自动被删除-------- FOREIGN KEY (charger\\_id) REFERENCES ClassCharger(id) ON DELETE CASCADE ------set null方式 在父表上update/delete记录时，将子表上匹配记录的列设为null -- 要注意子表的外键列不能为not null FOREIGN KEY (charger\\_id) REFERENCES ClassCharger(id) ON DELETE SET NULL ------Restrict方式 :拒绝对父表进行删除更新操作(了解) ------No action方式 在mysql中同Restrict,如果子表中有匹配的记录,则不允许对父表对应候选键 -- 进行update/delete操作（了解） View Code 多表查询) \\-- 准备两张表 -- company.employee -- company.department create table employee( emp\\_id int auto\\_increment primary key not null, emp\\_name varchar(50), age int, dept\\_id int ); insert into employee(emp\\_name,age,dept\\_id) values (&#39;A&#39;,19,200), (&#39;B&#39;,26,201), (&#39;C&#39;,30,201), (&#39;D&#39;,24,202), (&#39;E&#39;,20,200), (&#39;F&#39;,38,204); create table department( dept\\_id int, dept\\_name varchar(100) ); insert into department values (200,&#39;人事部&#39;), (201,&#39;技术部&#39;), (202,&#39;销售部&#39;), (203,&#39;财政部&#39;); mysql&gt; select \\* from employee; +--------+----------+------+---------+ | emp\\_id | emp\\_name | age | dept\\_id | +--------+----------+------+---------+ | 1 | A | 19 | 200 | | 2 | B | 26 | 201 | | 3 | C | 30 | 201 | | 4 | D | 24 | 202 | | 5 | E | 20 | 200 | | 6 | F | 38 | 204 | +--------+----------+------+---------+ 6 rows in set (0.00 sec) mysql&gt; select \\* from department; +---------+-----------+ | dept\\_id | dept\\_name | +---------+-----------+ | 200 | 人事部 | | 201 | 技术部 | | 202 | 销售部 | | 203 | 财政部 | +---------+-----------+ 4 rows in set (0.01 sec) 准备表 多表查询之连接查询) mysql&gt; SELECT \\* FROM employee,department; -- select employee.emp\\_id,employee.emp\\_name,employee.age, -- department.dept\\_name from employee,department; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 1 | A | 19 | 200 | 201 | 技术部 | | 1 | A | 19 | 200 | 202 | 销售部 | | 1 | A | 19 | 200 | 203 | 财政部 | | 2 | B | 26 | 201 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 2 | B | 26 | 201 | 202 | 销售部 | | 2 | B | 26 | 201 | 203 | 财政部 | | 3 | C | 30 | 201 | 200 | 人事部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 202 | 销售部 | | 3 | C | 30 | 201 | 203 | 财政部 | | 4 | D | 24 | 202 | 200 | 人事部 | | 4 | D | 24 | 202 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 4 | D | 24 | 202 | 203 | 财政部 | | 5 | E | 20 | 200 | 200 | 人事部 | | 5 | E | 20 | 200 | 201 | 技术部 | | 5 | E | 20 | 200 | 202 | 销售部 | | 5 | E | 20 | 200 | 203 | 财政部 | | 6 | F | 38 | 204 | 200 | 人事部 | | 6 | F | 38 | 204 | 201 | 技术部 | | 6 | F | 38 | 204 | 202 | 销售部 | | 6 | F | 38 | 204 | 203 | 财政部 | +--------+----------+------+---------+---------+-----------+ 1. 笛卡尔积查询 ) \\-- 查询两张表中都有的关联数据,相当于利用条件从笛卡尔积结果中筛选出了正确的结果。 select \\* from employee,department where employee.dept\\_id = department.dept\\_id; --select \\* from employee inner join department on employee.dept\\_id = department.dept\\_id; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 5 | E | 20 | 200 | 200 | 人事部 | +--------+----------+------+---------+---------+-----------+ 2. 内连接 ) \\--（1）左外连接：在内连接的基础上增加左边有右边没有的结果 select \\* from employee left join department on employee.dept\\_id = department.dept\\_id; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 5 | E | 20 | 200 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 6 | F | 38 | 204 | NULL | NULL | +--------+----------+------+---------+---------+-----------+ --（2）右外连接：在内连接的基础上增加右边有左边没有的结果 select \\* from employee RIGHT JOIN department on employee.dept\\_id = department.dept\\_id; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 5 | E | 20 | 200 | 200 | 人事部 | | NULL | NULL | NULL | NULL | 203 | 财政部 | +--------+----------+------+---------+---------+-----------+ --（3）全外连接：在内连接的基础上增加左边有右边没有的和右边有左边没有的结果 -- mysql不支持全外连接 full JOIN -- mysql可以使用此种方式间接实现全外连接 select \\* from employee RIGHT JOIN department on employee.dept\\_id = department.dept\\_id UNION select \\* from employee LEFT JOIN department on employee.dept\\_id = department.dept\\_id; +--------+----------+------+---------+---------+-----------+ | emp\\_id | emp\\_name | age | dept\\_id | dept\\_id | dept\\_name | +--------+----------+------+---------+---------+-----------+ | 1 | A | 19 | 200 | 200 | 人事部 | | 2 | B | 26 | 201 | 201 | 技术部 | | 3 | C | 30 | 201 | 201 | 技术部 | | 4 | D | 24 | 202 | 202 | 销售部 | | 5 | E | 20 | 200 | 200 | 人事部 | | NULL | NULL | NULL | NULL | 203 | 财政部 | | 6 | F | 38 | 204 | NULL | NULL | +--------+----------+------+---------+---------+-----------+ -- 注意 union与union all的区别：union会去掉相同的纪录 3. 外连接 多表查询之复合条件连接查询) \\-- 查询员工年龄大于等于25岁的部门 SELECT DISTINCT department.dept\\_name FROM employee,department WHERE employee.dept\\_id = department.dept\\_id AND age&gt;25; --以内连接的方式查询employee和department表，并且以age字段的升序方式显示 select employee.emp\\_id,employee.emp\\_name,employee.age,department.dept\\_name from employee,department where employee.dept\\_id = department.dept\\_id order by age asc; View Code 多表查询之子查询) \\-- 子查询是将一个查询语句嵌套在另一个查询语句中。 -- 内层查询语句的查询结果，可以为外层查询语句提供查询条件。 -- 子查询中可以包含：IN、NOT IN、ANY、ALL、EXISTS 和 NOT EXISTS等关键字 -- 还可以包含比较运算符：= 、 !=、&gt; 、&lt;等 -- 1. 带IN关键字的子查询 ---查询employee表，但dept\\_id必须在department表中出现过 select \\* from employee where dept\\_id IN (select dept\\_id from department); +--------+----------+------+---------+ | emp\\_id | emp\\_name | age | dept\\_id | +--------+----------+------+---------+ | 1 | A | 19 | 200 | | 2 | B | 26 | 201 | | 3 | C | 30 | 201 | | 4 | D | 24 | 202 | | 5 | E | 20 | 200 | +--------+----------+------+---------+ 5 rows in set (0.01 sec) -- 2. 带比较运算符的子查询 -- =、!=、&gt;、&gt;=、&lt;、&lt;=、&lt;&gt; -- 查询员工年龄大于等于25岁的部门 select dept\\_id,dept\\_name from department where dept\\_id IN (select DISTINCT dept\\_id from employee where age&gt;=25); -- 3. 带EXISTS关键字的子查询 -- EXISTS关字键字表示存在。在使用EXISTS关键字时，内层查询语句不返回查询的记录。 -- 而是返回一个真假值。Ture或False -- 当返回Ture时，外层查询语句将进行查询；当返回值为False时，外层查询语句不进行查询 select \\* from employee WHERE EXISTS (SELECT dept\\_name from department where dept\\_id=203); --department表中存在dept\\_id=203，Ture select \\* from employee WHERE EXISTS (SELECT dept\\_name from department where dept\\_id=205); -- Empty set (0.00 sec) ps: create table t1(select \\* from t2); View Code","categories":[{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/categories/sql/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/tags/sql/"}]},{"title":"资源","slug":"资源","date":"2020-09-03T12:22:56.040Z","updated":"2020-09-22T02:24:01.006Z","comments":true,"path":"2020/09/03/资源/","link":"","permalink":"https://topone233.github.io/2020/09/03/%E8%B5%84%E6%BA%90/","excerpt":"","text":"Markdown写作工具写作工具：Typora Markdown 写作工具 下载链接： Typora官网：https://typora.io/ 自用：0.9.89版本官网安装包（蓝奏云）：https://wwe.lanzous.com/icxyogbrykd 图片上传工具：Picgo 搭配Typora，效率神器 下载链接： Typora官网：https://molunerfinn.com/PicGo 自用：2.2.2版本官网安装包（蓝奏云）：https://wwe.lanzous.com/ie0iWgbryha Typora + Picgo + SM图床 设置： Typora： Ctrl + 逗号 进入偏好设置 -&gt; 图像 插入图片时 -&gt; 上传图片 -&gt; 勾选前两项 上传服务设定 -&gt; PicGo（app）-&gt; PicGo本地安装路径 打开Picgo -&gt; 打开配置文件 -&gt; 修改server端口号为：36677 插件中心 -&gt; 搜索：smms-user -&gt; 安装 到SM.MS官网 -&gt; User -&gt; Dashboard -&gt; API Token -&gt; 生成并复制Token 回到Picgo -&gt; 粘贴到刚才的插件 配置plugin-smms-user -&gt; 配置uploder-smms-user 上传区选择SM.MS登录用户 重启Picgo 回到第三步Typora页面 -&gt; 验证图片上传路径 -&gt; 验证成功 超赞的壁纸网站如果要安利一个壁纸网站的话，那么不管你问我多少遍，我都强烈推荐 Wallhaven.cc Wallhaven 号称「网上最好的壁纸」，这是一点也不谦虚，真是「天生骄傲」，口气这么大，当然肯定有它骄傲的地方。它界面简洁，没有任何广告。虽然这上面的壁纸基本都是用户上传的，但绝对都是一等一的优质高分辨率壁纸。 科学上网jsproxy +cloudflare 实现浏览器的在线代理： https://myproxy.qsx1c.workers.dev/ 搭建过程参考自：https://yoyling.com/enjoy/jsproxy.html","categories":[{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/categories/%E8%B5%84%E6%BA%90/"}],"tags":[{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/tags/%E8%B5%84%E6%BA%90/"},{"name":"配置","slug":"配置","permalink":"https://topone233.github.io/tags/%E9%85%8D%E7%BD%AE/"}]},{"title":"SQL Or NoSQL","slug":"SQL Or NoSQL","date":"2020-09-03T11:43:12.624Z","updated":"2020-09-21T04:32:48.451Z","comments":true,"path":"2020/09/03/SQL Or NoSQL/","link":"","permalink":"https://topone233.github.io/2020/09/03/SQL%20Or%20NoSQL/","excerpt":"","text":"原文地址:https://www.cnblogs.com/xrq730/p/11039384.html 前言你是否在为系统的数据库来一波大流量就几乎打满 CPU，日常 CPU 居高不下烦恼？你是否在各种 NoSql 间纠结不定，到底该选用那种最好？今天的你就是昨天的我，这也是写这篇文章的初衷。 这篇文章是我好几个月来一直想写的一篇文章，也是一直想学习的一个内容，作为互联网从业人员，我们要知道关系型数据库（MySql、Oracle）无法满足我们对存储的所有要求，因此对底层存储的选型，对每种存储引擎的理解非常重要。同时也由于过去一段时间的工作经历，对这块有了一些更多的思考，想通过自己的总结把这块写出来分享给大家。 结构化数据、非结构化数据与半结构化数据文章的开始，聊一下结构化数据、非结构化数据与半结构化数据，因为数据特点的不同，将在技术上直接影响存储引擎的选型。 结构化数据根据定义结构化数据指的是由二维表结构来逻辑表达和实现的数据，严格遵循数据格式与长度规范，也称作为行数据，特点为：数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的。例如： 因此关系型数据库完美契合结构化数据的特点，关系型数据库也是关系型数据最主要的存储与管理引擎。 非结构化数据指的是数据结构不规则或不完整，没有任何预定义的数据模型，不方便用二维逻辑表来表现的数据，例如办公文档（Word）、文本、图片、HTML、各类报表、视频音频等。 半结构化数据介于结构化与非结构化数据之间的数据就是半结构化数据了，它是结构化数据的一种形式，虽然不符合二维逻辑这种数据模型结构，但是包含相关标记，用来分割语义元素以及对记录和字段进行分层。常见的半结构化数据有 XML 和 JSON，例如： &lt;person&gt; &lt;name&gt;张三&lt;/name&gt; &lt;age&gt;18&lt;/age&gt; &lt;phone&gt;12345&lt;/phone&gt; &lt;/person&gt;这种结构也被成为自描述的结构。 以关系型数据库的方式做存储的架构演进首先，我们看一下使用关系型数据库的方式，企业一个系统发展的几个阶段的架构演进（由于本文写的是 Sql 与 NoSql，因此只以存储方式作为切入点，不会涉及类似 MQ、ZK 这些中间件内容）： 阶段一：企业刚发展的阶段，最简单，一个应用服务器配一个关系型数据库，每次读写数据库。 阶段二：无论是使用 MySQL 还是 Oracle 还是别的关系型数据库，数据库通常不会先成为性能瓶颈，通常随着企业规模的扩大，一台应用服务器扛不住上游过来的流量且一台应用服务器会产生单点故障的问题，因此加应用服务器并且在流量入口使用 Nginx 做一层负载均衡，保证把流量均匀打到应用服务器上。 阶段三：随着企业规模的继续扩大，此时由于读写都在同一个数据库上，数据库性能出现一定的瓶颈，此时简单地做一层读写分离，每次写主库，读备库，主备库之间通过 binlog 同步数据，就能很大程度上解决这个阶段的数据库性能问题 阶段四：企业发展越来越好了，业务越来越大了，做了读写分离数据库压力还是越来越大，这时候怎么办呢，一台数据库扛不住，那我们就分几台吧，做分库分表，对表做垂直拆分，对库做水平拆分。以扩数据库为例，扩出两台数据库，以一定的单号（例如交易单号），以一定的规则（例如取模），交易单号对 2 取模为 0 的丢到数据库 1 去，交易单号对 2 取模为 1 的丢到数据库 2 去，通过这样的方式将写数据库的流量均分到两台数据库上。一般分库分表会使用 Shard 的方式，通过一个中间件，便于连接管理、数据监控且客户端无需感知数据库 ip 关系型数据库的优点上面的方式，看似可以解决问题（实际上确实也能解决很多问题），正常对关系型数据库做一下读写分离 + 分库分表，支撑个 1W + 的读写 QPS 还是问题不大的。但是受限于关系型数据库本身，这套架构方案依然有着明显的不足，下面对利用关系型数据库方式做存储的方案的优点先进行一下分析，后一部分再分析一下缺点，对某个技术的优缺点的充分理解是技术选型的前提。 易理解 因为行 + 列的二维表逻辑是非常贴近逻辑世界的一个概念，关系模型相对网状、层次等其他模型更加容易被理解 操作方便 通用的 SQL 语言使得操作关系型数据库非常方便，支持 join 等复杂查询，Sql + 二维关系是关系型数据库最无可比拟的优点，这种易用性非常贴近开发者 数据一致性 支持 ACID 特性，可以维护数据之间的一致性，这是使用数据库非常重要的一个理由之一，例如同银行转账，张三转给李四 100 元钱，张三扣 100 元，李四加 100 元，而且必须同时成功或者同时失败，否则就会造成用户的资损 数据稳定 数据持久化到磁盘，没有丢失数据风险，支持海量数据存储 服务稳定 最常用的关系型数据库产品 MySql、Oracle 服务器性能卓越，服务稳定，通常很少出现宕机异常 关系型数据库的缺点紧接着的，我们看一下关系型数据库的缺点，也是比较明显的。 高并发下 IO 压力大 数据按行存储，即使只针对其中某一列进行运算，也会将整行数据从存储设备中读入内存，导致 IO 较高 为维护索引付出的代价大 为了提供丰富的查询能力，通常热点表都会有多个二级索引，一旦有了二级索引，数据的新增必然伴随着所有二级索引的新增，数据的更新也必然伴随着所有二级索引的更新，这不可避免地降低了关系型数据库的读写能力，且索引越多读写能力越差。有机会的话可以看一下自己公司的数据库，除了数据文件不可避免地占空间外，索引占的空间其实也并不少 为维护数据一致性付出的代价大 数据一致性是关系型数据库的核心，但是同样为了维护数据一致性的代价也是非常大的。我们都知道 SQL 标准为事务定义了不同的隔离级别，从低到高依次是读未提交、读已提交、可重复度、串行化，事务隔离级别越低，可能出现的并发异常越多，但是通常而言能提供的并发能力越强。那么为了保证事务一致性，数据库就需要提供并发控制与故障恢复两种技术，前者用于减少并发异常，后者可以在系统异常的时候保证事务与数据库状态不会被破坏。对于并发控制，其核心思想就是加锁，无论是乐观锁还是悲观锁，只要提供的隔离级别越高，那么读写性能必然越差 水平扩展后带来的种种问题难处理 前文提过，随着企业规模扩大，一种方式是对数据库做分库，做了分库之后，数据迁移（1 个库的数据按照一定规则打到 2 个库中）、跨库 join（订单数据里有用户数据，两条数据不在同一个库中）、分布式事务处理都是需要考虑的问题，尤其是分布式事务处理，业界当前都没有特别好的解决方案 表结构扩展不方便 由于数据库存储的是结构化数据，因此表结构 schema 是固定的，扩展不方便，如果需要修改表结构，需要执行 DDL（data definition language）语句修改，修改期间会导致锁表，部分服务不可用 全文搜索功能弱 例如 like “% 中国真伟大 %”，只能搜索到 “2019 年中国真伟大，爱祖国”，无法搜索到 “中国真是太伟大了” 这样的文本，即不具备分词能力，且 like 查询在 “% 中国真伟大” 这样的搜索条件下，无法命中索引，将会导致查询效率大大降低 写了这么多，我的理解核心还是前三点，它反映出的一个问题是关系型数据库在高并发下的能力是有瓶颈的，尤其是写入 / 更新频繁的情况下，出现瓶颈的结果就是数据库 CPU 高、Sql 执行慢、客户端报数据库连接池不够等错误，因此例如万人秒杀这种场景，我们绝对不可能通过数据库直接去扣减库存。 可能有朋友说，数据库在高并发下的能力有瓶颈，我公司有钱，加 CPU、换固态硬盘、继续买服务器加数据库做分库不就好了，问题是这是一种性价比非常低的方式，花 1000 万达到的效果，换其他方式可能 100 万就达到了，不考虑人员、服务器投入产出比的 Leader 就是个不合格的 Leader，且关系型数据库的方式，受限于它本身的特点，可能花了钱都未必能达到想要的效果。至于什么是花 100 万就能达到花 1000 万效果的方式呢？可以继续往下看，这就是我们要说的 NoSql。 结合 NoSql 的方式做存储的架构演进像上文分析的，数据库作为一种关系型数据的存储引擎，存储的是关系型数据，它有优点，同时也有明显的缺点，因此通常在企业规模不断扩大的情况下，不会一味指望通过增强数据库的能力来解决数据存储问题，而是会引入其他存储，也就是我们说的 NoSql。 NoSql 的全称为 Not Only SQL，泛指非关系型数据库，是对关系型数据库的一种补充，特别注意补充这两个字，这意味着 NoSql 与关系型数据库并不是对立关系，二者各有优劣，取长补短，在合适的场景下选择合适的存储引擎才是正确的做法。 比较简单的 NoSql 就是缓存： 针对那些读远多于写的数据，引入一层缓存，每次读从缓存中读取，缓存中读取不到，再去数据库中取，取完之后再写入到缓存，对数据做好失效机制通常就没有大问题了。通常来说，缓存是性能优化的第一选择也是见效最明显的方案。 但是，缓存通常都是 KV 型存储且容量有限（基于内存），无法解决所有问题，于是再进一步的优化，我们继续引入其他 NoSql： 数据库、缓存与其他 NoSql 并行工作，充分发挥每种 NoSql 的特点。当然 NoSql 在性能方面大大优于关系挺数据库的同时，往往也伴随着一些特性的缺失，比较常见的就是事务功能的缺失。 下面看一下常用的 NoSql 及他们的代表产品，并对每种 NoSql 的优缺点和适用场景做一下分析，便于熟悉每种 NoSql 的特点，方便技术选型。 KV 型 NoSql（Redis）KV 型 NoSql 顾名思义就是以键值对形式存储的非关系型数据库，是最简单、最容易理解也是大家最熟悉的一种 NoSql，因此比较快地带过。Redis、MemCache 是其中的代表，Redis 又是 KV 型 NoSql 中应用最广泛的 NoSql，KV 型数据库以 Redis 为例，最大的优点我总结下来就两点： 数据基于内存，读写效率高 KV 型数据，时间复杂度为 O(1)，查询速度快 因此，KV 型 NoSql 最大的优点就是高性能，利用 Redis 自带的 BenchMark 做基准测试，TPS 可达到 10 万的级别，性能非常强劲。同样的 Redis 也有所有 KV 型 NoSql 都有的比较明显的缺点： 只能根据 K 查 V，无法根据 V 查 K 查询方式单一，只有 KV 的方式，不支持条件查询，多条件查询唯一的做法就是数据冗余，但这会极大的浪费存储空间 内存是有限的，无法支持海量数据存储 同样的，由于 KV 型 NoSql 的存储是基于内存的，会有丢失数据的风险 综上所述，KV 型 NoSql 最合适的场景就是缓存的场景： 读远多于写 读取能力强 没有持久化的需求，可以容忍数据丢失，反正丢了再查询一把写入就是了 例如根据用户 id 查询用户信息，每次根据用户 id 去缓存中查询一把，查到数据直接返回，查不到去关系型数据库里面根据 id 查询一把数据写到缓存中去。 搜索型 NoSql（ElasticSearch）传统关系型数据库主要通过索引来达到快速查询的目的，但是在全文搜索的场景下，索引是无能为力的，like 查询一来无法满足所有模糊匹配需求，二来使用限制太大且使用不当容易造成慢查询，搜索型 NoSql 的诞生正是为了解决关系型数据库全文搜索能力较弱的问题，ElasticSearch 是搜索型 NoSql 的代表产品。 全文搜索的原理是倒排索引，我们看一下什么是倒排索引。要说倒排索引我们先看下什么是正排索引，传统的正排索引是文档 –&gt; 关键字的映射，例如 “Tom is my friend” 这句话，会将其切分为 “Tom”、”is”、”my”、”friend” 四个单词，在搜索的时候对文档进行扫描，符合条件的查出来。这种方式原理非常简单，但是由于其检索效率太低，基本没什么实用价值。 倒排索引则完全相反，它是关键字 –&gt; 文档的映射，我用张表格展示一下就比较清楚了： 意思是我现在这里有四个短句： “Tom is Tom” “Tom is my friend” “Thank you, Betty” “Tom is Betty’s husband” 搜索引擎会根据一定的切分规则将这句话切成 N 个关键字，并以关键字的维度维护关键字在每个文本中的出现次数。这样下次搜索 “Tom” 的时候，由于 Tom 这个词语在 “Tom is Tom”、”Tom is my friend”、”Tom is Betty’s husband”三句话中都有出现，因此这三条记录都会被检索出来，且由于”Tom is Tom”这句话中”Tom”出现了 2 次，因此这条记录对”Tom” 这个单词的匹配度最高，最先展示。这就是搜索引擎倒排索引的基本原理，假设某个关键字在某个文档中出现，那么倒排索引中有两部分内容： 文档 ID 在该文档中出现的位置情况 可以举一反三，我们搜索 “Betty Tom” 这两个词语也是一样，搜索引擎将 “Betty Tom” 切分为 “Tom”、”Betty” 两个单词，根据开发者指定的满足率，比如满足率 = 50%，那么只要记录中出现了两个单词之一的记录都会被检索出来，再按照匹配度进行展示。 搜索型 NoSql 以 ElasticSearch 为例，它的优点为： 支持分词场景、全文搜索，这是区别于关系型数据库最大特点 支持条件查询，支持聚合操作，类似关系型数据库的 Group By，但是功能更加强大，适合做数据分析 数据写文件无丢失风险，在集群环境下可以方便横向扩展，可承载 PB 级别的数据 高可用，自动发现新的或者失败的节点，重组和重新平衡数据，确保数据是安全和可访问的 同样，ElasticSearch 也有比较明显的缺点： 性能全靠内存来顶，也是使用的时候最需要注意的点，非常吃硬件资源、吃内存，大数据量下 64G + SSD 基本是标配，算得上是数据库中的爱马仕了。为什么要专门提一下内存呢，因为内存这个东西是很值钱的，相同的配置多一倍内存，一个月差不多就要多花几百块钱，至于 ElasticSearch 内存用在什么地方，大概有如下这些： Indexing Buffer—-ElasticSearch 基于 Luence，Lucene 的倒排索引是先在内存里生成，然后定期以 Segment File 的方式刷磁盘的，每个 Segment File 实际就是一个完整的倒排索引 Segment Memory—- 倒排索引前面说过是基于关键字的，Lucene 在 4.0 后会将所有关键字以 FST 这种数据结构的方式将所有关键字在启动的时候全量加载到内存，加快查询速度，官方建议至少留系统一半内存给 Lucene 各类缓存 —-Filter Cache、Field Cache、Indexing Cache 等，用于提升查询分析性能，例如 Filter Cache 用于缓存使用过的 Filter 的结果集 Cluter State Buffer—-ElasticSearch 被设计为每个 Node 都可以响应用户请求，因此每个 Node 的内存中都包含有一份集群状态的拷贝，一个规模很大的集群这个状态信息可能会非常大 读写之间有延迟，写入的数据差不多 1s 样子会被读取到，这也正常，写入的时候自动加入这么多索引肯定影响性能 数据结构灵活性不高，ElasticSearch 这个东西，字段一旦建立就没法修改类型了，假如建立的数据表某个字段没有加全文索引，想加上，那么只能把整个表删了再重建 因此，搜索型 NoSql 最适用的场景就是有条件搜索尤其是全文搜索的场景，作为关系型数据库的一种替代方案。 另外，搜索型数据库还有一种特别重要的应用场景。我们可以想，一旦对数据库做了分库分表后，原来可以在单表中做的聚合操作、统计操作是否统统失效？例如我把订单表分 16 个库，1024 张表，那么订单数据就散落在 1024 张表中，我想要统计昨天浙江省单笔成交金额最高的订单是哪笔如何做？我想要把昨天的所有订单按照时间排序分页展示如何做？这就是搜索型 NoSql 的另一大作用了，我们可以把分表之后的数据统一打在搜索型 NoSql 中，利用搜索型 NoSql 的搜索与聚合能力完成对全量数据的查询。 至于为什么把它放在 KV 型 NoSql 后面作为第二个写呢，因为通常搜索型 NoSql 也会作为一层前置缓存，来对关系型数据库进行保护。 列式 NoSql（HBase）列式 NoSql，大数据时代最具代表性的技术之一了，以 HBase 为代表。 列式 NoSql 是基于列式存储的，那么什么是列式存储呢，列式 NoSql 和关系型数据库一样都有主键的概念，区别在于关系型数据库是按照行组织的数据： 看到每行有 name、phone、address 三个字段，这是行式存储的方式，且可以观察 id = 2 的这条数据，即使 phone 字段没有，它也是占空间的。 列式存储完全是另一种方式，它是按每一列进行组织的数据： 这么做有什么好处呢？大致有以下几点： 查询时只有指定的列会被读取，不会读取所有列 存储上节约空间，Null 值不会被存储，一列中有时候会有很多重复数据（尤其是枚举数据，性别、状态等），这类数据可压缩，行式数据库压缩率通常在 3:15:1 之间，列式数据库的压缩率一般在 8:130:1 左右 列数据被组织到一起，一次磁盘 IO 可以将一列数据一次性读取到内存中 第二点说到了数据压缩，什么意思呢，以比较常见的字典表压缩方式举例： 自己看图理解一下，应该就懂了。 接着继续讲讲优缺点，列式 NoSql，以 HBase 为代表的，优点为： 海量数据无限存储，PB 级别数据随便存，底层基于 HDFS（Hadoop 文件系统），数据持久化 读写性能好，只要没有滥用造成数据热点，读写基本随便玩 横向扩展在关系型数据库及非关系型数据库中都是最方便的之一，只需要添加新机器就可以实现数据容量的线性增长，且可用在廉价服务器上，节省成本 本身没有单点故障，可用性高 可存储结构化或者半结构化的数据 列数理论上无限，HBase 本身只对列族数量有要求，建议 1~3 个 说了这么多 HBase 的优点，又到了说 HBase 缺点的时候了： HBase 是 Hadoop 生态的一部分，因此它本身是一款比较重的产品，依赖很多 Hadoop 组件，数据规模不大没必要用，运维还是有点复杂的 KV 式，不支持条件查询，或者说条件查询非常非常弱吧，HBase 在 Scan 扫描一批数据的情况下还是提供了前缀匹配这种 API 的，条件查询除非定义多个 RowKey 做数据冗余 不支持分页查询，因为统计不了数据总数 因此 HBase 比较适用于那种 KV 型的且未来无法预估数据增长量的场景，另外 HBase 使用还是需要一定的经验，主要体现在 RowKey 的设计上。 文档型 NoSql（MongoDB）坦白讲，根据我的工作经历，文档型 NoSql 我只有比较浅的使用经验，因此这部分只能结合之前的使用与网上的文章大致给大家介绍一下。 什么是文档型 NoSql 呢，文档型 NoSql 指的是将半结构化数据存储为文档的一种 NoSql，文档型 NoSql 通常以 JSON 或者 XML 格式存储数据，因此文档型 NoSql 是没有 Schema 的，由于没有 Schema 的特性，我们可以随意地存储与读取数据，因此文档型 NoSql 的出现是解决关系型数据库表结构扩展不方便的问题的。 MongoDB 是文档型 NoSql 的代表产品，同时也是所有 NoSql 产品中的明星产品之一，因此这里以 MongoDB 为例。按我的理解，作为文档型 NoSql，MongoDB 是一款完全和关系型数据库对标的产品，就我们从存储上来看： 看到，关系型数据库是按部就班地每个字段一列存，在 MongDB 里面就是一个 JSON 字符串存储。关系型数据可以为 name、phone 建立索引，MongoDB 使用 createIndex 命令一样可以为列建立索引，建立索引之后可以大大提升查询效率。其他方面而言，就大的基本概念，二者之间基本也是类似的： 因此，对于 MongDB，我们只要理解成一个 Free-Schema 的关系型数据库就完事了，它的优缺点比较一目了然，优点： 没有预定义的字段，扩展字段容易 相较于关系型数据库，读写性能优越，命中二级索引的查询不会比关系型数据库慢，对于非索引字段的查询则是全面胜出 缺点在于： 不支持事务操作，虽然 Mongodb4.0 之后宣称支持事务，但是效果待观测 多表之间的关联查询不支持（虽然有嵌入文档的方式），join 查询还是需要多次操作 空间占用较大，这个是 MongDB 的设计问题，空间预分配机制 + 删除数据后空间不释放，只有用 db.repairDatabase() 去修复才能释放 目前没发现 MongoDB 有关系型数据库例如 MySql 的 Navicat 这种成熟的运维工具 总而言之，MongDB 的使用场景很大程度上可以对标关系型数据库，但是比较适合处理那些没有 join、没有强一致性要求且表 Schema 会常变化的数据。 总结：数据库与 NoSql 及各种 NoSql 间的对比最后一部分，做一个总结，本文归根到底是两个话题： 何时选用关系型数据库，何时选用非关系型数据库 选用非关系型数据库，使用哪种非关系型数据库 首先是第一个话题，关系型数据库与非关系型数据库的选择，在我理解里面无非就是两点考虑： 第一点，不多解释应该都理解，非关系型数据库都是通过牺牲了 ACID 特性来获取更高的性能的，假设两张表之间有比较强的一致性需求，那么这类数据是不适合放在非关系型数据库中的。 第二点，核心数据不走非关系型数据库，例如用户表、订单表，但是这有一个前提，就是这一类核心数据会有多种查询模式，例如用户表有 ABCD 四个字段，可能根据 AB 查，可能根据 AC 查，可能根据 D 查，假设核心数据，但是就是个 KV 形式，比如用户的聊天记录，那么 HBase 一存就完事了。 这几年的工作经验来看，非核心数据尤其是日志、流水一类中间数据千万不要写在关系型数据库中，这一类数据通常有两个特点： 写远高于读 写入量巨大 一旦使用关系型数据库作为存储引擎，将大大降低关系型数据库的能力，正常读写 QPS 不高的核心服务会受这一类数据读写的拖累。 接着是第二个问题，如果我们使用非关系型数据库作为存储引擎，那么如何选型？其实上面的文章基本都写了，这里只是做一个总结（所有的缺点都不会体现事务这个点，因为这是所有 NoSql 相比关系型数据库共有的一个问题）： 但是这里特别说明，选型一定要结合实际情况而不是照本宣科，比如： 企业发展之初，明明一个关系型数据库就能搞定且支撑一年的架构，搞一套大而全的技术方案出来 有一些数据条件查询多，更适合使用 ElasticSearch 做存储降低关系型数据库压力，但是公司成本有限，这种情况下这类数据可以尝试继续使用关系型数据库做存储 有一类数据格式简单，就是个 KV 类型且增长量大，但是公司没有 HBase 这方面的人才，运维上可能会有一定难度，出于实际情况考虑，可先用关系型数据库顶一阵子 所以，如果不考虑实际情况，虽然合适有些存储引擎更加合适，但是强行使用反而适得其反，总而言之，适合自己的才是最好的。","categories":[{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/categories/nosql/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/tags/sql/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/tags/nosql/"}]},{"title":"Spring 事务管理","slug":"Spring 事务管理","date":"2020-09-02T03:25:57.245Z","updated":"2020-09-17T02:23:57.198Z","comments":true,"path":"2020/09/02/Spring 事务管理/","link":"","permalink":"https://topone233.github.io/2020/09/02/Spring%20%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/","excerpt":"","text":"在数据库操作中事务管理是一个重要的概念，例如银行转账。 Spring的事务管理简化了传统的数据库事务管理流程，提高了开发效率。 1.编程式事务管理 在代码中显示调用beginTransaction、commit、rollback 等与事务处理相关的方法，这就是编程式事务管理。当只有少数事务操作时，编程式事务管理才比较合适。 基于底层API的编程式事务管理 基于 TransactionTemplate 的编程式事务管理 2.声明式事务管理 Spring的声明式事务管理是通过AOP技术实现的事务管理，其本质是对方法前后进行拦截，然后在目标方法开始之前创建或加入一个事务，在执行完目标方法之后根据执行情况提交或回滚事务。 声明式事务管理最大的优点是不需要通过编程的方式管理事务，因而不需要在业务逻辑代码中掺杂事务处理的代码，只需相关的事务规则声明便可以将事务规则应用到业务逻辑中。 使用声明式事务管理不仅因为其简单，更主要的是可以使得纯业务代码不被污染，极大的方便了后期的代码维护。 与编程式事务管理相比，唯一不足的是：最细粒度只能作用到方法级别，无法像编程式那样可以作用到代码块级别。（不过可以通过变通的方法解决，比如将需要进行事务处理的代码块独立为方法等）。 Spring的声明式事务管理通过两种方式实现：XML、@Transactional 注解 2.1 基于XML创建Dao层 package com.statement.dao; public interface TestDao { public int save(String sql, Object param[]); public int delete(String sql, Object param[]); }package com.statement.dao; @Repository(&quot;TestDao&quot;) public class TestDaoImpl implements TestDao { @Autowired private JdbcTemplate jdbcTemplate; @Override public int save(String sql, Object[] param) { return jdbcTemplate.update(sql, param); } @Override public int delete(String sql, Object[] param) { return jdbcTemplate.update(sql, param); } }创建Service层，依赖注入数据访问层。 package com.statement.service; public interface TestService { public int save(String sql, Object param[]); public int delete(String sql, Object param[]); }package com.statement.service; @Service(&quot;testService&quot;) public class TestServiceImpl implements TestService { @Autowired private TestDao testDao; @Override public int save(String sql, Object[] param) { return testDao.save(sql, param); } @Override public int delete(String sql, Object[] param) { return testDao.delete(sql, param); } }创建Controller层，依赖注入Service层。 package com.statement.controller; @Controller(&quot;statementController&quot;) public class StatementController { @Autowired private TestService testService; public String test() { String message=&quot;&quot;; String deleteSql=&quot;delete from user&quot;; String saveSql=&quot;insert into user values(?,?,?)&quot;; Object param[]={1,&quot;chen&quot;,&quot;男&quot;}; try{ testService.delete(deletSql, null); testService.save(saveSql, param); testService.save(saveSql, param); }catch(Exception e) { message=&quot;主键重复，事务回滚！&quot;; e.printStackTrace(); } return message; } }创建配置文件 &lt;!-- 为数据源添加事务管理器 --&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 编写通知声明事务 --&gt; &lt;tx:advice id=&quot;myAdvice&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;!-- *表示任意方法 --&gt; &lt;tx:method name=&quot;*&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- 编写AOP，让Spring自动对目标对象生成代理，需要使用AspectJ的表达式 --&gt; &lt;aop:config&gt; &lt;!-- 定义切入点 --&gt; &lt;aop:pointcut expression=&quot;execution(* com.statement.sservice.*.*())&quot; id=&quot;txPointCut&quot;/&gt; &lt;!-- 切面：将切入点与通知关联 --&gt; &lt;aop:advisor advice-ref=&quot;myAdvice&quot; pointcut-ref=&quot;txPointCut&quot;/&gt; &lt;/aop:config&gt;创建测试类 package com.statement.test; public class Test { public static void main(String[] args) { ApplicationContext appCon=new ClassPathXmlApplicationContext(&quot;/com/statement/applicationContext.xml&quot;); StatementController ct=(StatementController)appCon.getBean(&quot;statementController&quot;); String result=ct.test(); System.out.println(result); } } 运行结果： 主键重复，事务回滚！2.2 基于@Transactional@Transactional 可以作用于接口、接口方法、类、类的方法上。 当作用于类上时，该类的所有public方法都将具有该类型的事务属性，同时也可以在方法级别使用该注解来覆盖类级别的定义。 Spring小组不建议在接口或接口方法上使用该注解，因为它只有在使用基于接口的代理时才会生效。 Dao、Service、Controller层相同，仅展示修改的部分代码。 配置文件 &lt;!-- 为数据源添加事务管理器 --&gt; &lt;bean id=&quot;txManager&quot; class=&quot;org.springframework.jdbc.datasource.DataSourceTransactionManager&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;/bean&gt; &lt;!-- 为事务管理器注册注解驱动器 --&gt; &lt;tx:annotation-driven transaction-manager=&quot;txManager&quot;/&gt;Spring MVC通常在Service层进行事务管理。 package com.statement.service; @Service(&quot;testService&quot;) @Transactional // 指定这个类需要接受Spring的事务管理 // 只能针对public属性范围内的方法 public class TestServiceImpl implements TestService {","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"Spring AOP","slug":"Spring AOP","date":"2020-08-24T04:05:54.847Z","updated":"2020-09-17T02:11:31.498Z","comments":true,"path":"2020/08/24/Spring AOP/","link":"","permalink":"https://topone233.github.io/2020/08/24/Spring%20AOP/","excerpt":"","text":"1.AOP的概念AOP（Aspect Oriented Programming）面向切面编程，与OOP（Object Oriented Programming，面向对象编程）相辅相成，提供了与OOP不同的抽象软件结构的视角。在OOP中，以类为程序的基本单元，而AOP的基本单元是Aspect（切面）。 尽管OOP可以通过封装、继承达到代码的复用，但仍然有同样的代码分散在各个方法中。AOP采取横向抽取机制，将分散在各个方法中的重复代码提取出来，然后在编译或运行阶段应用到需要执行的地方。这种横向抽取是OOP无法实现的，因为OOP实现的父子关系的纵向复用。AOP不是OOP的替代品，而是补充，两者相辅相成。 1.1 切面Aspect 是指：封装横切到系统功能（如事务处理）的类。 1.2 连接点Joinpoint ：程序运行中的一些时间点，例如：方法的调用、异常的抛出。 1.3 切入点Pointcut ：指需要处理的连接点。Spring AOP中，所有的方法执行都是连接点，而切入点是一个描述信息，修饰连接点。通过切入点确定哪些连接点需要被处理。 1.4 通知Advice ：由切面添加到特定的连接点（满足切入点规则）的一段代码，即在定义好的切入点处所要执行的代码。可以理解为切面开启后切面的方法，通知是切面的具体实现。 根据Spring中通知中目标类方法中的连接点位置，通知可分为6种类型： 1.4.1 环绕通知​ MethodInterceptor 在目标方法执行前、后实施增强，可用于日志记录、事务处理等功能。@Around 1.4.2 前置通知​ MethodBeforeAdvice 在目标方法执行前实施增强，可用于权限管理等功能。@Before 1.4.3 后置返回通知​ AfterReturningAdvice 在目标方法执行成功后实施增强，可用于关闭流、删除临时文件等功能。@AfterReturning 1.4.4 后置（最终）通知​ AfterAdvice 在目标方法执行后实施增强，与后置返回不同的是，不管是否发生异常都要执行，类似于finally。可用于释放资源。@After 1.4.5 异常通知​ ThrowsAdvice 在方法抛出异常后实施增强，可用于处理异常、记录日志等功能。@AfterThrowing 1.4.6 引入通知​ IntroductionInterceptor 在目标类中添加一些新的方法和属性，可用于修改目标类（增强类）。 1.5 引入Introduction ：在不修改代码的前提下，引入可以在运行期为实现类动态的添加自定义的方法和属性 1.6 目标对象Target Object ：指所有被通知的对象。 1.7 代理Proxy ：通知应用到目标对象之后被动态创建的对象。 1.8 织入Weaving ：将切面代码插入到目标对象上，从而生成代理对象的过程。 织入方式： 编译期织入：需要有特殊的Java编译器 类装载期织入：需要有特殊的类装载器 动态代理织入：在运行期为目标类添加通知生成子类的方式。Spring AOP默认采用动态代理织入。 2.动态代理动态代理 Spring AOP中常用JDK和CGLIB两种动态代理技术。 2.1 JDK动态代理JDK动态代理必须借助一个接口才能产生代理对象。对于使用业务接口的类，Spring默认使用JDK动态代理实现AOP。 package dynamic.jdk; public interface TestDao { public void save(); public void modify(); public void delete(); }创建接口实现类作为目标类，在代理类中对其方法进行增强处理。 package dynamic.jdk; public class TestDaoImpl implements TestDao { @Override public void save() { System.out.println(&quot;保存&quot;); } @Override public void modify() { System.out.println(&quot;修改&quot;); } @Override public void delete() { System.out.println(&quot;删除&quot;); } }创建切面类，定义多个通知（增强处理的功能方法）。 package aspect; public class MyAspect { public void check() { System.out.println(&quot;模拟权限控制&quot;); } public void except() { System.out.println(&quot;模拟异常处理&quot;); } public void log() { System.out.println(&quot;模拟日志记录&quot;); } }创建代理类，JDK动态代理中代理类必须实现java.lang.reflect.InvocationHandler接口，并编写代理方法。 package dynamic.jdk; public class JDKDynamicProxy implements InvocationHandler { // 声明目标类接口对象（真实对象） private TestDao testDao; /** * 创建代理的方法，建立代理对象和真实对象的代理关系，并返回代理对象 */ public Object createProxy(TestDao testDao) { this.testDao=testDao; // 类加载器 ClassLoader cld=JDKDynamicProxy.class.getClassLoader(); // 被代理对象实现的所有接口 Class[] clazz=testDao.getClass().getInterFaces(); // 使用代理类进行增强，返回代理后的对象 return Proxy.newProxyInstance(cld, clazz, this); } /** * 代理的逻辑方法，所有动态代理类的方法调用都交给该方法处理 * proxy 是被代理对象 * method 是将要被执行的方法 * args 是执行方法是需要的参数 * return 是返回代理结果 */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { // 创建一个切面 MyAspect myAspect=new MyAspect(); // 前置增强 myAspect.check(); myAspect.except(); // 在目标类上调用方法并传入参数，相当于调用testDao中的方法 Object obj=method.invoke(testDao, args); // 后置增强 myAspect.log(); myAspect.monitor(); return obj; } }创建测试类，在main方法中创建代理对象和目标对象，然后从代理对象中获取对目标对象增强后的对象，最后调用该对象的添加、修改、删除方法。 package dynamic.jdk; public class JDKDynamicTest { public static void main(String[] args) { // 创建代理对象 JDKDynamicProxy jdkProxy=new JDKDynamicProxy(); // 创建目标对象 TestDao testDao=new TestDaoImpl(); // 从代理对象中获取增强后的目标对象。该对象是一个被代理的对象，它会进入代理的逻辑方法invoke中 TestDao testDaoAdvice=(TestDao)jdkProxy.createProxy(testDao); // 执行方法 testDaoAdvice.save(); System.out.println(&quot;===============&quot;); testDaoAdvice.modify(); System.out.println(&quot;===============&quot;); testDaoAdvice.delete(); } } 运行结果： 模拟权限控制 模拟异常处理 保存 模拟日志记录 =============== 模拟权限控制 模拟异常处理 修改 模拟日志记录 =============== 模拟权限控制 模拟异常处理 删除 模拟日志记录2.2 CGLIB 动态代理JDK动态代理必须提供接口才能使用，对于没有提供接口的类，只能采用CGLIB动态代理。 CGLIB（Code Generation Library，代码生成库）是一个高性能开源的代码生成库，采用非常底层的字节码技术，对指定的目标类生成一个子类，并对子类进行增强。Spring Core包已经集成了所需的jar包。 创建目标类，不需要实现任何接口。 package dynamic.cglib; public class TestDao { public void save() { System.out.println(&quot;保存&quot;); } public void modify() { System.out.println(&quot;修改&quot;); } public void delete() { System.out.println(&quot;删除&quot;); } }创建代理类，该类实现MethodInterceptor接口 package dynamic.cglib; public class CglibDynamicProxy implements MethodInterceptor { /** * 创建代理的方法，生成CGLIB代理对象 * target 是目标对象，需要增强的对象 * 返回目标对象的CGLLIB代理对象 */ public Object createProxy(Object target) { // 创建一个动态类对象，即增强类对象 Enhancer enhancer=new Enhancer(); // 确定需要增强的类，设置其父类 enhancer.setSuperclass(target.getClass()); // 确定代理逻辑对象为当前对象，要求当前对象实现MethodInterceptor的方法 enhancer.setCallback(this); // 返回创建的代理对象 return enhancer.create(); } /** * intercept 方法会在程序执行目标方法时被调用 * proxy 是CGLIb根据指定父类生成的代理对象 * method 是拦截方法 * args 是拦截方法的参数数组 * methodProxy 是方法的代理对象，用于执行父类的方法 * 返回代理结果 */ @Override public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { MyAspect myAspect=new MyAspect(); myAspect.check(); myAspect.except(); // 目标方法执行，返回代理结果 Object obj=methodProxy.invokeSuper(proxy, args); myAspect.log(); myAspect.monitor(); return obj; } }创建测试类。 package dynamic.cglib; public class CglibDynamicTest { public static void main(String[] args) { // 创建代理对象 CglibDynamicProxy cdp=new CglibDynamicProxy(); // 创建目标对象 TestDao testDao=new TestDao(); // 获取增强后的目标对象 TestDao testDaoAdvice=(TestDao)cdp.createProxy(testDao); // 执行方法 testDaoAdvice.save(); System.out.println(&quot;==============&quot;); testDaoAdvice.modify(); System.out.println(&quot;==============&quot;); testDaoAdvice.delete(); } } 运行结果相同3.基于代理类的AOP实现纵观AOP编程，程序员只需要参与三个部分： 定义普通业务组件 定义切入点，一个切入点可能横切多个业务组件 定义增强处理，增强处理就是AOP框架为普通业务组件织入的处理动作 所以进行AOP编程的关键就是定义切入点和定义增强处理，一旦定义了合适的切入点和增强处理，AOP框架将自动生成AOP代理，即：代理对象的方法=增强处理+被代理对象的方法。 Spring默认使用JDK动态代理实现AOP编程。使用org.springframework.aop.framework.ProxyFactoryBean 创建代理是Spring AOP实现的最基本方式。 3.1 ProxyFactoryBeanProxyFactoryBean 是org.springframework.beans.factory.FactoryBean 接口的实现类，FactoryBean负责实例化一个Bean实例，ProxyFactoryBean负责为其他Bean实例创建代理实例。 下面通过一个实现环绕通知的实例演示Spring使用ProxyFactoryBean创建AOP代理的过程。 3.1.1 创建切面类由于该实例实现环绕通知，切面类需要实现 MethodInterceptor 接口。 package spring.proxyfactorybean; public class MyAspect implements MethodInterceptor { @Override public Object invoke(MethodInvocation arg) throw Throwable { check(); except(); Object obj=arg.proceed(); log(); return obj; } public void check() { System.out.println(&quot;模拟权限控制&quot;); } public void except() { System.out.println(&quot;模拟异常处理&quot;); } public void log() { System.out.println(&quot;模拟日志记录&quot;); } }3.1.2 配置切面并指定代理切面类需要配置为Bean实例，这样Spring容器才能识别为切面对象。 applicationContext.xml &lt;!-- 定义目标对象（使用上一个案例的） --&gt; &lt;bean id=&quot;testDao&quot; class=&quot;dynamic.jdk.TestDaoImpl&quot;/&gt; &lt;!-- 创建一个切面 --&gt; &lt;bean id=&quot;myAspect&quot; class=&quot;spring.proxyfactorybean.MyAspect&quot;/&gt; &lt;!-- 使用Spring代理工厂定义一个名为testDaoProxy的代理对象 --&gt; &lt;bean id=&quot;testDaoProxy&quot; class=&quot;org.springframework.aop.framework.ProxyFactoryBean&quot;&gt; &lt;!-- 指定代理实现的接口 --&gt; &lt;property name=&quot;proxyInterfaces&quot; value=&quot;dynamic.jdk.TestDao&quot;/&gt; &lt;!-- 指定目标对象 --&gt; &lt;property name=&quot;target&quot; ref=&quot;testDao&quot;/&gt; &lt;!-- 指定切面，织入环绕通知 --&gt; &lt;property name=&quot;interceptorNames&quot; value=&quot;myAspect&quot;/&gt; &lt;!-- 指定代理方式，true指定CGLIB动态代理（默认为false，指定JDK动态代理）--&gt; &lt;property name=&quot;proxyTargetClass&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt;3.1.3 测试package spring.proxyfactorbean; public class ProxyFactoryBeanTest { public static void main(String[] args) { ApplicationContext appCon=new ClassPathXmlApplicationContext(&quot;/spring/proxyfactorybean/applicationContext.xml&quot;); TestDao testDaoAdvice=(TestDao)appCon.getBean(&quot;testDaoProxy&quot;); testDaoAdvice.save(); System.out.println(&quot;===============&quot;); testDaoAdvice.modify(); System.out.println(&quot;===============&quot;); testDaoAdvice.delete(); } } 运行结果与之前相同4.基于注解开发AspectJ AspectJ 是一个基于Java的AOP框架。从Spring 2.0以后引入了AspectJ的支持。建议使用AspectJ实现AOP。 AspectJ实现Spring AOP有两种方式：基于XML、基于注解。基于注解要比基于XML配置开发便捷许多。 注解名称 描述 @Aspect 用于定义一个切面。注解在切面类上。 @Pointcut 用于定义切入点表达式。在使用时需要定义一个切入点方法，该方法是一个返回void且方法体为空的普通方法。 @Before 用于定义前置通知。在使用时通常为其指定value属性值。 @AfterReturning 用于定义后置返回通知。在使用时通常为其指定value属性值。 @Around 用于定义环绕通知。在使用时通常为其指定value属性值。 @AfterThrowing 用于定义异常通知。在使用时通常为其指定value属性值。还有一个throwing属性，用于访问目标方法抛出的异常，该属性值与异常通知方法中同名的形参一致。 @After 用于定义后置（最终）通知。在使用时通常为其指定value属性值。 4.1 创建切面类，并进行注解首先需要使用@Aspect 定义一个切面类，由于该类在Spring中是作为组件使用的，所以还需要使用@Component 。然后使用@Pointcut 注解切入点表达式，并通过定义方法来表示切入点名称。最后在每个通知方法上添加相应的注解，并将切入点名称作为参数传递给需要执行增强的通知方法。 package aspectj.annotation; @Aspect @Component public class MyAspect { /** * execution(* dynamic.jdk.*.*(..))定义切入点表达式 * 意思是：匹配dynamic.jdk包中任意类的任意方法的执行 * 第一个* 返回类型，使用*代表所有类型。注意第一个*与包名之间有一个空格 * 第二个* 表示的类名，使用*代表匹配包中的所有类 * 第三个* 表示的是方法名，使用*表示所有方法 * (..)表示方法的参数，&quot;...&quot;表示任意参数 */ @Pointcut(&quot;execution(* dynamic.jdk.*.*(..))&quot;) private void myPointCut() {} /** * 前置通知，使用Joinpoint接口作为参数获取目标对象信息 */ @Before(&quot;myPointCut()&quot;) public void before(JoinPoint jp) { System.out.print(&quot;前置通知：模拟权限控制&quot;); System.out.println(&quot;, 目标类对象：&quot; + jp.getTarget() + &quot;, 被增强处理的方法：&quot; + jp.getSignature().getName()); } /** * 后置返回通知 */ @AfterReturning(&quot;myPointCut()&quot;) public void afterReturning(JoinPoint jp) { System.out.print(&quot;后置返回通知：模拟删除临时文件&quot;); System.out.println(&quot;, 被增强处理的方法：&quot; + jp.getSignature().getName()); } /** * 环绕通知 * ProceedingJoinPoint 是JoinPoint的子接口，代表可以执行的目标方法 * 返回值的类型必须是Object * 必须一个参数是ProceedingJoinPoint类型 * 必须 throws Throwable */ @Around(&quot;myPointCut()&quot;) public Object around(ProceedingJoinPoint pjp) throws Throwable { System.out.print(&quot;环绕开始：执行目标方法前，模拟开启事务&quot;); // 执行当前目标方法 Object obj=pjp,proceed(); System.out.println(&quot;, 目标类对象：&quot; + jp.getTarget() + &quot;, 被增强处理的方法：&quot; + jp.getSignature().getName()); return obj; } /** * 异常通知 */ @AfterThrowing(value=&quot;myPointCut()&quot;, throwing=&quot;e&quot;) public void except(Throwable e) { System.out.println(&quot;异常通知：&quot; + &quot;程序执行异常&quot; + e.getMessage()); } /** * 后置（最终）通知 */ @After(&quot;myPointCut()&quot;) public void after() { System.out.println(&quot;最终通知：模拟释放资源&quot;); } }4.2 注解目标类使用@Repository 将目标类 TestDaoImpl 注解为目标对象。 @Repository(&quot;testDao&quot;)4.3 创建配置文件applicationContext.xml &lt;!-- 指定需要扫描的包，使注解生效 --&gt; &lt;context:component-scan base-package=&quot;aspectj.annotation&quot;/&gt; &lt;context:component-scan base-package=&quot;dynamic.jdk&quot;/&gt; &lt;!-- 启动基于注解的AspectJ支持 --&gt; &lt;aop:aspectj-autoproxy /&gt;4.4 测试public class AnnotationAspectJTest { public static void main(String[] args) { ApplicationContext appCon=new ClassPathXmlApplicationContext(&quot;/aspectj/applicationContext.xml&quot;); TestDao testDaoAdvice=(TestDao)appCon.getBean(&quot;testDao&quot;); testDaoAdvice.save(); } } 运行结果： 前置通知：模拟权限控制，目标类对象：dynamic.jdk.TestDaoImpl@647fd8ce，被增强处理的方法：save 环绕开始：执行目标方法前，模拟开启事务 保存 最终通知：模拟释放资源 环绕结束：执行目标方法后，模拟关闭事务 后置返回通知：模拟删除临时文件，被增强处理的方法：save异常通知得到执行，需要在TestDaoImpl类的save方法中添加异常代码，例如“ int n = 10/0; ”。 运行结果： 前置通知：模拟权限控制，目标类对象：dynamic.jdk.TestDaoImpl@647fd8ce，被增强处理的方法：save 环绕开始：执行目标方法前，模拟开启事务 最终通知：模拟释放资源 异常通知：程序执行异常/ by zero","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"Spring Bean","slug":"Spring Bean","date":"2020-08-22T08:14:36.429Z","updated":"2020-09-17T02:24:43.791Z","comments":true,"path":"2020/08/22/Spring Bean/","link":"","permalink":"https://topone233.github.io/2020/08/22/Spring%20Bean/","excerpt":"","text":"Spring IoC容器可以创建、装配、配置应用组件对象，这里的组件对象称为Bean。 1.Bean 的配置Spring可以看作一个工厂，用于生产和管理Spring容器中的Bean。 如果要使用这个工厂生产和管理Bean，需要将Bean配置在Spring的配置文件中。支持 XML 和 Properties 两种格式的配置文件，常用 XML格式。 元素的常用属性及其子元素： 属性或子元素 描述 id Bean在BeanFactory中的唯一标识，在代码中通过BeanFactory获取Bean实例时需要依次作为索引名称 class Bean的具体实现类，例如dao.TestDIDaoImpl scope 指定Bean实例的作用域 &lt; property&gt; 用于设置一个属性。name 指定Bean实例中相应的属性名称、value 指定Bean的属性值、ref 指定属性对BeanFactory中其他Bean的引用关系 &lt; constructor-arg&gt; 使用构造方法注入，指定构造方法的参数。index指定参数的序号，ref指定对BeanFactory中其他Bean的引用关系，type指定参数类型，value指定参数的常量值 &lt; map&gt;、 &lt; set&gt; 的子元素，封装对应类型的依赖注入 &lt; list&gt; 的子元素，封装List 、数组类型的依赖注入 &lt; entry&gt; 的子元素，用于设置一个键值对 2.Bean 的实例化在面向对象编程时，如果要使用某个对象，需要事先实例化该对象。Spring中，也需要先实例化Bean。 有3种方式：构造方法实例化、静态工厂实例化、实例工厂实例化。最常用构造方法实例化。 2.1构造方法实例化Spring容器调用Bean对应类中的无参数构造方法来实例化Bean。 创建BeanClass类 package instance; public class BeanClass { public String message; public BeanClass() { message = &quot;构造方法实例化Bean&quot;; } public BeanClass(String s) { message = s; } }创建配置文件applicationContext.xml &lt;!-- 构造方法实例化Bean --&gt; &lt;bean id=&quot;demo&quot; class=&quot;instance.BeanClass&quot;/&gt;创建测试类 package test; public class TestInstance { public static void mani(String[] args) { ApplicationContext appCon = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); BeanClass b = (BeanClass)appCon.getBean(&quot;demo&quot;); System.out.println(b1 + b1.message); } } 结果：instance.BeanClass@490ab905构造方法实例化Bean3.Bean的作用域 scope参数 描述 singleton 默认的作用域，定义的Bean在Spring容器中只有一个Bean实例（返回同一个） prototype Spring容器每次获取Bean，都将创建一个新的Bean实例（每次都返回新的） request 每次HTTP请求都会返回一个Bean实例 session 在一个session中，将返回同一个Bean实例 application 为每个ServletContext对象创建一个实例，即同一个应用共享一个 websocket 为每个WebSocket对象创建一个实例 singleton 与 prototype是最常用的，后面4种仅在Web Spring应用上下文中使用。 4.Bean的生命周期一个对象的生命周期包括：创建（实例化与初始化）、使用、销毁。Bean也遵循这一过程。但是Spring通过了许多对外接口，允许开发者对 实例化（开辟空间）、初始化（属性初始化）、销毁 3个过程的前后做一些操作。 Spring容器可以管理singleton作用域下Bean的生命周期，能够精确知道Bean何时被创建，何时初始化完成，何时被销毁。而对于prototype作用域下的Bean，Spring只负责创建。创建之后Bean实例就交给客户端的代码管理，Spring将不再跟踪其生命周期，也不会管理这些Bean。 Bean的生命周期： 根据Bean的配置情况实例化一个Bean。 根据Spring上下文对实例化的Bean进行依赖注入，即对Bean的属性进行初始化。 如果Bean实现了 BeanNameAware接口，将调用它实现的 setBeanName(String beanId)方法，此处参数传递的是Bean的id，让实现这个接口的Bean知道自己在Spring容器中的的名字。 如果Bean实现了 BeanFactoryAware接口，将调用它实现的 setBeanFactory方法，此处参数传递的是当前Spring工厂实例的引用。此接口是在Bean实例化后、Setter方法之前调用。可以使得Bean获取容器的内部信息，从而进行某些定制化的操作。 如果Bean实现了 ApplicationContextAware接口，将调用它实现的setApplicationContext(ApplicationContext)方法，此处参数传递的是Spring上下文实例的引用。该方法会将容器本身作为参数传给该方法，将Spring传入的参数赋给该类对象的applicationContext实例变量，接下来可以通过该变量来访问容器本身。 如果Bean关联了 BeanPostProcessor接口，将调用初始化方法 postProcessBeforeInitialization(Object obj, String beanName)对Bean进行前置处理。BeanFactoryPostProcessor是Bean属性处理容器，管理所有未实例化的数据（修改属性）。 如果Bean实现了 InitializingBean接口，将调用 afterPropertiesSet方法。 如果Bean在Spring配置文件中配置了 init-method属性，将自动调用其配置的初始化方法。 注意：Spring为Bean提供了两种初始化方式：实现InitializingBean接口、init-method指定。 ​ 两种方式可以同时使用，但如果调用afterPropertiesSet时出错，则不会调用init-method指定的方法。 ​ 通过反射调用init-method指定的方法效率相对较低，但是消除了对Spring的依赖。 如果Bean关联了 BeanPostProcessor接口，将调用 postProcessAfterInitialization(Object obj, String beanName)方法进行后置处理，由于是在Bean初始化结束时调用After方法，也可用于内存或缓存技术。 注意：此时已经可以使用该Bean，由于该Bean的作用域是singleton，所以调用的是同一个Bean实例。 当Bean不再需要时将进入销毁阶段，如果Bean实现了 DisposableBean接口，则调用其实现的destroy方法将Bean销毁 如果在配置文件中通过 destroy-method属性指定了Bean的销毁方法，将调用其配置的销毁方法进行销毁 实例演示： package life; public class BeanLife { public void initMyself() { System.out.println(this.getClass().getName() + &quot;执行自定义的初始化方法&quot;); } public void destroyMyself() { System.out.println(this.getClass().getName() + &quot;执行自定义的销毁方法&quot;)； } }&lt;!-- 使用init-method属性指定初始化方法，使用destroy-method属性指定销毁方法--&gt; &lt;bean id=&quot;beanLife&quot; class=&quot;life.BeanLife&quot; init-method=&quot;initMyself&quot; destroy-method=&quot;destroyMyself&quot;/&gt;package test; public class TestLife { public static void main(String[] args) { ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); BeanLife blife = (BeanLife)ctx.genBean(&quot;beanLife&quot;); System.out.println(&quot;获得对象后&quot; + blife); // 关闭容器，销毁Bean对象 ctx.close(); } }5.Bean的装配Bean的装配可以理解为将Bean依赖注入到Spring容器中。Spring容器支持：基于XML配置、基于注解、自动装配等多种装配方式，最常见的是基于注解。 5.1基于注解的装配尽管使用XML配置文件可以很简单的装配Bean，但如果有大量的Bean，会导致XML配置文件过于庞大，不方便升级与维护，因此更推荐使用注解（annotation）。 @Repository 将数据访问层（DAO）的类标识为Bean。 @Service 将业务逻辑组件类（Service层）标注为Bean。 @Controller 将控制器组件类标注为Bean。 @Component 可以作用在任何层次上，标注一个Bean。为了更加层次化，不推荐使用。 @Autowired 对类成员变量、方法、构造方法进行标注，完成自动装配工作。可以消除setter和getter方法。默认按照Bean的类型进行装配，如果想按照名称装配，需要和@Qualifier 一起使用 @Resource(name=” “) 与@Autowired 功能一样，区别在于该注解默认是按照名称来装配注入的，只有找不到与名称匹配的Bean时才会按照Bean的类型来装配注入。@Resource有两个属性：name、type。name指定Bean实例名称，即按照名称来装配；type指定类型，即按照类型来装配。 @Qualifier 与@Autowired配合使用，当需要按照名称装配时。Bean的实例名称由@Qualifier 的参数指定。","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"Spring IoC","slug":"Spring IoC","date":"2020-08-22T08:09:05.509Z","updated":"2020-09-17T04:39:06.951Z","comments":true,"path":"2020/08/22/Spring IoC/","link":"","permalink":"https://topone233.github.io/2020/08/22/Spring%20IoC/","excerpt":"","text":"1. IoC 的基本概念 IoC（Inversion of Control，控制反转）是一个比较抽象的概念，是Spring框架的核心，用来消减程序的耦合问题。DI（Dependency Injection，依赖注入）是IoC的另外一种说法，只是从不同的角度描述相同的概念。 想吃面包，你可以自己做。也可以选择在面包店下单，告诉店家你的需求，然后等着吃就行。 想买汽车，直接给工厂下单付款即可。 上面的例子包含了控制反转的思想：把制作面包的主动权交给面包店。 当某个Java对象（调用者，例如我），需要调用另一个Java对象（被调用者，即被依赖对象，例如面包）时，以前我们通常会“new 被调用者”来创建对象（例如我们自己做面包）。这种方式会增加调用者与被调用者之间的耦合性，不利于后期代码的升级和维护。 Spring出现后，对象的实例由Spring容器（例如面包店）来创建。Spring容器会负责控制程序之间的关系（例如面包店负责控制我们与面包的关系）。这样控制权就由调用者转移到Spring容器，控制权发生了反转。 依赖注入：Spring容器负责将依赖对象赋值给调用者的成员变量，相当于为调用者注入它所依赖的实例。这就是依赖注入。 综上所述，控制反转是一种通过描述（XML或者注解）并通过第三方去产生或获取特定对象的方式。实现控制反转的是IoC容器，其实现方式是依赖注入。 2.IoC 容器前面我们知道，实现控制反转的是IoC容器。IoC容器的设计主要是基于BeanFactory 和 ApplicationContext 两个接口。 2.1 BeanFactory 接口BeanFactory 由org.springframework.beans.factory.BeanFactory接口定义，提供了完整的IoC服务支持，是一个管理Bean的工厂，主要负责初始化各种Bean。 BeanFactory接口有很多实现类，常用的是XmlBeanFactory，根据XML配置文件中的定义来配置Bean。 使用BeanFactory实例加载Spring配置文件实际并不多见，仅作了解。 2.2 ApplicationContext 接口ApplicationContext 是BeanFactory的子接口，也称应用上下文。除了包含BeanFactory的所有功能以外，还添加了对国际化、资源访问、事件传播等内容的支持。 创建ApplicationContext接口实例通常有以下三种方法： 2.2.1 通过ClassPathXmlApplicationContextpublic static void main(String[] args) { // 初始化Spring容器ApplicationContext，加载指定的XML配置文件 ApplicationContext appCon = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); // 通过容器获取test实例 TestDao tt = (TestDao)appCon.getBean(&quot;test&quot;); tt.sayHello(); }2.2.2通过FileSystemXmlApplicationContext创建// 仅需要修改这一行代码 ApplicationContext appCon = new FileSystemXmlApplicationContext(&quot;D:\\test\\applicationContext.xml&quot;);FileSystemXmlApplicationContext 将从指定文件的绝对路径中寻找XML配置文件，但是绝对路径会导致程序的灵活性变差，不推荐使用。通常Spring的Java应用采用ClassPathXmlApplicationContext类来实例化ApplicationContext容器，而Web应用中，将交给Web服务器完成。 2.2.3通过Web服务器实例化ApplicationContext容器Web服务器实例化ApplicationContext容器时，一般使用基于org.springframework.web.context.ContextLoaderListener的实现方式。 &lt;context-param&gt; &lt;!-- 加载src目录下的applicationContext.xml文件 --&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath:applicationContext.xml &lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 指定以ContextLoaderListener方式启动Spring容器 --&gt; &lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt; &lt;/listener&gt;3.依赖注入 Spring中实现IoC容器的方法是依赖注入。依赖注入的作用是在使用Spring创建对象时，动态地将其所依赖的对象（例如属性值）注入Bean组件中。 3.1使用属性的setter方法注入依赖注入通常有两种实现方式：构造方法注入、属性的setter方法注入。（这两种方式都是基于Java的反射机制）。 使用setter方法注入是Spring中最主流的注入方式，利用Java Bean规范所定义的setter方法来完成注入，灵活且可读性高。 下面将两者放在一起进行，方便对比。 3.1.1创建dao包在dao包创建TestDIDao接口及其实现类TestDIDaoImpl package dao; public interface TestDIDao { public void sayHello(); }package dao; public class TestDIDaoImpl implements TestDIDao { @Override public void sayHello() { System.out.println(&quot;TestDIDao say: Hello&quot;); } }3.1.2创建service包service包中创建TestDIService接口及其实现类TestDIServiceImpl package service; public interface TestDIService { public void sayHello(); }package service; import dao.TestDIDao; public class TestDIServiceImpl implements TestDIService { private TestDIDao testDIDao; // 添加testDIDao属性的setter方法，用于实现依赖注入 public void setTestDIDao(TestDIDao testDIDao) { this.testDIDao = testDIDao; } /* 构造方法，用于实现依赖注入接口对象testDIDao public TestDIServiceImpl(TestDIDao testDIDao) { super(); this.testDIDao = testDIDao; } */ @Override public void sayHello() { // 调用testDIDao中的sayHelllo方法 testDIDao.sayHello(); System.out.println(&quot;TestDIService setter方法注入 say: Hello&quot;); } }3.1.3编写配置文件在src根目录下创建Spring配置文件 applicationContext.xml。将TestDIServiceImpl类托管给Spring，让Spring创建其对象，同时调用其setter方法完成依赖注入。 &lt;!-- 将TestDIDaoImpl类配置给Spring，让Spring创建其实例 --&gt; &lt;bean id=&quot;myTestDIDao&quot; class=&quot;dao.TestDIDaoImpl&quot;/&gt; &lt;!-- 使用setter方法注入 --&gt; &lt;bean id=&quot;testDIService&quot; class=&quot;service.TestDIServiceImpl&quot;&gt; &lt;!-- 调用TestDIServiceImpl类的setter方法，将myTestDIDao注入到TestDIServiceImpl类的属性testDIDao上--&gt; &lt;property name=&quot;testDIDao&quot; ref=&quot;myTestDIDao&quot;/&gt; &lt;/bean&gt; /* &lt;!-- 使用构造方法注入 --&gt; &lt;bean id=&quot;testDIService&quot; class=&quot;service.TestDIServiceImpl&quot;&gt; &lt;!-- 将myTestDIDao注入到TestDIServiceImpl类的属性testDIDao上--&gt; &lt;!-- constructor-arg元素用于定义类构造方法的参数，index定义参数的位置--&gt; &lt;!-- ref指定某个实例的引用，如果参数是常量值，ref由value代替--&gt; &lt;constructor-arg index=&quot;o&quot; ref=&quot;myTestDIDao&quot;/&gt; &lt;/bean&gt; */3.1.4测试创建test包，并创建TestDI测试类 package test; public class TestDI { public static void main(String[] args) { ApplicationContext appCon = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;) TestDIservice ts = (TestDIService)appCon.getBean(&quot;testDIService&quot;); ts.sayHello(); } }","categories":[{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"}]},{"title":"CORS RESTful Web Service","slug":"CORS RESTful Web Service","date":"2020-08-20T13:28:36.452Z","updated":"2020-09-17T04:47:38.530Z","comments":true,"path":"2020/08/20/CORS RESTful Web Service/","link":"","permalink":"https://topone233.github.io/2020/08/20/CORS%20RESTful%20Web%20Service/","excerpt":"","text":"Enabling Cross Origin Requests for a RESTful Web Service 直奔主题，将使用Spring Boot快速构建项目部分省略，如果需要请访问：https://spring.io/guides/gs/rest-service-cors/ 前言RESTfulRepresentational State Transfer 表述行状态转移，是一种设计风格和开发方式。 Web应用最重要的REST原则是，客户端与服务端之间的交互请求是无状态的。客户端到服务端的请求必须包含理解请求所必需的信息；请求可以由任何可用服务器回答。 资源与URL REST全称是表述性状态转移，那究竟指的是什么的表述? 其实指的就是资源。任何事物，只要有被引用到的必要，它就是一个资源。 要让一个资源可以被识别，需要有个唯一标识，在Web中这个唯一标识就是URI(Uniform Resource Identifier)。URI既可以看成是资源的地址，也可以看成是资源的名称。如果某些信息没有使用URI来表示，那它就不能算是一个资源， 只能算是资源的一些信息而已。 统一资源接口 RESTful架构应该遵循统一接口原则，统一接口包含了一组受限的预定义的操作，不论什么样的资源，都是通过使用相同的接口进行资源的访问。接口应该使用标准的HTTP方法如GET，PUT和POST，并遵循这些方法的语义。 如果按照HTTP方法的语义来暴露资源，那么接口将会拥有安全性和幂等性的特性，例如GET和HEAD请求都是安全的， 无论请求多少次，都不会改变服务器状态。而GET、HEAD、PUT和DELETE请求都是幂等的，无论对资源操作多少次， 结果总是一样的，后面的请求并不会产生比第一次更多的影响。 CORSCross Origin Resource Sharing 跨源资源共享。 是一种机制，它使用额外的 HTTP头来告诉浏览器 让运行在一个 origin (domain) 上的Web应用被准许访问来自不同源服务器上的指定的资源。当一个资源从与该资源本身所在的服务器不同的域、协议或端口请求一个资源时，资源会发起一个跨域 HTTP 请求。 1.Resource Representation Classpackage com.example.restservicecors; public class Greeting { private final long id; private final String content; public Greeting() { this.id = -1; this.content = &quot;&quot;; } public Greeting(long id, String content) { this.id = id; this.content = content; } public long getId() { return id; } public String getContent() { return content; } }Spring使用Jackson JSON库自动将Greeting类型的实例编组为JSON 2.Resource Controllerpackage com.example.restservicecors; import java.util.concurrent.atomic.AtomicLong; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.CrossOrigin; import org.springframework.web.bind.annotation.RestController; @RestController public class GreetingController { private static final String template = &quot;Hello, %s!&quot;; private final AtomicLong counter = new AtomicLong(); @GetMapping(&quot;/greeting&quot;) public Greeting greeting(@RequestParam(required=false, defaultValue=&quot;World&quot;) String name) { System.out.println(&quot;==== in greeting ====&quot;); /** * 创建并返回一个新的Greeting对象 * 具有基于计数器（counter）的下一个值的 id 和 context * 并使用Greeting模板格式化给定的name */ return new Greeting(counter.incrementAndGet(), String.format(template, name)); } }@RestController将类标记为控制器，其中每个方法返回域对象而不是视图。包含 @Controller 和 @ResponseBody @GetMapping使HTTP GET /greeting 的请求，映射到greeting() 方法。对于其他HTTP请求也有对应的注释（例如 @PostMapping）。它们都派生自@RequeMapping。@RequeMapping(method=GET) @RequestParam将查询字符串参数名的值绑定到greeting()方法的名称参数中。如果请求中没有name参数，则使用默认参数“World” @ResponseBody告诉SpringMVC 不需要通过视图层呈现greeting对象，返回的greeting对象是响应体，应该直接写入 传统MVC控制器和RESTful web服务控制器之间的一个关键区别创建HTTP响应体的方式。RESTful 将对象数据将以JSON的形式直接写入HTTP响应，不依赖视图。 Greeting对象必须转换为JSON。由于Spring的HTTP消息转换器支持，不需要手动转换。在类路径中的Jackson2会自动选择Spring的MappingJackson2HttpMessageConverter来将Greeting实例转换为JSON。 3.Enabling CORS为了使RESTful Web的响应中包含CORS访问控制头，必须添加@CrossOrigin 在处理方法中（也可以添加到控制器类，该类的所有处理方法都启用CORS） @CrossOrigin(origins = &quot;http://localhost:9000&quot;， maxAge = 3000) @GetMapping(&quot;/greeting&quot;) public Greeting greeting(@RequestParam(required=false, defaultValue=&quot;World&quot;) String name) { System.out.println(&quot;==== in greeting ====&quot;); return new Greeting(counter.incrementAndGet(), String.format(template, name)); }@CrossOrigin仅对这个特定的方法允许跨源资源共享。包含以下属性 origins methods allowedHeaders exposedHeaders allowCredentials maxAge （默认30分钟） 4.Creating the Application Classpackage com.example.restservicecors; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.context.annotation.Bean; import org.springframework.web.servlet.config.annotation.CorsRegistry; import org.springframework.web.servlet.config.annotation.WebMvcConfigurer; @SpringBootApplication public class RestServiceCorsApplication { public static void main(String[] args) { SpringApplication.run(RestServiceCorsApplication.class, args); } /** * 添加一种方法来配置如何处理跨域资源共享 */ @Bean public WebMvcConfigurer corsConfigurer() { return new WebMvcConfigurer() { @Override public void addCorsMappings(CorsRegistry registry) { registry.addMapping(&quot;/greeting-javaconfig&quot;).allowedOrigins(&quot;http://localhost:9000&quot;); } }; } }@SpringBootApplication添加了以下所有内容： @Configuration：将类标记为程序上下文的Bean定义的源 @EnableAutoConfiguration：告诉Spring Boot根据类路径设置、其他bean和各种属性设置开始添加bean @ComponentScan：告诉Spring在com/example包中寻找其他组件、配置、服务，让它找到控制器 5.Build an executable JAR// 运行 ./mvnw spring-boot: run //构建jar文件 ./mvnw clean //运行jar文件 java -jar target/demo.jar6.Test the Service访问：http://localhost:8080/greeting 结果：{&quot;id&quot;:1,&quot;content&quot;:&quot;Hello, World!&quot;} 访问：http://localhost:8080/greeting?name=User 结果：{&quot;id&quot;:2,&quot;content&quot;:&quot;Hello, User!&quot;}创建一个js客户端来使用服务 hello.js $(document).ready(function() { $.ajax({ url: &quot;http://localhost:8080/greeting&quot; }).then(function(data, status, jqxhr) { $(&#39;.greeting-id&#39;).append(data.id); $(&#39;.greeting-content&#39;).append(data.content); console.log(jqxhr); }); });index.html &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Hello CORS&lt;/title&gt; &lt;script src=&quot;https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;hello.js&quot;&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;div&gt; &lt;p class=&quot;greeting-id&quot;&gt;The ID is &lt;/p&gt; &lt;p class=&quot;greeting-content&quot;&gt;The content is &lt;/p&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;./mvnw spring-boot:run -Dserver.port=9000 访问：http://localhost:9000/ 结果： The ID is 1 The content is Hello,World!","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"},{"name":"CORS","slug":"cors","permalink":"https://topone233.github.io/tags/cors/"},{"name":"RESTful","slug":"restful","permalink":"https://topone233.github.io/tags/restful/"}]},{"title":"Java 集合","slug":"Java 集合","date":"2020-07-31T16:00:00.000Z","updated":"2020-09-17T02:21:21.757Z","comments":true,"path":"2020/08/01/Java 集合/","link":"","permalink":"https://topone233.github.io/2020/08/01/Java%20%E9%9B%86%E5%90%88/","excerpt":"","text":"","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"集合","slug":"集合","permalink":"https://topone233.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"Java IO","slug":"Java IO","date":"2020-07-26T16:00:00.000Z","updated":"2020-09-17T02:21:52.548Z","comments":true,"path":"2020/07/27/Java IO/","link":"","permalink":"https://topone233.github.io/2020/07/27/Java%20IO/","excerpt":"","text":"","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"IO","slug":"io","permalink":"https://topone233.github.io/tags/io/"}]},{"title":"十大经典排序算法(Java)","slug":"十大经典排序算法(Java)","date":"2020-07-21T06:14:54.671Z","updated":"2020-09-20T05:49:41.885Z","comments":true,"path":"2020/07/21/十大经典排序算法(Java)/","link":"","permalink":"https://topone233.github.io/2020/07/21/%E5%8D%81%E5%A4%A7%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95(Java)/","excerpt":"","text":"1.排序算法说明1.1 排序的定义对一序列对象根据某个关键字进行排序。本文对十大排序算法进行解读。 1.2 术语说明 稳定：如果 a 原本在 b 前面，而 a=b，排序之后 a 仍然在 b 的前面； 不稳定：如果 a 原本在 b 的前面，而 a=b，排序之后 a 可能会出现在 b 的后面； 内排序：所有排序操作都在内存中完成； 外排序：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行； 时间复杂度： 一个算法执行所耗费的时间。 空间复杂度：运行完一个程序所需内存的大小。 1.3 算法总结 图片名词解释： n: 数据规模 k: “桶” 的个数 In-place: 占用常数内存，不占用额外内存 Out-place: 占用额外内存 1.4 算法分类 1.5 比较和非比较的区别常见的快速排序、归并排序、堆排序、冒泡排序等属于比较排序。在排序的最终结果里，元素之间的次序依赖于它们之间的比较。每个数都必须和其他数进行比较，才能确定自己的位置。在冒泡排序之类的排序中，问题规模为 n，又因为需要比较 n 次，所以平均时间复杂度为 O(n²)。在归并排序、快速排序之类的排序中，问题规模通过分治法消减为 logN 次，所以时间复杂度平均 O(nlogn)。比较排序的优势是，适用于各种规模的数据，也不在乎数据的分布，都能进行排序。可以说，比较排序适用于一切需要排序的情况。 计数排序、基数排序、桶排序则属于非比较排序。非比较排序是通过确定每个元素之前，应该有多少个元素来排序。针对数组 arr，计算 arr[i] 之前有多少个元素，则唯一确定了 arr[i] 在排序后数组中的位置。非比较排序只要确定每个元素之前的已有的元素个数即可，所有一次遍历即可解决。算法时间复杂度 O(n)。非比较排序时间复杂度底，但由于非比较排序需要占用空间来确定唯一位置。所以对数据规模和数据分布有一定的要求。 2.冒泡排序（Bubble Sort）冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢 “浮” 到数列的顶端。 2.1 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤 1~3，直到排序完成。 2.2 动图演示 2.3 代码实现 /** * 冒泡排序 * 依次比较相邻两个元素，并调整位置 * 一趟排序后最大的数“冒泡”成功，到最右边 * 重复“冒泡” * @param array * @return */ public static int[] bubbleSort(int[] array) { if (array == null || array.length == 0) { return array; } // 外层：length-1次循环 for (int i = 0; i &lt; array.length - 1; i++) { for (int j = 0; j &lt; array.length - 1 - i; j++) { // 将较小的与大的交换位置 if (array[j + 1] &lt; array[j]) { // 采用临时变量法交换 int temp = array[j + 1]; array[j + 1] = array[j]; array[j] = temp; } } } return array; }2.4 算法分析冒泡排序是稳定的排序算法，最容易实现的排序, 最坏的情况是每次都需要交换, 共需遍历并交换将近n²/2次, 时间复杂度为O(n²). 最佳的情况是内循环遍历一次后发现排序是对的, 因此退出循环, 时间复杂度为O(n). 平均来讲, 时间复杂度为O(n²). 由于冒泡排序中只有缓存的temp变量需要内存空间, 因此空间复杂度为常量O(1)。 3.选择排序（Selection Sort）无论什么数据进去都是 O(n^2) 的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 选择排序 (Selection-sort) 是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 3.1 算法描述n 个记录的直接选择排序可经过 n-1 趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为 R[1..n]，有序区为空； 第 i 趟排序 (i=1,2,3…n-1) 开始时，当前有序区和无序区分别为 R[1..i-1]和 R(i..n）。该趟排序从当前无序区中 - 选出关键字最小的记录 R[k]，将它与无序区的第 1 个记录 R 交换，使 R[1..i]和 R[i+1..n)分别变为记录个数增加 1 个的新有序区和记录个数减少 1 个的新无序区； n-1 趟结束，数组有序化了。 3.2 动图演示 3.3 代码实现 /** * 选择排序 * 遍历数组，在未排序的队列，找到最小或最大的数，放在左边形成有序队列 * @param array * @return */ public static int[] selectionSort(int[] array) { if (array.length == 0) { return array; } for (int i = 0; i &lt; array.length - 1; i++) { int minIndex = i; for (int j = i + 1; j &lt; array.length; j++) { // 找到最小的数 if (array[j] &lt; array[minIndex]) // 将最小数的索引保存 minIndex = j; } // 将i位置元素与找到的最小数，交换位置 if (minIndex != i) { int temp = array[minIndex]; array[minIndex] = array[i]; array[i] = temp; } } return array; }3.4 算法分析最佳情况：O(n^2) 最差情况：O(n^2) 平均情况： O(n^2) 4.插入排序（Insertion Sort）插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，通常采用 in-place 排序（即只需用到 O(1) 的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 4.1 算法描述一般来说，插入排序都采用 in-place 在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤 3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤 2~5。 4.2 动图演示 4.3 代码实现提供两种写法，一种是移位法，一种是交换法。移位法是完全按照以上算法描述实，再插入过程中将有序序列中比待插入数字大的数据向后移动，由于移动时会覆盖待插入数据，所以需要额外的临时变量保存待插入数据，代码实现如下： /** * 插入排序-移位法 * 每次在右侧未排序队列，选中一个元素，在左侧有序队列中，找适合自己的位置，并插入 * @param array * @return */ public static int[] insertionSort(int[] arr) { if (arr == null || arr.length == 0) { return arr; } for (int i = 0; i &lt; arr.length; i++) { int j = i - 1; // temp即被选中，进行操作的元素 int temp = a[i]; // 如果比待插入数据大，就后移 while (j &gt;= 0 &amp;&amp; temp &lt; arr[j]) { // j位置元素大，后移，腾出位置 arr[j + 1] = arr[j]; // 一直向前，直到找到合适的位置 j--; } // 找到比待插入数据小的位置，将待插入数据插入 arr[j + 1] = temp; } return arr; }而交换法不需求额外的保存待插入数据，通过不停的向前交换带插入数据，类似冒泡法，直到找到比它小的值，也就是待插入数据找到了自己的位置: public static void insertionSort(int[] arr) { if (arr == null || arr.length == 0) { return; } for (int i = 1; i &lt; arr.length; i++) { int j = i - 1; while (j &gt;= 0 &amp;&amp; arr[j] &gt; arr[i]) { // 只要大就交换操作 arr[j + 1] = arr[j] + arr[j+1]; arr[j] = arr[j + 1] - arr[j]; arr[j + 1] = arr[j + 1] - arr[j]; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); } } }4.4 算法分析最佳情况： O(n) 最坏情况： O(n^2) 平均情况： O(n^2) 空间复杂度：O(1) 5.希尔排序（Shell Sort）希尔排序是希尔（Donald Shell）于 1959 年提出的一种排序算法。希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破 O(n2）的第一批算法之一。它与插入排序的不同之处在于，它会优先比较距离较远的元素; 直接插入排序是稳定的；而希尔排序是不稳定的。希尔排序又叫缩小增量排序。 希尔排序是把记录按一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至 1 时，整个文件恰被分成一组，算法便终止。 5.1 算法描述我们来看下希尔排序的基本步骤，在此我们选择增量 gap=length/2，缩小增量继续以 gap = gap/2 的方式，这种增量选择我们可以用一个序列来表示，{n/2,(n/2)/2…1}，称为增量序列。希尔排序的增量序列的选择与证明是个数学难题，我们选择的这个增量序列是比较常用的，也是希尔建议的增量，称为希尔增量，但其实这个增量序列不是最优的。此处我们做示例使用希尔增量。 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列 t1，t2，…，tk，其中 ti&gt;tj，tk=1； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 5.2 过程演示 5.3 代码实现 /** * 希尔排序 * * @param array * @return */ public static int[] ShellSort(int[] array) { int len = array.length; // 初始增量gap int temp, gap = len / 2; while (gap &gt; 0) { for (int i = gap; i &lt; len; i++) { // temp放分组的第二个数，即中位数gap右边元素 temp = array[i]; // preIndex：分组第一个数 = i与中位数gap的差 = i自增的次数 = 左侧下标 int preIndex = i - gap; //如果左侧元素大，交换位置 while (preIndex &gt;= 0 &amp;&amp; array[preIndex] &gt; temp) { array[preIndex + gap] = array[preIndex]; preIndex -= gap; } array[preIndex + gap] = temp; } gap /= 2; } return array; } 第二种写法： public static int[] ShellSort(int[] arr) { int gap = arr.length / 2; // 不断缩小gap，直到1为止 for (;gap &gt; 0; gap = gap/2) { // j用来控制分组内多个元素时，比较的次数 for (int j = 0; (j + gap) &lt; arr.length; j++) { // k左侧元素，k+gap=右侧元素。依次调整每个分组 for (int k = 0; (k + gap) &lt; arr.length; k++) { if (arr[k] &gt; arr[k+gap]) { // 交换操作 arr[k] = arr[k] + arr[k+gap]; arr[k+gap] = arr[k] - arr[k+gap]; arr[k] = arr[k] - arr[k+gap]; System.out.println(&quot; Sorting: &quot; + Arrays.toString(arr)); }5.4 算法分析*最佳情况： O(nlog n) 最坏情况： O(nlog n) 平均情况：O(nlog n) * 6.归并排序（Merge Sort）和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是 O(n log n）的时间复杂度。代价是需要额外的内存空间。 归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。归并排序是一种稳定的排序方法。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为 2 - 路归并。 6.1 算法描述 把长度为 n 的输入序列分成两个长度为 n/2 的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 6.2 动图演示 6.3 代码实现 /** * 归并排序 * 先分治，在合并 * @param array * @return */ public static int[] MergeSort(int[] array) { if (array.length &lt;= 1) { return array; } // &gt;&gt; 1 等价于 /2 int mid = array.length &gt;&gt; 1; int[] left = Arrays.copyOfRange(array, 0, mid); int[] right = Arrays.copyOfRange(array, mid, array.length); return merge(MergeSort(left), MergeSort(right)); } /** * 归并排序——将两段排序好的数组结合成一个排序数组 * * @param left * @param right * @return */ public static int[] merge(int[] left, int[] right) { int[] result = new int[left.length + right.length]; int i = 0, j = 0, k = 0; while (i &lt; left.length &amp;&amp; j &lt; right.length) { if (left[i] &lt;= right[j]) { result[k++] = left[i++]; }else { result[k++] = right[j++]; } } // left中的剩余元素移入结果数组 while (i &lt; left.length) { result[k++] = left[i++]； } // right中的剩余元素移入结果数组 while (j &lt; right.length) { result[k++] = right[j++]; } return result; }6.4 算法分析最佳情况：O(nlog n) 最差情况：O(nlog n) 平均情况： O(nlog n) 空间复杂度：O(n) 7.快速排序（Quick Sort）快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行快速排序，以达到整个序列有序。 7.1 算法描述快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 7.2 动图演示 7.3 代码实现 /** * 快速排序方法（左右指针法） * 从左至右依次选择元素作为基准，比较之后的元素 * 将小于基准的元素一起放到基准左边（无须排序）， * @param array * @param start * @param end * @return */ public static int[] QuickSort(int[] array, int start, int end) { if (array.length &lt; 1 || start &lt; 0 || end &gt;= array.length || start &gt; end) { return null; } int smallIndex = partition(array, start, end); if (smallIndex &gt; start) QuickSort(array, start, smallIndex - 1); if (smallIndex &lt; end) QuickSort(array, smallIndex + 1, end); return array; } /** * 快速排序算法——partition * @param array * @param start * @param end * @return */ public static int partition(int[] array, int start, int end) { int pivot = (int) (start + Math.random() * (end - start + 1)); int smallIndex = start - 1; swap(array, pivot, end); for (int i = start; i &lt;= end; i++) if (array[i] &lt;= array[end]) { smallIndex++; if (i &gt; smallIndex) swap(array, i, smallIndex); } return smallIndex; } /** * 交换数组内两个元素 * @param array * @param i * @param j */ public static void swap(int[] array, int i, int j) { int temp = array[i]; array[i] = array[j]; array[j] = temp; }7.4 算法分析最佳情况： O(nlog n) 最差情况： O(n^2) 平均情况： O(nlog n) 空间复杂度：O(1) 8.堆排序（Heap Sort）堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 8.1 算法描述 将初始待排序关键字序列 (R1,R2….Rn) 构建成大顶堆，此堆为初始的无序区；（一般升序采用大顶堆，降序采用小顶堆）； 将堆顶元素 R[1]与最后一个元素 R[n]交换，此时得到新的无序区 (R1,R2,……Rn-1) 和新的有序区(Rn，), 且满足 R[1,2…n-1]&lt;=R[n]； 恢复堆。由于交换后新的堆顶 R[1]可能违反堆的性质，因此需要对当前无序区 (R1,R2,……Rn-1) 调整为新堆，然后再次将 R[1]与无序区最后一个元素交换，得到新的无序区 (R1,R2….Rn-2) 和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为 n-1，则整个排序过程完成。 8.2 动图演示 8.3 代码实现注意：这里用到了完全二叉树的部分性质：详情见《数据结构二叉树知识点总结》 /** * 堆排序 * 构建大顶堆，交换堆顶元素与末尾元素，恢复大顶堆 * 每次将最大值“沉”到数组末端，升序 * @param arr */ public static void sort(int[] arr) { // 1.构建大顶堆 // 从最后一个非叶子节点开始 for (int i = arr.length / 2 - 1; i &gt;= 0; i--) { // 第一个非叶子节点i，从下至上、从左至右调整结构 adjustHeap(arr, i, arr.length); } // 2.交换堆顶与末尾 + 调整堆结构 // j就是末尾元素 for (int j = arr.length-1; j &gt; 0; j--) { // swap方法，交换操作 swap(arr, i, j); // adjustHeap方法，重新调整堆结构 adjustHeap(arr, i, j); } } /** * 调整大顶堆（仅是调整过程，建立在大顶堆已构建的基础上） * @param arr * @param i * @param length */ public static void adjustHeap(int[] arr, int i, int length) { // 以i节点为父节点，先将值放到temp保存 int temp = arr[i]; // 从i节点的左子节点开始，找出最大值，放到父节点位置 for (int k = i*2+1; k &lt; length; k = k*2+1) { // 如果左子节点小于右子节点，k指向右子节点 if (k+1 &lt; length &amp;&amp; arr[k] &lt; arr[k + 1]) { k++; } // 如果子节点大于父节点，将子节点值赋给父节点，最终使i为最大值 // 注意，这里k覆盖了i位置元素 if (arr[k] &gt; temp) { arr[i] = arr[k]; i = k; }else { break; } } // 此时才真正确定i的位置，将保存的值还给i arr[i] = temp; } /** * 交换元素 * @param arr * @param a * @param b */ public static void swap(int[] arr, int a, int b) { int temp = arr[a]; arr[a] = arr[b]; arr[b] = temp; }8.4 算法分析由于堆排序中初始化堆的过程比较次数较多, 因此它不太适用于小序列。同时由于多次任意下标相互交换位置, 相同元素之间原本相对的顺序被破坏了, 因此, 它是不稳定的排序。 最佳情况： O(nlogn) 最差情况： O(nlogn) 平均情况： O(nlogn) 空间复杂度：O(1) 9.计数排序（Counting Sort）计数排序的核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 计数排序 (Counting sort) 是一种稳定的排序算法。计数排序使用一个额外的数组 C，其中第 i 个元素是待排序数组 A 中值等于 i 的元素的个数。然后根据数组 C 来将 A 中的元素排到正确的位置。它只能对整数进行排序。 9.1 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为 i 的元素出现的次数，存入数组 C 的第 i 项； 对所有的计数累加（从 C 中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素 i 放在新数组的第 C(i) 项，每放一个元素就将 C(i) 减去 1。 9.2 动图演示 9.3 代码实现/** * 计数排序 * * @param array * @return */ public static int[] CountingSort(int[] array) { if (array.length == 0) return array; int bias, min = array[0], max = array[0]; for (int i = 1; i &lt; array.length; i++) { if (array[i] &gt; max) max = array[i]; if (array[i] &lt; min) min = array[i]; } bias = 0 - min; int[] bucket = new int[max - min + 1]; Arrays.fill(bucket, 0); for (int i = 0; i &lt; array.length; i++) { bucket[array[i] + bias]++; } int index = 0, i = 0; while (index &lt; array.length) { if (bucket[i] != 0) { array[index] = i - bias; bucket[i]--; index++; } else i++; } return array; }9.4 算法分析当输入的元素是 n 个 0 到 k 之间的整数时，它的运行时间是 O(n + k)。计数排序不是比较排序，排序的速度快于任何比较排序算法。由于用来计数的数组 C 的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上 1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。 最佳情况： O(n+k) 最差情况： O(n+k) 平均情况： O(n+k) 10.桶排序（Bucket Sort）桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。 桶排序 (Bucket sort) 的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排 10.1 算法描述 人为设置一个 BucketSize，作为每个桶所能放置多少个不同数值（例如当 BucketSize==5 时，该桶可以存放｛1,2,3,4,5｝这几种数字，但是容量不限，即可以存放 100 个 3）； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序，可以使用其它排序方法，也可以递归使用桶排序； 从不是空的桶里把排好序的数据拼接起来。 注意，如果递归使用桶排序为各个桶排序，则当桶数量为 1 时要手动减小 BucketSize 增加下一循环桶的数量，否则会陷入死循环，导致内存溢出。 10.2 图片演示 10.3 代码实现 /** * 桶排序 * * @param array * @param bucketSize * @return */ public static ArrayList&lt;Integer&gt; BucketSort(ArrayList&lt;Integer&gt; array, int bucketSize) { if (array == null || array.size() &lt; 2) return array; int max = array.get(0), min = array.get(0); // 找到最大值最小值 for (int i = 0; i &lt; array.size(); i++) { if (array.get(i) &gt; max) max = array.get(i); if (array.get(i) &lt; min) min = array.get(i); } int bucketCount = (max - min) / bucketSize + 1; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; bucketArr = new ArrayList&lt;&gt;(bucketCount); ArrayList&lt;Integer&gt; resultArr = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; bucketCount; i++) { bucketArr.add(new ArrayList&lt;Integer&gt;()); } for (int i = 0; i &lt; array.size(); i++) { bucketArr.get((array.get(i) - min) / bucketSize).add(array.get(i)); } for (int i = 0; i &lt; bucketCount; i++) { if (bucketSize == 1) { // 如果带排序数组中有重复数字时 感谢 @见风任然是风 朋友指出错误 for (int j = 0; j &lt; bucketArr.get(i).size(); j++) resultArr.add(bucketArr.get(i).get(j)); } else { if (bucketCount == 1) bucketSize--; ArrayList&lt;Integer&gt; temp = BucketSort(bucketArr.get(i), bucketSize); for (int j = 0; j &lt; temp.size(); j++) resultArr.add(temp.get(j)); } } return resultArr; }10.4 算法分析桶排序最好情况下使用线性时间 O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为 O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 *最佳情况： O(n+k) 最差情况： O(n+k) 平均情况： O(n^2) * 11.基数排序（Radix Sort）将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。 基数排序按照优先从高位或低位来排序有两种实现方案： MSD（Most significant digital） 从最左侧高位开始进行排序。先按k1排序分组, 同一组中记录, 关键码k1相等, 再对各组按k2排序分成子组, 之后, 对后面的关键码继续这样的排序分组, 直到按最次位关键码kd对各子组排序后. 再将各组连接起来, 便得到一个有序序列。MSD方式适用于位数多的序列。 LSD（Least significant digital） 从最右侧低位开始进行排序。先从kd开始排序，再对kd-1进行排序，依次重复，直到对k1排序后便得到一个有序序列。LSD方式适用于位数少的序列。 基数排序基于分别排序，分别收集，不改变相同元素之间的相对顺序，所以是稳定的。 下面以LSD为例。 11.1 算法描述 取得数组中的最大数，并取得位数； arr 为原始数组，从最低位开始取每个位组成 radix 数组； 对 radix 进行计数排序（利用计数排序适用于小范围数的特点）； 11.2 动图演示 11.3 代码实现 /** * 基数排序 * @param array * @return */ public static int[] RadixSort(int[] array) { if (array == null || array.length &lt; 2) { return array; } // 1.先算出最大数的位数； int max = array[0]; for (int i = 1; i &lt; array.length; i++) { if (a[i] &gt; max) { max = a[i]; } } int maxDigit = 0; while (max != 0) { max /= 10; maxDigit++; } int[][] buckets = new int[10][a.length]; int base = 10; //从低位到高位，对每一位遍历，将所有元素分配到桶中 for (int i = 0; i &lt; maxDigit; i++) { //存储各个桶中存储元素的数量 int[] bucketLen = new int[10]; //收集：将不同桶里数据挨个捞出来,为下一轮高位排序做准备,由于靠近桶底的元素排名靠前,因此从桶底先捞 for (int j = 0; j &lt; a.length; j++) { int whichBucket = (a[j] % base) / (base / 10); buckets[whichBucket][bucketLen[whichBucket]] = a[j]; bucketLen[whichBucket]++; } int k = 0; //收集：将不同桶里数据挨个捞出来,为下一轮高位排序做准备,由于靠近桶底的元素排名靠前,因此从桶底先捞 for (int l = 0; l &lt; buckets.length; l++) { for (int m =0; m &lt; bucketLen[l]; m++) { a[k++] = buckets[l][m]; } } System.out.println(&quot;Sorting: &quot; + Arrays.toString(a)); base *= 10; } return array; }11.4 算法分析最佳情况：O(d(n+r)) 最差情况：O(d(n+r)) 平均情况：O(d*(n+r)) 空间复杂度: O(n+r) 其中，d 为位数，r 为基数，n 为原数组个数。在基数排序中，因为没有比较操作，所以在复杂上，最好的情况与最坏的情况在时间上是一致的，均为 O(d*(n + r))。 基数排序更适合用于对时间, 字符串等这些整体权值未知的数据进行排序，适用于。 (1)数据范围较小，建议在小于1000 (2)每个数值都要大于等于0 基数排序 vs 计数排序 vs 桶排序 这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异： 基数排序：根据键值的每位数字来分配桶 计数排序：每个桶只存储单一键值 桶排序：每个桶存储一定范围的数值 参考资料：https://juejin.im/post/5b95da8a5188255c775d8124 https://www.cnblogs.com/guoyaohua/p/8600214.html https://www.cnblogs.com/Young111/p/11300929.html","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"排序算法","slug":"排序算法","permalink":"https://topone233.github.io/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"}]},{"title":"反射、注解和动态代理","slug":"反射、注解和动态代理","date":"2020-07-20T08:48:44.137Z","updated":"2020-09-20T05:49:03.102Z","comments":true,"path":"2020/07/20/反射、注解和动态代理/","link":"","permalink":"https://topone233.github.io/2020/07/20/%E5%8F%8D%E5%B0%84%E3%80%81%E6%B3%A8%E8%A7%A3%E5%92%8C%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/","excerpt":"","text":"原文地址 一、Java 反射机制及基本用法反射是指计算机程序在运行时访问、检测和修改它本身状态或行为的一种能力，是一种元编程语言特性，有很多语言都提供了对反射机制的支持，它使程序能够编写程序。Java 的反射机制使得 Java 能够动态的获取类的信息和调用对象的方法。 在 Java 中，Class（类类型）是反射编程的起点，代表运行时类型信息（RTTI，Run-Time Type Identification）。java.lang.reflect 包含了 Java 支持反射的主要组件，如 Constructor、Method 和 Field 等，分别表示类的构造器、方法和域，它们的关系如下图所示。 Constructor 和 Method 与 Field 的区别在于前者继承自抽象类 Executable，是可以在运行时动态调用的，而 Field 仅仅具备可访问的特性，且默认为不可访问。下面了解下它们的基本用法： 获取 Class 对象有三种方式，Class.forName 适合于已知类的全路径名，典型应用如加载 JDBC 驱动。对同一个类，不同方式获得的 Class 对象是相同的。 // 1. 采用Class.forName获取类的Class对象 Class clazz0 = Class.forName(&quot;com.yhthu.java.ClassTest&quot;); System.out.println(&quot;clazz0:&quot; + clazz0); // 2. 采用.class方法获取类的Class对象 Class clazz1 = ClassTest.class; System.out.println(&quot;clazz1:&quot; + clazz1); // 3. 采用getClass方法获取类的Class对象 ClassTest classTest = new ClassTest(); Class clazz2 = classTest.getClass(); System.out.println(&quot;clazz2:&quot; + clazz2); // 4. 判断Class对象是否相同 System.out.println(&quot;Class对象是否相同:&quot; + ((clazz0.equals(clazz1)) &amp;&amp; (clazz1.equals(clazz2)))); 注意：三种方式获取的 Class 对象相同的前提是使用了相同的类加载器，比如上述代码中默认采用应用程序类加载器（sun.misc.Launcher$AppClassLoader）。不同类加载器加载的同一个类，也会获取不同的 Class 对象： // 自定义类加载器 ClassLoader myLoader = new ClassLoader() { @Override public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException { try { String fileName = name.substring(name.lastIndexOf(&quot;.&quot;) + 1) + &quot;.class&quot;; InputStream is = getClass().getResourceAsStream(fileName); if (is == null) { return super.loadClass(name); } byte[] b = new byte[is.available()]; is.read(b); return defineClass(name, b, 0, b.length); } catch (IOException e) { throw new ClassNotFoundException(name); } } }; // 采用自定义类加载器加载 Class clazz3 = Class.forName(&quot;com.yhthu.java.ClassTest&quot;, true, myLoader); // clazz0与clazz3并不相同 System.out.println(&quot;Class对象是否相同:&quot; + clazz0.equals(clazz3)); 通过 Class 的 getDeclaredXxxx 和 getXxx 方法获取构造器、方法和域对象，两者的区别在于前者返回的是当前 Class 对象申明的构造器、方法和域，包含修饰符为 private 的；后者只返回修饰符为 public 的构造器、方法和域，但包含从基类中继承的。 // 返回申明为public的方法，包含从基类中继承的 for (Method method: String.class.getMethods()) { System.out.println(method.getName()); } // 返回当前类申明的所有方法，包含private的 for (Method method: String.class.getDeclaredMethods()) { System.out.println(method.getName()); } 通过 Class 的 newInstance 方法和 Constructor 的 newInstance 方法方法均可新建类型为 Class 的对象，通过 Method 的 invoke 方法可以在运行时动态调用该方法，通过 Field 的 set 方法可以在运行时动态改变域的值，但需要首先设置其为可访问（setAccessible）。 二、注解注解（Annotation）是 Java5 引入的一种代码辅助工具，它的核心作用是对类、方法、变量、参数和包进行标注，通过反射来访问这些标注信息，以此在运行时改变所注解对象的行为。Java 中的注解由内置注解和元注解组成。内置注解主要包括： @Override - 检查该方法是否是重载方法。如果发现其父类，或者是引用的接口中并没有该方法时，会报编译错误。 @Deprecated - 标记过时方法。如果使用该方法，会报编译警告。 @SuppressWarnings - 指示编译器去忽略注解中声明的警告。 @SafeVarargs - Java 7 开始支持，忽略任何使用参数为泛型变量的方法或构造函数调用产生的警告。 @FunctionalInterface - Java 8 开始支持，标识一个匿名函数或函数式接口。 这里，我们重点关注元注解，元注解位于 java.lang.annotation 包中，主要用于自定义注解。元注解包括： @Retention - 标识这个注解怎么保存，是只在代码中，还是编入 class 文件中，或者是在运行时可以通过反射访问，枚举类型分为别 SOURCE、CLASS 和 RUNTIME； @Documented - 标记这些注解是否包含在用户文档中。 @Target - 标记这个注解应该是哪种 Java 成员，枚举类型包括 TYPE、FIELD、METHOD、CONSTRUCTOR 等； @Inherited - 标记这个注解可以继承超类注解，即子类 Class 对象可使用 getAnnotations() 方法获取父类被 @Inherited 修饰的注解，这个注解只能用来申明类。 @Repeatable - Java 8 开始支持，标识某注解可以在同一个声明上使用多次。 自定义元注解需重点关注两点：1）注解的数据类型；2）反射获取注解的方法。首先，注解中的方法并不支持所有的数据类型，仅支持八种基本数据类型、String、Class、enum、Annotation 和它们的数组。比如以下代码会产生编译时错误： @Documented @Inherited @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) public @interface AnnotationTest { // 1. 注解数据类型不能是Object；2. 默认值不能为null Object value() default null; // 支持的定义方式 String value() default &quot;&quot;; }其次，上节中提到的反射相关类（Class、Constructor、Method 和 Field）和 Package 均实现了 AnnotatedElement 接口，该接口定义了访问反射信息的方法，主要如下： // 获取指定注解类型 getAnnotation(Class&lt;T&gt;):T; // 获取所有注解，包括从父类继承的 getAnnotations():Annotation[]; // 获取指定注解类型，不包括从父类继承的 getDeclaredAnnotation(Class&lt;T&gt;):T // 获取所有注解，不包括从父类继承的 getDeclaredAnnotations():Annotation[]; // 判断是否存在指定注解 isAnnotationPresent(Class&lt;? extends Annotation&gt;:boolean当使用上例中的 AnnotationTest 标注某个类后，便可在运行时通过该类的反射方法访问注解信息了。 @AnnotationTest(&quot;yhthu&quot;) public class AnnotationReflection { public static void main(String[] args) { AnnotationReflection ar = new AnnotationReflection(); Class clazz = ar.getClass(); // 判断是否存在指定注解 if (clazz.isAnnotationPresent(AnnotationTest.class)) { // 获取指定注解类型 Annotation annotation = clazz.getAnnotation(AnnotationTest.class); // 获取该注解的值 System.out.println(((AnnotationTest) annotation).value()); } } } 当自定义注解只有一个方法 value() 时，使用注解可只写值，例如：@AnnotationTest(“yhthu”) 三、动态代理参考上一篇：动态代理 代理是一种结构型设计模式，当无法或不想直接访问某个对象，或者访问某个对象比较复杂的时候，可以通过一个代理对象来间接访问，代理对象向客户端提供和真实对象同样的接口功能。经典设计模式中，代理模式有四种角色： Subject 抽象主题类——申明代理对象和真实对象共同的接口方法； RealSubject 真实主题类——实现了 Subject 接口，真实执行业务逻辑的地方； ProxySubject 代理类——实现了 Subject 接口，持有对 RealSubject 的引用，在实现的接口方法中调用 RealSubject 中相应的方法执行； Cliect 客户端类——使用代理对象的类。 在实现上，代理模式分为静态代理和动态代理，静态代理的代理类二进制文件是在编译时生成的，而动态代理的代理类二进制文件是在运行时生成并加载到虚拟机环境的。JDK 提供了对动态代理接口的支持，开源的动态代理库（Cglib、Javassist 和 Byte Buddy）提供了对接口和类的代理支持，本节将简单比较 JDK 和 Cglib 实现动态代理的异同，后续章节会对 Java 字节码编程做详细分析。 3.1 JDK 动态代理接口JDK 实现动态代理是通过 Proxy 类的 newProxyInstance 方法实现的，该方法的三个入参分别表示： public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) ClassLoader loader，定义代理生成的类的加载器，可以自定义类加载器，也可以复用当前 Class 的类加载器； Class&lt;?&gt;[] interfaces，定义代理对象需要实现的接口； InvocationHandler h，定义代理对象调用方法的处理，其 invoke 方法中的 Object proxy 表示生成的代理对象，Method 表示代理方法， Object[] 表示方法的参数。 通常的使用方法如下： private Object getProxy() { return Proxy.newProxyInstance(JDKProxyTest.class.getClassLoader(), new Class&lt;?&gt;[]{Subject.class}, new MyInvocationHandler(new RealSubject())); } private static class MyInvocationHandler implements InvocationHandler { private Object realSubject; public MyInvocationHandler(Object realSubject) { this.realSubject = realSubject; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(&quot;Some thing before method invoke&quot;); Object result = method.invoke(realSubject, args); System.out.println(&quot;Some thing after method invoke&quot;); return result; } }类加载器采用当前类的加载器，默认为应用程序类加载器（sun.misc.Launcher$AppClassLoader）；接口数组以 Subject.class 为例，调用方法处理类 MyInvocationHandler 实现 InvocationHandler 接口，并在构造器中传入 Subject 的真正的业务功能服务类 RealSubject，在执行 invoke 方法时，可以在实际方法调用前后织入自定义的处理逻辑，这也就是 AOP（面向切面编程）的原理。关于 JDK 动态代理，有两个问题需要清楚： Proxy.newProxyInstance 的代理类是如何生成的？Proxy.newProxyInstance 生成代理类的核心分成两步： // 1. 获取代理类的Class对象 Class&lt;?&gt; cl = getProxyClass0(loader, intfs); // 2. 利用Class获取Constructor，通过反射生成对象 cons.newInstance(new Object[]{h});与反射获取 Class 对象时搜索 classpath 路径的. class 文件不同的是，这里的 Class 对象完全是 “无中生有” 的。getProxyClass0 根据类加载器和接口集合返回了 Class 对象，这里采用了缓存的处理。 // 缓存(key, sub-key) -&gt; value，其中key为类加载器，sub-key为代理的接口，value为Class对象 private static final WeakCache&lt;ClassLoader, Class&lt;?&gt;[], Class&lt;?&gt;&gt; proxyClassCache = new WeakCache&lt;&gt;(new KeyFactory(), new ProxyClassFactory()); // 如果实现了代理接口的类已存在就返回缓存对象，否则就通过ProxyClassFactory生成 private static Class&lt;?&gt; getProxyClass0(ClassLoader loader, Class&lt;?&gt;... interfaces) { if (interfaces.length &gt; 65535) { throw new IllegalArgumentException(&quot;interface limit exceeded&quot;); } return proxyClassCache.get(loader, interfaces); }如果实现了代理接口的类已存在就返回缓存对象，否则就通过 ProxyClassFactory 生成。ProxyClassFactory 又是通过下面的代码生成 Class 对象的。 // 生成代理类字节码文件 byte[] proxyClassFile = ProxyGenerator.generateProxyClass(proxyName, interfaces, accessFlags); try { // defineClass0为native方法，生成Class对象 return defineClass0(loader, proxyName, proxyClassFile, 0, proxyClassFile.length); } catch (ClassFormatError e) { throw new IllegalArgumentException(e.toString()); }generateProxyClass 方法是用来生成字节码文件的，根据生成的字节码文件，再在 native 层生成 Class 对象。 InvocationHandler 的 invoke 方法是怎样调用的？回答这个问题得先看下上面生成的 Class 对象究竟是什么样的，将 ProxyGenerator 生成的字节码保存成文件，然后反编译打开（IDEA 直接打开），可见生成的 Proxy.class 主要包含 equals、toString、hashCode 和代理接口的 request 方法实现。 public final class $Proxy extends Proxy implements Subject { // m1 = Object的equals方法 private static Method m1; // m2 = Object的toString方法 private static Method m2; // Subject的request方法 private static Method m3; // Object的hashCode方法 private static Method m0; // 省略m1/m2/m0，此处只列出request方法实现 public final void request() throws { try { super.h.invoke(this, m3, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } }由于生成的代理类继承自 Proxy，super.h 即是 Prxoy 的 InvocationHandler，即代理类的 request 方法直接调用了 InvocationHandler 的实现，这就回答了 InvocationHandler 的 invoke 方法是如何被调用的了。 3.2 Cglib 动态代理接口和类Cglib 的动态代理是通过 Enhancer 类实现的，其 create 方法生成动态代理的对象，有五个重载方法： create():Object create(Class, Callback):Object create(Class, Class[], Callback):Object create(Class, Class[], CallbackFilter, Callback):Object create(Class[], Object):Object常用的是第二个和第三个方法，分别用于动态代理类和动态代理接口，其使用方法如下： private Object getProxy() { // 1. 动态代理类 return Enhancer.create(RealSubject.class, new MyMethodInterceptor()); // 2. 动态代理接口 return Enhancer.create(Object.class, new Class&lt;?&gt;[]{Subject.class}, new MyMethodInterceptor()); } private static class MyMethodInterceptor implements MethodInterceptor { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(&quot;Some thing before method invoke&quot;); Object result = proxy.invokeSuper(obj, args); System.out.println(&quot;Some thing after method invoke&quot;); return result; } }从上小节可知，JDK 只能代理接口，代理生成的类实现了接口的方法；而 Cglib 是通过继承被代理的类、重写其方法来实现的，如：create 方法入参的第一个参数就是被代理类的类型。当然，Cglib 也能代理接口，比如 getProxy() 方法中的第二种方式。","categories":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"}]},{"title":"HashMap 剖析 (基于 jdk1.8)","slug":"HashMap 剖析 (基于 jdk1.8)","date":"2020-07-10T09:07:59.406Z","updated":"2020-09-17T02:26:03.542Z","comments":true,"path":"2020/07/10/HashMap 剖析 (基于 jdk1.8)/","link":"","permalink":"https://topone233.github.io/2020/07/10/HashMap%20%E5%89%96%E6%9E%90%20(%E5%9F%BA%E4%BA%8E%20jdk1.8)/","excerpt":"","text":"原文地址 www.cnblogs.com 本文的源码是基于 JDK1.8 版本，在学习 HashMap 之前，先了解数组和链表的知识。 数组数组具有遍历快，增删慢的特点。数组在堆中是一块连续的存储空间，遍历时数组的首地址是知道的（首地址 = 首地址 + 元素字节数 * 下标），所以遍历快（数组遍历的时间复杂度为 O(1) ）；增删慢是因为，当在中间插入或删除元素时，会造成该元素后面所有元素地址的改变，所以增删慢（增删的时间复杂度为 O(n) ）。 链表链表具有增删快，遍历慢的特点。链表中各元素的内存空间是不连续的，一个节点至少包含节点数据与后继节点的引用，所以在插入删除时，只需修改该位置的前驱节点与后继节点即可，链表在插入删除时的时间复杂度为 O(1)。但是在遍历时，get(n) 元素时，需要从第一个开始，依次拿到后面元素的地址，进行遍历，直到遍历到第 n 个元素（时间复杂度为 O(n) ），所以效率极低。 HashMapHash 表是一个数组 + 链表的结构，这种结构能够保证在遍历与增删的过程中，如果不产生 hash 碰撞，仅需一次定位就可完成，时间复杂度能保证在 O(1)。 在 jdk1.7 中，只是单纯的数组 + 链表的结构，但是如果散列表中的 hash 碰撞过多时，会造成效率的降低，所以在 JKD1.8 中对这种情况进行了控制，当一个 hash 值上的链表长度大于 8 时，该节点上的数据就不再以链表进行存储，而是转成了一个红黑树。 红黑树: static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; { TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; } hash 碰撞hash 是指，两个元素通过 hash 函数计算出的值是一样的，是同一个存储地址。当后面的元素要插入到这个地址时，发现已经被占用了，这时候就产生了 hash 冲突 hash 冲突的解决方法开放定址法 (查询产生冲突的地址的下一个地址是否被占用，直到寻找到空的地址)，再散列法，链地址法等。hashmap 采用的就是链地址法，jdk1.7 中，当冲突时，在冲突的地址上生成一个链表，将冲突的元素的 key，通过 equals 进行比较，相同即覆盖，不同则添加到链表上，此时如果链表过长，效率就会大大降低，查找和添加操作的时间复杂度都为 O(n)；但是在 jdk1.8 中如果链表长度大于 8，链表就会转化为红黑树，下图就是 1.8 版本的（图片来源 https://segmentfault.com/a/1190000012926722），时间复杂度也降为了 O(logn)，性能得到了很大的优化。 HashMap 的底层实现首先，hashMap 的主干是一个 Node 数组（jdk1.7 及之前为 Entry 数组）每一个 Node 包含一个 key 与 value 的键值对，与一个 next 指向下一个 node，hashMap 由多个 Node 对象组成。 Node 是 HhaspMap 中的一个静态内部类 ： static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + &quot;=&quot; + value; } //hashCode等其他代码 } 再看下 hashMap 中几个重要的字段： //默认初始容量为16，0000 0001 左移4位 0001 0000为16，主干数组的初始容量为16，而且这个数组 //必须是2的倍数(后面说为什么是2的倍数) static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 //最大容量为int的最大值除2 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //默认加载因子为0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; //阈值，如果主干数组上的链表的长度大于8，链表转化为红黑树 static final int TREEIFY_THRESHOLD = 8; //hash表扩容后，如果发现某一个红黑树的长度小于6，则会重新退化为链表 static final int UNTREEIFY_THRESHOLD = 6; //当hashmap容量大于64时，链表才能转成红黑树 static final int MIN_TREEIFY_CAPACITY = 64; //临界值=主干数组容量*负载因子 int threshold； HashMap 的构造方法//initialCapacity为初始容量，loadFactor为负载因子 public HashMap(int initialCapacity, float loadFactor) { //初始容量小于0，抛出非法数据异常 if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); //初始容量最大为MAXIMUM_CAPACITY if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //负载因子必须大于0，并且是合法数字 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; //将初始容量转成2次幂 this.threshold = tableSizeFor(initialCapacity); } //tableSizeFor的作用就是，如果传入A，当A大于0，小于定义的最大容量时， //如果A是2次幂则返回A，否则将A转化为一个比A大且差距最小的2次幂。 //例如传入7返回8，传入8返回8，传入9返回16 static final int tableSizeFor(int cap) { int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } //调用上面的构造方法，自定义初始容量，负载因子为默认的0.75 public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } //默认构造方法，负载因子为0.75，初始容量为DEFAULT_INITIAL_CAPACITY=16，初始容量在第一次put时才会初始化 public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } //传入一个MAP集合的构造方法 public HashMap(Map&lt;? extends K, ? extends V&gt; m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); } HashMap 的 put() 方法put 方法的源码分析是本篇的一个重点，因为通过该方法我们可以窥探到 HashMap 在内部是如何进行数据存储的，所谓的数组 + 链表 + 红黑树的存储结构是如何形成的，又是在何种情况下将链表转换成红黑树来优化性能的。带着一系列的疑问，我们看这个 put 方法： public V put(K key, V value) { return putVal(hash(key), key, value, false, true); } 也就是 put 方法调用了 putVal 方法，其中传入一个参数位 hash(key)，我们首先来看看 hash() 这个方法。 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } 此处如果传入的 int 类型的值：①向一个 Object 类型赋值一个 int 的值时，会将 int 值自动封箱为 Integer。②integer 类型的 hashcode 都是他自身的值，即 h=key；h &gt;&gt;&gt; 16 为无符号右移 16 位，低位挤走，高位补 0；^ 为按位异或，即转成二进制后，相异为 1，相同为 0，由此可发现，当传入的值小于 2 的 16 次方 - 1 时，调用这个方法返回的值，都是自身的值。然后再执行 putVal 方法： //onlyIfAbsent是true的话，不要改变现有的值 //evict为true的话，表处于创建模式 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果主干上的table为空，长度为0，调用resize方法，调整table的长度（resize方法在下图中） if ((tab = table) == null || (n = tab.length) == 0) /* 这里调用resize，其实就是第一次put时，对数组进行初始化。 如果是默认构造方法会执行resize中的这几句话： newCap = DEFAULT_INITIAL_CAPACITY; 新的容量等于默认值16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); threshold = newThr; 临界值等于16*0.75 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; 将新的node数组赋值给table，然后return newTab 如果是自定义的构造方法则会执行resize中的： int oldThr = threshold; newCap = oldThr; 新的容量等于threshold，这里的threshold都是2的倍数，原因在 于传入的数都经过tableSizeFor方法，返回了一个新值，上面解释过 float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); threshold = newThr; 新的临界值等于 (int)(新的容量*负载因子) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; return newTab; */ n = (tab = resize()).length; //将调用resize后构造的数组的长度赋值给n if ((p = tab[i = (n - 1) &amp; hash]) == null) //将数组长度与计算得到的hash值比较 tab[i] = newNode(hash, key, value, null);//位置为空，将i位置上赋值一个node对象 else { //位置不为空 Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; // 如果这个位置的old节点与new节点的key完全相同 ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 则e=p else if (p instanceof TreeNode) // 如果p已经是树节点的一个实例，既这里已经是树了 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else { //p与新节点既不完全相同，p也不是treenode的实例 for (int binCount = 0; ; ++binCount) { //一个死循环 if ((e = p.next) == null) { //e=p.next,如果p的next指向为null p.next = newNode(hash, key, value, null); //指向一个新的节点 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // 如果链表长度大于等于8 treeifyBin(tab, hash); //将链表转为红黑树 break; } if (e.hash == hash &amp;&amp; //如果遍历过程中链表中的元素与新添加的元素完全相同，则跳出循环 ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; //将p中的next赋值给p,即将链表中的下一个node赋值给p， //继续循环遍历链表中的元素 } } if (e != null) { //这个判断中代码作用为：如果添加的元素产生了hash冲突，那么调用 //put方法时，会将他在链表中他的上一个元素的值返回 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) //判断条件成立的话，将oldvalue替换 //为newvalue，返回oldvalue；不成立则不替换，然后返回oldvalue e.value = value; afterNodeAccess(e); //这个方法在后面说 return oldValue; } } ++modCount; //记录修改次数 if (++size &gt; threshold) //如果元素数量大于临界值，则进行扩容 resize(); //下面说 afterNodeInsertion(evict); return null; } 在 Java 8 中，如果一个桶中的元素个数超过 TREEIFY_THRESHOLD(默认是 8)，就使用红黑树来替换链表，从而提高速度。上诉代码这个替换的方法叫 treeifyBin() 即树形化。 看一下 treeifyBin() 的源码: //将桶内所有的 链表节点 替换成 红黑树节点 final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) { int n, index; Node&lt;K,V&gt; e; //如果当前哈希表为空，或者哈希表中元素的个数小于 进行树形化的阈值(默认为 64)，就去新建/扩容 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) { //如果哈希表中的元素个数超过了 树形化阈值，进行树形化 // e 是哈希表中指定位置桶里的链表节点，从第一个开始 TreeNode&lt;K,V&gt; hd = null, tl = null; //红黑树的头、尾节点 do { //新建一个树形节点，内容和当前链表节点 e 一致 TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) //确定树头节点 hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); //让桶的第一个元素指向新建的红黑树头结点，以后这个桶里的元素就是红黑树而不是链表了 if ((tab[index] = hd) != null) hd.treeify(tab); } } TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) { return new TreeNode&lt;&gt;(p.hash, p.key, p.value, next); } 注释已经很详细了，咱们说一下这个初始化的问题 //如果 table 还未被初始化，那么初始化它 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; resize() 扩容机制，单元素如何散列到新的数组中，链表中的元素如何散列到新的数组中，红黑树中的元素如何散列到新的数组中？ //上图中说了默认构造方法与自定义构造方法第一次执行resize的过程，这里再说一下扩容的过程 final Node&lt;K,V&gt;[] resize() { Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) { //扩容肯定执行这个分支 if (oldCap &gt;= MAXIMUM_CAPACITY) { //当容量超过最大值时，临界值设置为int最大值 threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //扩容容量为2倍，临界值为2倍 newThr = oldThr &lt;&lt; 1; } else if (oldThr &gt; 0) // 不执行 newCap = oldThr; else { // 不执行 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } if (newThr == 0) { // 不执行 float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; //将新的临界值赋值赋值给threshold @SuppressWarnings({&quot;rawtypes&quot;,&quot;unchecked&quot;}) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //新的数组赋值给table //扩容后，重新计算元素新的位置 if (oldTab != null) { //原数组 for (int j = 0; j &lt; oldCap; ++j) { //通过原容量遍历原数组 Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) { //判断node是否为空，将j位置上的节点 //保存到e,然后将oldTab置为空，这里为什么要把他置为空呢，置为空有什么好处吗？？ //难道是吧oldTab变为一个空数组，便于垃圾回收？？ 这里不是很清楚 oldTab[j] = null; if (e.next == null) //判断node上是否有链表 newTab[e.hash &amp; (newCap - 1)] = e; //无链表，确定元素存放位置， //扩容前的元素地址为 (oldCap - 1) &amp; e.hash ,所以这里的新的地址只有两种可能，一是地址不变， //二是变为 老位置+oldCap else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else { // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; /* 这里如果判断成立，那么该元素的地址在新的数组中就不会改变。因为oldCap的最高位的1，在e.hash对应的位上为0，所以扩容后得到的地址是一样的，位置不会改变 ，在后面的代码的执行中会放到loHead中去，最后赋值给newTab[j]； 如果判断不成立，那么该元素的地址变为 原下标位置+oldCap，也就是lodCap最高位的1，在e.hash对应的位置上也为1，所以扩容后的地址改变了，在后面的代码中会放到hiHead中，最后赋值给newTab[j + oldCap] 举个栗子来说一下上面的两种情况： 设：oldCap=16 二进制为：0001 0000 oldCap-1=15 二进制为：0000 1111 e1.hash=10 二进制为：0000 1010 e2.hash=26 二进制为：0101 1010 e1在扩容前的位置为：e1.hash &amp; oldCap-1 结果为：0000 1010 e2在扩容前的位置为：e2.hash &amp; oldCap-1 结果为：0000 1010 结果相同，所以e1和e2在扩容前在同一个链表上，这是扩容之前的状态。 现在扩容后，需要重新计算元素的位置，在扩容前的链表中计算地址的方式为e.hash &amp; oldCap-1 那么在扩容后应该也这么计算呀，扩容后的容量为oldCap*2=32 0010 0000 newCap=32，新的计算 方式应该为 e1.hash &amp; newCap-1 即：0000 1010 &amp; 0001 1111 结果为0000 1010与扩容前的位置完全一样。 e2.hash &amp; newCap-1 即：0101 1010 &amp; 0001 1111 结果为0001 1010,为扩容前位置+oldCap。 而这里却没有e.hash &amp; newCap-1 而是 e.hash &amp; oldCap，其实这两个是等效的，都是判断倒数第五位 是0，还是1。如果是0，则位置不变，是1则位置改变为扩容前位置+oldCap。 再来分析下loTail loHead这两个的执行过程（假设(e.hash &amp; oldCap) == 0成立）： 第一次执行： e指向oldTab[j]所指向的node对象，即e指向该位置上链表的第一个元素 loTail为空,所以loHead指向与e相同的node对象，然后loTail也指向了同一个node对象。 最后，在判断条件e指向next，就是指向oldTab链表中的第二个元素 第二次执行： lotail不为null，所以lotail.next指向e，这里其实是lotail指向的node对象的next指向e， 也可以说是，loHead的next指向了e，就是指向了oldTab链表中第二个元素。此时loHead指向 的node变成了一个长度为2的链表。然后lotail=e也就是指向了链表中第二个元素的地址。 第三次执行： 与第二次执行类似，loHead上的链表长度变为3，又增加了一个node，loTail指向新增的node ...... hiTail与hiHead的执行过程与以上相同，这里就不再做解释了。 由此可以看出，loHead是用来保存新链表上的头元素的，loTail是用来保存尾元素的，直到遍 历完链表。 这是(e.hash &amp; oldCap) == 0成立的时候。 (e.hash &amp; oldCap) == 0不成立的情况也相同，其实就是把oldCap遍历成两个新的链表， 通过loHead和hiHead来保存链表的头结点，然后将两个头结点放到newTab[j]与 newTab[j+oldCap]上面去 */ do { next = e.next; if ((e.hash &amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { loTail.next = null; //尾节点的next设置为空 newTab[j] = loHead; } if (hiTail != null) { hiTail.next = null; //尾节点的next设置为空 newTab[j + oldCap] = hiHead; } } } } } return newTab; } 有关 JDK1.7 扩容出现的死循环的问题: /** * Transfers all entries from current table to newTable. */ void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry&lt;K,V&gt; e = src[j]; if (e != null) { src[j] = null; do { // B线程执行到这里之后就暂停了 Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } } } 并发下的 Rehash 1）假设我们有两个线程。我用红色和浅蓝色标注了一下。我们再回头看一下我们的 transfer 代码中的这个细节： do { Entry&lt;K,V&gt; next = e.next; // &lt;--假设线程一执行到这里就被调度挂起了 int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); 而我们的线程二执行完成了。于是我们有下面的这个样子。 注意，因为 Thread1 的 e 指向了 key(3)，而 next 指向了 key(7)，其在线程二 rehash 后，指向了线程二重组后的链表。我们可以看到链表的顺序被反转后。 2）线程一被调度回来执行。 先是执行 newTalbe[i] = e; 然后是 e = next，导致了 e 指向了 key(7)， 而下一次循环的 next = e.next 导致了 next 指向了 key(3) 3）一切安好。 线程一接着工作。把 key(7) 摘下来，放到 newTable[i] 的第一个，然后把 e 和 next 往下移。 4）环形链接出现。 e.next = newTable[i] 导致 key(3).next 指向了 key(7) 注意：此时的 key(7).next 已经指向了 key(3)， 环形链表就这样出现了。 于是，当我们的线程一调用到，HashTable.get(11) 时，悲剧就出现了——Infinite Loop。 因为 HashMap 本来就不支持并发。要并发就用 ConcurrentHashmap HashMap 的 get() 方法public V get(Object key) { Node&lt;K,V&gt; e; //直接调用了getNode() return (e = getNode(hash(key), key)) == null ? null : e.value; } final Node&lt;K,V&gt; getNode(int hash, Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //先判断数组是否为空，长度是否大于0，那个node节点是否存在 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { //如果找到，直接返回 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { //如果是红黑树，去红黑树找 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //链表找 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } 这里关于first = tab[(n - 1) &amp; hash] 这里通过(n - 1)&amp; hash即可算出桶的在桶数组中的位置，可能有的朋友不太明白这里为什么这么做，这里简单解释一下。HashMap 中桶数组的大小 length 总是 2 的幂，此时，(n - 1) &amp; hash 等价于对 length 取余。但取余的计算效率没有位运算高，所以(n - 1) &amp; hash也是一个小的优化。举个例子说明一下吧，假设 hash = 185，n = 16。计算过程示意图如下 在上面源码中，除了查找相关逻辑，还有一个计算 hash 的方法。这个方法源码如下： /** * 计算键的 hash 值 */ static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } 看这个方法的逻辑好像是通过位运算重新计算 hash，那么这里为什么要这样做呢？为什么不直接用键的 hashCode 方法产生的 hash 呢？大家先可以思考一下，我把答案写在下面。 这样做有两个好处，我来简单解释一下。我们再看一下上面求余的计算图，图中的 hash 是由键的 hashCode 产生。计算余数时，由于 n 比较小，hash 只有低 4 位参与了计算，高位的计算可以认为是无效的。这样导致了计算结果只与低位信息有关，高位数据没发挥作用。为了处理这个缺陷，我们可以上图中的 hash 高 4 位数据与低 4 位数据进行异或运算，即 hash ^ (hash &gt;&gt;&gt; 4)。通过这种方式，让高位数据与低位数据进行异或，以此加大低位信息的随机性，变相的让高位数据参与到计算中。此时的计算过程如下： 在 Java 中，hashCode 方法产生的 hash 是 int 类型，32 位宽。前 16 位为高位，后 16 位为低位，所以要右移 16 位。 上面所说的是重新计算 hash 的一个好处，除此之外，重新计算 hash 的另一个好处是可以增加 hash 的复杂度。当我们覆写 hashCode 方法时，可能会写出分布性不佳的 hashCode 方法，进而导致 hash 的冲突率比较高。通过移位和异或运算，可以让 hash 变得更复杂，进而影响 hash 的分布性。这也就是为什么 HashMap 不直接使用键对象原始 hash 的原因了。 由于个人能力问题, 先学习这些, 数据结构这个大山, 我一定要刨平它。 基于 jdk1.7 版本的 HashMap https://www.jianshu.com/p/dde9b12343c1 参考博客: https://www.cnblogs.com/wenbochang/archive/2018/02/22/8458756.html https://segmentfault.com/a/1190000012926722 https://blog.csdn.net/pange1991/article/details/82377980","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"HashMap","slug":"hashmap","permalink":"https://topone233.github.io/tags/hashmap/"}]},{"title":"HashMap 面试必问总结","slug":"HashMap 面试必问总结","date":"2020-07-10T09:07:59.400Z","updated":"2020-09-17T02:14:32.419Z","comments":true,"path":"2020/07/10/HashMap 面试必问总结/","link":"","permalink":"https://topone233.github.io/2020/07/10/HashMap%20%E9%9D%A2%E8%AF%95%E5%BF%85%E9%97%AE%E6%80%BB%E7%BB%93/","excerpt":"","text":"原文地址 www.cnblogs.com 1.HashMap 的数据结构？​ 哈希表结构（链表散列：数组 + 链表）实现，结合数组和链表的优点。当链表长度超过 8 时，链表转换为红黑树。transient Node&lt;K,V&gt;[] table; 2.HashMap 的工作原理？ HashMap 底层是 hash 数组和单向链表实现，数组中的每个元素都是链表，由 Node 内部类（实现 Map.Entry&lt;K,V&gt; 接口）实现，HashMap 通过 put &amp; get 方法存储和获取。 存储对象时，将 K/V 键值传给 put() 方法： ​ ①、调用 hash(K) 方法计算 K 的 hash 值，然后结合数组长度，计算得数组下标； ​ ②、调整数组大小（当容器中的元素个数大于 capacity * loadfactor 时，容器会进行扩容 resize 为 2n）； ​ ③、如果 K 的 hash 值在 HashMap 中不存在，则执行插入，若存在，则发生碰撞； 如果 K 的 hash 值在 HashMap 中存在，且它们两者 equals 返回 true，则更新键值对； 如果 K 的 hash 值在 HashMap 中存在，且它们两者 equals 返回 false，则插入链表的尾部（尾插法）或者红黑树中。 （JDK 1.7 之前使用头插法、JDK 1.8 使用尾插法）（注意：当碰撞导致链表大于 TREEIFY_THRESHOLD = 8 时，就把链表转换成红黑树） 获取对象时，将 K 传给 get() 方法： ①、调用 hash(K) 方法（计算 K 的 hash 值）从而获取该键值所在链表的数组下标； ②、顺序遍历链表，equals() 方法查找相同 Node 链表中 K 值对应的 V 值。 hashCode 是定位的，找到存储位置；equals 是定性的，比较两者是否相等。 3. 当两个对象的 hashCode 相同会发生什么？ 因为 hashCode 相同，不一定就是相等的（equals 方法比较），如果两个对象所在数组的下标相同，”碰撞” 就此发生。又因为 HashMap 使用链表存储对象，这个 Node 会存储到链表中。 4. 你知道 hash 的实现吗？为什么要这样实现？ JDK 1.8 中，是通过 hashCode() 的高 16 位异或低 16 位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度，功效和质量来考虑的，减少系统的开销，也不会造成因为高位没有参与下标的计算，从而引起的碰撞。 5. 为什么要用异或运算符？​ 保证了对象的 hashCode 的 32 位值只要有一位发生改变，整个 hash() 返回值就会改变。尽可能的减少碰撞。 6. HashMap 的 table 的容量如何确定？loadFactor 是什么？ 该容量如何变化？这种变化会带来什么问题？​ ①、table 数组大小是由capacity这个参数确定的，默认是 16，也可以构造时传入，最大限制是 1&lt;&lt;30； ​ ②、loadFactor 是装载因子，主要目的是用来确认 table 数组是否需要动态扩展，默认值是 0.75，比如 table 数组大小为 16，装载 因子为 0.75 时，threshold 就是 12，当 table 的实际大小超过 12 时，table 就需要动态扩容； ​ ③、扩容时，调用 resize() 方法，将 table 长度变为原来的两倍（注意是 table 长度，而不是 threshold） ​ ④、如果数据很大的情况下，扩展时将会带来性能的损失，在性能要求很高的地方，这种损失很可能很致命。 7.HashMap 中 put 方法的过程？ 答：“调用哈希函数获取 Key 对应的 hash 值，再计算其数组下标； 如果没有出现哈希冲突，则直接放入数组；如果出现哈希冲突，则以链表的方式放在链表后面； 如果链表长度超过阀值 (TREEIFY THRESHOLD==8)，就把链表转成红黑树，链表长度低于 6，就把红黑树转回链表; 如果结点的 key 已经存在，则替换其 value 即可； 如果集合中的键值对大于 12，调用 resize 方法进行数组扩容。” 8. 数组扩容的过程？ 创建一个新的数组，其容量为旧数组的两倍，并重新计算旧数组中结点的存储位置。结点在新数组中的位置只有两种，原下标位置或原下标 + 旧数组的大小。 9. 拉链法导致的链表过深问题为什么不用二叉查找树代替，而选择红黑树？为什么不一直使用红黑树？​ 之所以选择红黑树是为了解决二叉查找树的缺陷，二叉查找树在特殊情况下会变成一条线性结构（这就跟原来使用链表结构一样了，造成很深的问题），遍历查找会非常慢。而红黑树在插入新数据后可能需要通过左旋，右旋、变色这些操作来保持平衡，引入红黑树就是为了查找数据快，解决链表查询深度的问题，我们知道红黑树属于平衡二叉树，但是为了保持 “平衡” 是需要付出代价的，但是该代价所损耗的资源要比遍历线性链表要少，所以当长度大于 8 的时候，会使用红黑树，如果链表长度很短的话，根本不需要引入红黑树，引入反而会慢。 10. 说说你对红黑树的见解？ 1、每个节点非红即黑 2、根节点总是黑色的 3、如果节点是红色的，则它的子节点必须是黑色的（反之不一定） 4、每个叶子节点都是黑色的空节点（NIL 节点） 5、从根节点到叶节点或空子节点的每条路径，必须包含相同数目的黑色节点（即相同的黑色高度） 11.jdk8 中对 HashMap 做了哪些改变？ 在 java 1.8 中，如果链表的长度超过了 8，那么链表将转换为红黑树。（桶的数量必须大于 64，小于 64 的时候只会扩容） 发生 hash 碰撞时，java 1.7 会在链表的头部插入，而 java 1.8 会在链表的尾部插入 在 java 1.8 中，Entry 被 Node 替代 (换了一个马甲)。 12.HashMap，LinkedHashMap，TreeMap 有什么区别？ HashMap 参考其他问题： LinkedHashMap 保存了记录的插入顺序，在用 Iterator 遍历时，先取到的记录肯定是先插入的；遍历比 HashMap 慢； TreeMap 实现 SortMap 接口，能够把它保存的记录根据键排序（默认按键值升序排序，也可以指定排序的比较器） 13.HashMap &amp; TreeMap &amp; LinkedHashMap 使用场景？ 一般情况下，使用最多的是 HashMap。 HashMap：在 Map 中插入、删除和定位元素时； TreeMap：在需要按自然顺序或自定义顺序遍历键的情况下； LinkedHashMap：在需要输出的顺序和输入的顺序相同的情况下。 14.HashMap 和 HashTable 有什么区别？ ①、HashMap 是线程不安全的，HashTable 是线程安全的； ②、由于线程安全，所以 HashTable 的效率比不上 HashMap； ③、HashMap 最多只允许一条记录的键为 null，允许多条记录的值为 null，而 HashTable 不允许； ④、HashMap 默认初始化数组的大小为 16，HashTable 为 11，前者扩容时，扩大两倍，后者扩大两倍 + 1； ⑤、HashMap 需要重新计算 hash 值，而 HashTable 直接使用对象的 hashCode 15.Java 中的另一个线程安全的与 HashMap 极其类似的类是什么？同样是线程安全，它与 HashTable 在线程同步上有什么不同？ ConcurrentHashMap 类（是 Java 并发包 java.util.concurrent 中提供的一个线程安全且高效的 HashMap 实现）。 HashTable 是使用 synchronize 关键字加锁的原理（就是对对象加锁）； 而针对 ConcurrentHashMap，在 JDK 1.7 中采用 分段锁的方式；JDK 1.8 中直接采用了 CAS（无锁算法）+ synchronized。 16.HashMap &amp; ConcurrentHashMap 的区别？ 除了加锁，原理上无太大区别。另外，HashMap 的键值对允许有 null，但是 ConCurrentHashMap 都不允许。 17. 为什么 ConcurrentHashMap 比 HashTable 效率要高？ HashTable 使用一把锁（锁住整个链表结构）处理并发问题，多个线程竞争一把锁，容易阻塞； ConcurrentHashMap JDK 1.7 中使用分段锁（ReentrantLock + Segment + HashEntry），相当于把一个 HashMap 分成多个段，每段分配一把锁，这样支持多线程访问。锁粒度：基于 Segment，包含多个 HashEntry。 JDK 1.8 中使用 CAS + synchronized + Node + 红黑树。锁粒度：Node（首结点）（实现 Map.Entry&lt;K,V&gt;）。锁粒度降低了。 18. 针对 ConcurrentHashMap 锁机制具体分析（JDK 1.7 VS JDK 1.8）？ JDK 1.7 中，采用分段锁的机制，实现并发的更新操作，底层采用数组 + 链表的存储结构，包括两个核心静态内部类 Segment 和 HashEntry。 ①、Segment 继承 ReentrantLock（重入锁） 用来充当锁的角色，每个 Segment 对象守护每个散列映射表的若干个桶； ②、HashEntry 用来封装映射表的键 - 值对； ③、每个桶是由若干个 HashEntry 对象链接起来的链表 JDK 1.8 中，采用 Node + CAS + Synchronized 来保证并发安全。取消类 Segment，直接用 table 数组存储键值对；当 HashEntry 对象组成的链表长度超过 TREEIFY_THRESHOLD 时，链表转换为红黑树，提升性能。底层变更为数组 + 链表 + 红黑树。 19.ConcurrentHashMap 在 JDK 1.8 中，为什么要使用内置锁 synchronized 来代替重入锁 ReentrantLock？​ ①、粒度降低了；​ ②、JVM 开发团队没有放弃 synchronized，而且基于 JVM 的 synchronized 优化空间更大，更加自然。​ ③、在大量的数据操作下，对于 JVM 的内存压力，基于 API 的 ReentrantLock 会开销更多的内存。 20.ConcurrentHashMap 简单介绍？①、重要的常量： private transient volatile int sizeCtl; 当为负数时，-1 表示正在初始化，-N 表示 N - 1 个线程正在进行扩容； 当为 0 时，表示 table 还没有初始化； 当为其他正数时，表示初始化或者下一次进行扩容的大小。 ②、数据结构： Node 是存储结构的基本单元，继承 HashMap 中的 Entry，用于存储数据； TreeNode 继承 Node，但是数据结构换成了二叉树结构，是红黑树的存储结构，用于红黑树中存储数据； TreeBin 是封装 TreeNode 的容器，提供转换红黑树的一些条件和锁的控制。 ③、存储对象时（put() 方法）： 1. 如果没有初始化，就调用 initTable() 方法来进行初始化； 2. 如果没有 hash 冲突就直接 CAS 无锁插入； 3. 如果需要扩容，就先进行扩容； 4. 如果存在 hash 冲突，就加锁来保证线程安全，两种情况：一种是链表形式就直接遍历到尾端插入，一种是红黑树就按照红黑树结构插入； 5. 如果该链表的数量大于阀值 8，就要先转换成红黑树的结构，break 再一次进入循环 6. 如果添加成功就调用 addCount() 方法统计 size，并且检查是否需要扩容。 ④、扩容方法 transfer()：默认容量为 16，扩容时，容量变为原来的两倍。 helpTransfer()：调用多个工作线程一起帮助进行扩容，这样的效率就会更高。 ⑤、获取对象时（get() 方法）： 1. 计算 hash 值，定位到该 table 索引位置，如果是首结点符合就返回； 2. 如果遇到扩容时，会调用标记正在扩容结点 ForwardingNode.find() 方法，查找该结点，匹配就返回； 3. 以上都不符合的话，就往下遍历结点，匹配就返回，否则最后就返回 null。 21.ConcurrentHashMap 的并发度是什么？ 程序运行时能够同时更新 ConccurentHashMap 且不产生锁竞争的最大线程数。默认为 16，且可以在构造函数中设置。当用户设置并发度时，ConcurrentHashMap 会使用大于等于该值的最小 2 幂指数作为实际并发度（假如用户设置并发度为 17，实际并发度则为 32） 有时间会对 HashTable，ConcurrentHashmap 解析。 22.为什么要重写hashcode和equals方法？​ 用HashMap存入自定义的类时，如果不重写这个自定义类的hashcode和equals方法，得到的结果会和预期的不一样。 ​ 重写hashcode和equals方法，来覆盖Object里的同名方法。Object的固有方法是根据两个对象的内存地址来判断，两个不同的对象，内存地址一定不会相同，所以无论值是否相等，结果都一定不会相等 参考博客：https://www.cnblogs.com/heqiyoujing/p/11143298.html https://www.jianshu.com/p/75adf47958a7","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"HashMap","slug":"hashmap","permalink":"https://topone233.github.io/tags/hashmap/"}]},{"title":"浅谈HashMap","slug":"浅谈HashMap","date":"2020-07-10T09:07:59.392Z","updated":"2020-09-20T05:49:32.502Z","comments":true,"path":"2020/07/10/浅谈HashMap/","link":"","permalink":"https://topone233.github.io/2020/07/10/%E6%B5%85%E8%B0%88HashMap/","excerpt":"","text":"HashMap 什么是哈希表 HashMap的实现原理 为何HashMap的数组长度一定是2的次幂 重写equals方法需同时重写hashCode方法 JDK1.8中HashMap的性能优化 参考自：https://blog.csdn.net/woshimaxiao1/article/details/83661464 1.什么是哈希表讨论哈希表之前，先大概了解下其他数据结构 数组采用一段连续的存储单元来存储数据。 通过指定下标查找，时间复杂度为O(1)； 通过给定值查找，需要遍历数组，逐一对比给定关键字和数组元素，时间复杂度O(n)。 对于有序数组，则可采用二分查找、插值查找、斐波那契查找等方式，可将复杂度提高到O(logn)；一般的插入删除操作，涉及到数组元素的移动，评价复杂度也为O(n)。 线性链表对于链表的新增、删除等操作，在找到指定操作位置后，仅需处理结点间的引用即可，时间复杂度为O(1),而查找操作需要遍历链表逐一进行对比，复杂度为O(n)。 二叉树对一棵相对平衡的有序二叉树，CRUD，平均复杂度均为O(logn)。 哈希表哈希表(hash table)进行CRUD，性能十分之高，不考虑哈希冲突的情况下，仅需一次定位即可完成，时间复杂度为O(1)。 数据结构的物理存储结构只有两种：顺序存储结构和链式存储结构。栈、队列、树、图等都是从逻辑结构去抽象，映射到内存中，也是这两种物理组织形式。 哈希表主干是数组。如果要新增或查找某个元素，把元素的关键字，通过某个函数映射到数组中的某个位置，通过数组下标一次定位就可以完成。这个函数可以简单描述为：存储位置=f(关键字)。函数f一般成为哈希函数，直接影响到哈希表的优劣。 哈希冲突如果两个不同的元素，通过哈希函数得出的实际存储地址相同；或者元素哈希运算得到一个存储地址，进行插入的时候发现已经被其他元素占用了，这就是所谓的哈希冲突，也叫哈希碰撞。 好的哈希函数会尽可能使计算简单和散列地址分布均匀。但是，数组是一块连续的固定长度的内存空间，再好的哈希函数也不能保证得到的存储地址绝对不发生冲突。 哈希冲突的解决方案有多种：开放定址法(发生冲突，继续寻找下一块未被占用的存储地址)、再散列函数法、链地址法。HashMap采用的即是链地址法，也就是数组+链表的方式。 2.HashMap的实现原理HashMap的主干是一个Entry数组。Entry是HashMap的基本组成单元，每一个Entry包含一个key-value键值对。（其实所谓的Map就是保存了两个对象之间的映射关系的一种集合）。 //主干是一个Entry数组，初始值为空数组，长度一定是2的次幂 transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPty_TABLE;Entry是HashMap中的一个静态内部类。代码如下: static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; V value; //存储执向下一个Entry的引用，单链表结构 Entry&lt;K,V&gt; next; //对key的hashcode值进行hash运算后得到的值，存储在Entry，避免重复计算 int hash; //creats new entry Entry(int h, k K, V v, Entry&lt;K,V&gt; n) { value = v; next = n; key = k; hash = h; }总结一下：HashMap由数组+链表组成，Entry数组是HashMap的主体，链表用于解决哈希冲突。如果定位到的数组位置不含链表(当前entry的next指向null)，那么CRUD很快，O(1)，仅需一次寻址即可；如果定位到的数组包含链表，添加操作，复杂度O(n)，先遍历链表，存在即覆盖，否则新增；查找操作，仍需遍历链表，然后通过key对象的equals方法逐一对比查找。所以，HashMap中的链表出现越少，性能越好。 构造器HashMap的4个构造器size、threshold、loadFactor、modCount //实际存储key-value键值对的个数 transient int size; //阈值，当table == {}时，该值为初始容量(默认16) //当table被填充了，也就是为table分配内存空间后，threshold一般为capacity*loadFactory //HashMap在进行扩容时需要参考threshold int threshold; //负载因子，代表了table的填充度，默认是0.75 //负载因子存在的原因，还是为了减缓哈希冲突 //如果初始桶为16，等到满16个才扩容，某些桶可能就有不止一个元素了 //所以加载因子默认为0.75，也就是说大小为16的HashMap，到了第13个元素，就会扩容成32 final float loadFactor; //HashMap被改变的次数 //由于HashMap非线程安全，在对HashMap进行迭代时，如果其他线程的参与导致HashMap的结构发生了变化(put、remove等操作)，需要抛出异常ConcurrentModificationException transient int modeCount;示例代码: public HashMap(int initialCapacity, float loadFactor) { //此处对传入的初始容量进行校验，最大不能超过MAXIMUM_CAPACITY = 1&lt;&lt;30(230) if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; //init方法在HashMap中没有实际实现，不过在其子类如 linkedHashMap中就会有对应实现 init(); } 从上面这段代码我们可以看出，在常规构造器中，没有为数组table分配内存空间（有一个入参为指定Map的构造器例外），而是在执行put操作的时候才真正构建table数组 putput操作的实现: public V put(K key, V value) { //如果table数组为空数组{}，进行数组填充（为table分配实际内存空间），入参为threshold， //此时threshold为initialCapacity 默认是1&lt;&lt;4(24=16) if (table == EMPTY_TABLE) { inflateTable(threshold); } //如果key为null，存储位置为table[0]或table[0]的冲突链上 if (key == null) return putForNullKey(value); int hash = hash(key);//对key的hashcode进一步计算，确保散列均匀 int i = indexFor(hash, table.length);//获取在table中的实际位置 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { //如果该对应数据已存在，执行覆盖操作。用新value替换旧value，并返回旧value Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++;//保证并发访问时，若HashMap内部结构发生变化，快速响应失败 addEntry(hash, key, value, i);//新增一个entry return null; } inflateTable这个方法用于为主干数组table在内存中分配存储空间，通过roundUpToPowerOf2(toSize)可以确保capacity为大于或等于toSize的最接近toSize的二次幂，比如toSize=13,则capacity=16;to_size=16,capacity=16;to_size=17,capacity=32。 private void inflateTable(int toSize) { //capacity一定是2的次幂 int capacity = roundUpToPowerOf2(toSize); //此处为threshold赋值，取capacity*loadFactor和MAXIMUM_CAPACITY+1的最小值， //capaticy一定不会超过MAXIMUM_CAPACITY，除非loadFactor大于1 threshold = (int) Math.min(capacity * loadFactor, MAXIMUM_CAPACITY + 1); table = new Entry[capacity]; initHashSeedAsNeeded(capacity); } roundUpToPowerOf2中的这段处理使得数组长度一定为2的次幂，Integer.highestOneBit是用来获取最左边的bit（其他bit位为0）所代表的数值 private static int roundUpToPowerOf2(int number) { // assert number &gt;= 0 : &quot;number must be non-negative&quot;; return number &gt;= MAXIMUM_CAPACITY ? MAXIMUM_CAPACITY : (number &gt; 1) ? Integer.highestOneBit((number - 1) &lt;&lt; 1) : 1; } hash函数: //这是一个神奇的函数，用了很多的异或，移位等运算 //对key的hashcode进一步进行计算以及二进制位的调整等来保证最终获取的存储位置尽量分布均匀 final int hash(Object k) { int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); } 以上hash函数计算出的值，通过indexFor进一步处理来获取实际的存储位置 //返回数组下标 static int indexFor(int h, int length) { return h &amp; (length-1); }h&amp;（length-1）保证获取的index一定在数组范围内，举个例子，默认容量16，length-1=15，h=18,转换成二进制计算为index=2。位运算对计算机来说，性能更高一些（HashMap中有大量位运算） 所以最终存储位置的确定流程是这样的： hashCode() hash() indexFor() key ----------&gt; hashcode ----------&gt; h ------------&gt; 存储下标 h&amp;(length-1)再来看看addEntry的实现： void addEntry(int hash, K key, V value, int bucketIndex) { if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) { //当size超过临界阈值threshold，并且即将发生哈希冲突时进行扩容 resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); } createEntry(hash, key, value, bucketIndex); } 通过以上代码能够得知，当发生哈希冲突并且size大于阈值的时候，需要进行数组扩容，扩容时，需要新建一个长度为之前数组2倍的新的数组，然后将当前的Entry数组中的元素全部传输过去，扩容后的新数组长度为之前的2倍，所以扩容相对来说是个耗资源的操作 3.为何HashMap的数组长度一定是2的次幂我们来继续看上面提到的resize方法: void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } Entry[] newTable = new Entry[newCapacity]; transfer(newTable, initHashSeedAsNeeded(newCapacity)); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1); } 如果数组进行扩容，数组长度发生变化，而存储位置 index = h&amp;(length-1),index也可能会发生变化，需要重新计算index，我们先来看看transfer这个方法: void transfer(Entry[] newTable, boolean rehash) { int newCapacity = newTable.length; //for循环中的代码，逐个遍历链表，重新计算索引位置，将老数组数据复制到新数组中去（数组不存储实际数据，所以仅仅是拷贝引用而已） for (Entry&lt;K,V&gt; e : table) { while(null != e) { Entry&lt;K,V&gt; next = e.next; if (rehash) { e.hash = null == e.key ? 0 : hash(e.key); } int i = indexFor(e.hash, newCapacity); //将当前entry的next链指向新的索引位置,newTable[i]有可能为空，有可能也是个entry链，如果是entry链，直接在链表头部插入。 e.next = newTable[i]; newTable[i] = e; e = next; } } } 这个方法将老数组中的数据逐个链表地遍历，扔到新的扩容后的数组中，我们的数组索引位置的计算是通过 对key值的hashcode进行hash扰乱运算后，再通过和 length-1进行位运算得到最终数组索引位置。 HashMap的数组长度一定保持2的次幂，比如16的二进制表示为 10000，那么length-1就是15，二进制为01111，同理扩容后的数组长度为32，二进制表示为100000，length-1为31，二进制表示为011111。从下图可以我们也能看到这样会保证低位全为1，而扩容后只有一位差异，也就是多出了最左位的1，这样在通过 h&amp;(length-1)的时候，只要h对应的最左边的那一个差异位为0，就能保证得到的新的数组索引和老数组索引一致(大大减少了之前已经散列良好的老数组的数据位置重新调换)，个人理解。 还有，数组长度保持2的次幂，length-1的低位都为1，会使得获得的数组索引index更加均匀 我们看到，上面的&amp;运算，高位是不会对结果产生影响的（hash函数采用各种位运算可能也是为了使得低位更加散列），我们只关注低位bit，如果低位全部为1，那么对于h低位部分来说，任何一位的变化都会对结果产生影响，也就是说，要得到index=21这个存储位置，h的低位只有这一种组合。这也是数组长度设计为必须为2的次幂的原因。 如果不是2的次幂，也就是低位不是全为1此时，要使得index=21，h的低位部分不再具有唯一性了，哈希冲突的几率会变的更大，同时，index对应的这个bit位无论如何不会等于1了，而对应的那些数组位置也就被白白浪费了。 getget方法: public V get(Object key) { //如果key为null,则直接去table[0]处去检索即可。 if (key == null) return getForNullKey(); Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue(); } get方法通过key值返回对应value，如果key为null，直接去table[0]处检索。 我们再看一下getEntry方法: final Entry&lt;K,V&gt; getEntry(Object key) { if (size == 0) { return null; } //通过key的hashcode值计算hash值 int hash = (key == null) ? 0 : hash(key); //indexFor (hash&amp;length-1) 获取最终数组索引，然后遍历链表，通过equals方法比对找出对应记录 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } return null; } 可以看出，get方法的实现相对简单，key(hashcode)–&gt;hash–&gt;indexFor–&gt;最终索引位置，找到对应位置table[i]，再查看是否有链表，遍历链表，通过key的equals方法比对查找对应的记录。要注意的是，有人觉得上面在定位到数组位置之后然后遍历链表的时候，e.hash == hash这个判断没必要，仅通过equals判断就可以。其实不然，试想一下，如果传入的key对象重写了equals方法却没有重写hashCode，而恰巧此对象定位到这个数组位置，如果仅仅用equals判断可能是相等的，但其hashCode和当前对象不一致，这种情况，根据Object的hashCode的约定，不能返回当前对象，而应该返回null，后面的例子会做出进一步解释。 4.重写equals方法需同时重写hashCode方法先来看下如果重写equals而不重写hashcode会发生什么: public class MyTest { private static class Person{ int idCard; String name; public Person(int idCard, String name) { this.idCard = idCard; this.name = name; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()){ return false; } Person person = (Person) o; //两个对象是否等值，通过idCard来确定 return this.idCard == person.idCard; } } public static void main(String []args){ HashMap&lt;Person,String&gt; map = new HashMap&lt;Person, String&gt;(); Person person = new Person(1234,&quot;乔峰&quot;); //put到hashmap中去 map.put(person,&quot;天龙八部&quot;); //get取出，从逻辑上讲应该能输出“天龙八部” System.out.println(&quot;结果:&quot;+map.get(new Person(1234,&quot;萧峰&quot;))); } } 实际输出结果：null如果我们已经对HashMap的原理有了一定了解，这个结果就不难理解了。尽管我们在进行get和put操作的时候，使用的key从逻辑上讲是等值的（通过equals比较是相等的），但由于没有重写hashCode方法，所以put操作时，key(hashcode1)–&gt;hash–&gt;indexFor–&gt;最终索引位置 ，而通过key取出value的时候 key(hashcode1)–&gt;hash–&gt;indexFor–&gt;最终索引位置，由于hashcode1不等于hashcode2，导致没有定位到一个数组位置而返回逻辑上错误的值null(也有可能碰巧定位到一个数组位置，但是也会判断其entry的hash值是否相等，上面get方法中有提到) 所以，在重写equals的方法的时候，必须注意重写hashCode方法，同时还要保证通过equals判断相等的两个对象，调用hashCode方法要返回同样的整数值。而如果equals判断不相等的两个对象，其hashCode可以相同(只不过会发生哈希冲突，应尽量避免) 5.JDK1.8中HashMap的性能优化假如一个数组槽位上链上数据过多（即拉链过长的情况）导致性能下降该怎么办？ JDK1.8在JDK1.7的基础上针对增加了红黑树来进行优化。即当链表超过8时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"HashMap","slug":"hashmap","permalink":"https://topone233.github.io/tags/hashmap/"}]},{"title":"九大常见数据结构","slug":"九大常见数据结构","date":"2020-07-10T09:07:59.386Z","updated":"2020-09-20T05:49:23.270Z","comments":true,"path":"2020/07/10/九大常见数据结构/","link":"","permalink":"https://topone233.github.io/2020/07/10/%E4%B9%9D%E5%A4%A7%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"参考：https://mp.weixin.qq.com/s/lnMvB3zgWZTmCfCvnNwTbA 数据结构想必大家都不会陌生，对于一个成熟的程序员而言，熟悉和掌握数据结构和算法也是基本功之一。数据结构本身其实不过是数据按照特点关系进行存储或者组织的集合，特殊的结构在不同的应用场景中往往会带来不一样的处理效率。 常用的数据结构可根据数据访问的特点分为线性结构和非线性结构。线性结构包括常见的链表、栈、队列等，非线性结构包括树、图等。数据结构种类繁多，本文将通过图解的方式对常用的数据结构进行理论上的介绍和讲解，以方便大家掌握常用数据结构的基本知识。 1 数组数组可以说是最基本最常见的数据结构。数组一般用来存储相同类型的数据，可通过数组名和下标进行数据的访问和更新。数组中元素的存储是按照先后顺序进行的，同时在内存中也是按照这个顺序进行连续存放。数组相邻元素之间的内存地址的间隔一般就是数组数据类型的大小。 2 链表链表相较于数组，除了数据域，还增加了指针域用于构建链式的存储数据。链表中每一个节点都包含此节点的数据和指向下一节点地址的指针。由于是通过指针进行下一个数据元素的查找和访问，使得链表的自由度更高。 这表现在对节点进行增加和删除时，只需要对上一节点的指针地址进行修改，而无需变动其它的节点。不过事物皆有两极，指针带来高自由度的同时，自然会牺牲数据查找的效率和多余空间的使用。 一般常见的是有头有尾的单链表，对指针域进行反向链接，还可以形成双向链表或者循环链表。 链表和数组对比链表和数组在实际的使用过程中需要根据自身的优劣势进行选择。链表和数组的异同点也是面试中高频的考察点之一。这里对单链表和数组的区别进行了对比和总结。 3 跳表从上面的对比中可以看出，链表虽然通过增加指针域提升了自由度，但是却导致数据的查询效率恶化。特别是当链表长度很长的时候，对数据的查询还得从头依次查询，这样的效率会更低。跳表的产生就是为了解决链表过长的问题，通过增加链表的多级索引来加快原始链表的查询效率。这样的方式可以让查询的时间复杂度从 O(n) 提升至 O(logn)。 跳表通过增加的多级索引能够实现高效的动态插入和删除，其效率和红黑树和平衡二叉树不相上下。目前 redis 和 levelDB 都有用到跳表。 从上图可以看出，索引级的指针域除了指向下一个索引位置的指针，还有一个 down 指针指向低一级的链表位置，这样才能实现跳跃查询的目的。 4 栈栈是一种比较简单的数据结构，常用一句话描述其特性，后进先出。栈本身是一种线性结构，但是在这个结构中只有一个口子允许数据的进出。这种模式可以参考腔肠动物… 即进食和排泄都用一个口… 栈的常用操作包括入栈 push 和出栈 pop，对应于数据的压入和压出。还有访问栈顶数据、判断栈是否为空和判断栈的大小等。由于栈后进先出的特性，常可以作为数据操作的临时容器，对数据的顺序进行调控，与其它数据结构相结合可获得许多灵活的处理。 5 队列队列是栈的兄弟结构，与栈的后进先出相对应，队列是一种先进先出的数据结构。顾名思义，队列的数据存储是如同排队一般，先存入的数据先被压出。常与栈一同配合，可发挥最大的实力。 6 树树作为一种树状的数据结构，其数据节点之间的关系也如大树一样，将有限个节点根据不同层次关系进行排列，从而形成数据与数据之间的父子关系。常见的数的表示形式更接近 “倒挂的树”，因为它将根朝上，叶朝下。 树是图的一种，树与图的区别在于：树是没有环的，而图是可以有环的。 树的数据存储在结点中，每个结点有零个或者多个子结点。没有父结点的结点在最顶端，成为根节点；没有非根结点有且只有一个父节点；每个非根节点又可以分为多个不相交的子树。 这意味着树是具备层次关系的，父子关系清晰，家庭血缘关系明朗；这也是树与图之间最主要的区别。 别看树好像很高级，其实可看作是链表的高配版。树的实现就是对链表的指针域进行了扩充，增加了多个地址指向子结点。同时将 “链表” 竖起来，从而凸显了结点之间的层次关系，更便于分析和理解。 树可以衍生出许多的结构，若将指针域设置为双指针，那么即可形成最常见的二叉树，即每个结点最多有两个子树的树结构。二叉树根据结点的排列和数量还可进一度划分为完全二叉树、满二叉树、平衡二叉树、红黑树等。 完全二叉树：除了最后一层结点，其它层的结点数都达到了最大值；同时最后一层的结点都是按照从左到右依次排布。 满二叉树：除了最后一层，其它层的结点都有两个子结点。 平衡二叉树平衡二叉树又被称为 AVL 树，它是一棵二叉排序树，且具有以下性质：它是一棵空树或它的左右两个子树的高度差的绝对值不超过 1，并且左右两个子树都是一棵平衡二叉树。 二叉排序树：是一棵空树，或者：若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值；若它的右子树不空，则右子树上所有结点的值均大于它的根结点的值；它的左、右子树也分别为二叉排序树。 树的高度：结点层次的最大值 平衡因子：左子树高度 - 右子树高度 二叉排序树意味着二叉树中的数据是排好序的，顺序为左结点 &lt;根节点&lt;右结点，这表明二叉排序树的中序遍历结果是有序的。 （二叉树四种遍历方式[前序遍历、中序遍历、后序遍历、层序遍历] ） 平衡二叉树的产生是为了解决二叉排序树在插入时发生线性排列的现象。由于二叉排序树本身为有序，当插入一个有序程度十分高的序列时，生成的二叉排序树会持续在某个方向的字数上插入数据，导致最终的二叉排序树会退化为链表，从而使得二叉树的查询和插入效率恶化。 平衡二叉树的出现能够解决上述问题，但是在构造平衡二叉树时，却需要采用不同的调整方式，使得二叉树在插入数据后保持平衡。主要的四种调整方式有 LL（左旋）、RR（右旋）、LR（先左旋再右旋）、RL（先右旋再左旋）。这里先给大家介绍下简单的单旋转操作，左旋和右旋。LR 和 RL 本质上只是 LL 和 RR 的组合。 在插入一个结点后应该沿搜索路径将路径上的结点平衡因子进行修改，当平衡因子大于 1 时，就需要进行平衡化处理。从发生不平衡的结点起，沿刚才回溯的路径取直接下两层的结点，如果这三个结点在一条直线上，则采用单旋转进行平衡化，如果这三个结点位于一条折线上，则采用双旋转进行平衡化。 左旋：S 为当前需要左旋的结点，E 为当前结点的父节点。 左旋的操作可以用一句话简单表示：将当前结点 S 的左孩子旋转为当前结点父结点 E 的右孩子，同时将父结点 E 旋转为当前结点 S 的左孩子。 右旋：S 为当前需要左旋的结点，E 为当前结点的父节点。右单旋是左单旋的镜像旋转。 左旋的操作同样可以用一句话简单表示：将当前结点 S 的左孩子 E 的右孩子旋转为当前结点 S 的左孩子，同时将当前结点 S 旋转为左孩子 E 的右孩子 红黑树平衡二叉树（AVL）为了追求高度平衡，需要通过平衡处理使得左右子树的高度差必须小于等于 1。高度平衡带来的好处是能够提供更高的搜索效率，其最坏的查找时间复杂度都是 O(logN)。但是由于需要维持这份高度平衡，所付出的代价就是当对树种结点进行插入和删除时，需要经过多次旋转实现复衡。这导致 AVL 的插入和删除效率并不高。 为了解决这样的问题，能不能找一种结构能够兼顾搜索和插入删除的效率呢？这时候红黑树便申请出战了。 红黑树具有五个特性： 每个结点要么是红的要么是黑的。 根结点是黑的。 每个叶结点（叶结点即指树尾端 NIL 指针或 NULL 结点）都是黑的。 如果一个结点是红的，那么它的两个儿子都是黑的。 对于任意结点而言，其到叶结点树尾端 NIL 指针的每条路径都包含相同数目的黑结点。 红黑树通过将结点进行红黑着色，使得原本高度平衡的树结构被稍微打乱，平衡程度降低。红黑树不追求完全平衡，只要求达到部分平衡。这是一种折中的方案，大大提高了结点删除和插入的效率。C++ 中的 STL 就常用到红黑树作为底层的数据结构。 除了上面所提及的树结构，还有许多广泛应用在数据库、磁盘存储等场景下的树结构。比如 B 树、B + 树等。这里就先不介绍了诶，下次在讲述相关存储原理的时候将会着重介绍。（其实是因为懒） B树（B-tree）B树和平衡二叉树稍有不同的是B树属于多叉树又名平衡多路查找树（查找路径不只两个），数据库索引技术里大量使用者B树和B+树的数据结构。B树相对于平衡二叉树，每个节点包含的关键字增多了，特别是在B树应用到数据库中的时候，数据库充分利用了磁盘块的原理（磁盘数据存储是采用块的形式存储的，每个块的大小为4K，每次IO进行数据读取时，同一个磁盘块的数据可以一次性读取出来）把节点大小限制和充分使用在磁盘快大小范围；把树的节点关键字增多后树的层级比原来的二叉树少了，减少数据查找的次数和复杂度; 规则： 排序方式：所有节点关键字是按递增次序排列，并遵循左小右大原则； 子节点数：非叶节点的子节点数&gt;1，且&lt;=M ，且M&gt;=2，空树除外（注：M阶代表一个树节点最多有多少个查找路径，M=M路,当M=2则是2叉树,M=3则是3叉）； 关键字数：枝节点的关键字数量大于等于ceil(m/2)-1个且小于等于M-1个（注：ceil()是个朝正无穷方向取整的函数 如ceil(1.1)结果为2)； 所有叶子节点均在同一层、叶子节点除了包含了关键字和关键字记录的指针外也有指向其子节点的指针只不过其指针地址都为null对应下图最后一层节点的空格子; 查询： 如上图我要从上图中找到E字母，查找流程如下 获取根节点的关键字进行比较，当前根节点关键字为M，E&lt;M（26个字母顺序），所以往找到指向左边的子节点（二分法规则，左小右大，左边放小于当前节点值的子节点、右边放大于当前节点值的子节点）； 拿到关键字D和G，D&lt;E&lt;G 所以直接找到D和G中间的节点； 拿到E和F，因为E=E 所以直接返回关键字和指针信息（如果树结构里面没有包含所要查找的节点则返回null）； 插入： 节点拆分规则：当前是要组成一个5路查找树，那么此时m=5,关键字数必须&lt;=5-1（这里关键字数&gt;4就要进行节点拆分）； 排序规则：满足节点本身比左边节点大，比右边节点小的排序规则; 删除： 节点合并规则：当前是要组成一个5路查找树，那么此时m=5,关键字数必须大于等于ceil（5/2）（这里关键字数&lt;2就要进行节点合并）； 满足节点本身比左边节点大，比右边节点小的排序规则; 关键字数小于二时先从子节点取，子节点没有符合条件时就向向父节点取，取中间值往父节点放； B+树B+树是B树的一个升级版，相对于B树来说B+树更充分的利用了节点的空间，让查询速度更加稳定，其速度完全接近于二分法查找。查找的效率要比B树更高、更稳定。 规则： B+跟B树不同B+树的非叶子节点不保存关键字记录的指针，只进行数据索引，这样使得B+树每个非叶子节点所能保存的关键字大大增加； B+树叶子节点保存了父节点的所有关键字记录的指针，所有数据地址必须要到叶子节点才能获取到。所以每次数据查询的次数都一样； B+树叶子节点的关键字从小到大有序排列，左边结尾数据都会保存右边节点开始数据的指针。 非叶子节点的子节点数=关键字数（来源百度百科）（根据各种资料 这里有两种算法的实现方式，另一种为非叶节点的关键字数=子节点数-1（来源维基百科)，虽然他们数据排列结构不一样，但其原理还是一样的Mysql 的B+树是用第一种方式实现）; 特点： B+树的层级更少：相较于B树B+每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快； B+树查询速度更稳定：B+所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定; B+树天然具备排序功能：B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。 B+树全节点遍历更快：B+树遍历整棵树只需要遍历所有的叶子节点即可，，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描。 B树相对于B+树的优点是，如果经常访问的数据离根节点很近，而B树的非叶子节点本身存有关键字其数据的地址，所以这种数据检索的时候会要比B+树快。 7 堆了解完二叉树，再来理解堆就不是什么难事了。堆通常是一个可以被看做一棵树的数组对象。堆的具体实现一般不通过指针域，而是通过构建一个一维数组与二叉树的父子结点进行对应，因此堆总是一颗完全二叉树。 对于任意一个父节点的序号 n 来说（这里 n 从 0 算），它的子节点的序号一定是 2n+1，2n+2，因此可以直接用数组来表示一个堆。 不仅如此，堆还有一个性质：堆中某个节点的值总是不大于或不小于其父节点的值。将根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。 堆常用来实现优先队列，在面试中经常考的问题都是与排序有关，比如堆排序、topK 问题等。由于堆的根节点是序列中最大或者最小值，因而可以在建堆以及重建堆的过程中，筛选出数据序列中的极值，从而达到排序或者挑选 topK 值的目的。 8 散列表散列表也叫哈希表，是一种通过键值对直接访问数据的机构。在初中，我们就学过一种能够将一个 x 值通过一个函数获得对应的一个 y 值的操作，叫做映射。散列表的实现原理正是映射的原理，通过设定的一个关键字和一个映射函数，就可以直接获得访问数据的地址，实现 O(1) 的数据访问效率。在映射的过程中，事先设定的函数就是一个映射表，也可以称作散列函数或者哈希函数。 散列表的实现最关键的就是散列函数的定义和选择。一般常用的有以下几种散列函数： 直接寻址法：取关键字或关键字的某个线性函数值为散列地址。 数字分析法：通过对数据的分析，发现数据中冲突较少的部分，并构造散列地址。例如同学们的学号，通常同一届学生的学号，其中前面的部分差别不太大，所以用后面的部分来构造散列地址。 平方取中**法**：当无法确定关键字里哪几位的分布相对比较均匀时，可以先求出关键字的平方值，然后按需要取平方值的中间几位作为散列地址。这是因为：计算平方之后的中间几位和关键字中的每一位都相关，所以不同的关键字会以较高的概率产生不同的散列地址。 取随机数法：使用一个随机函数，取关键字的随机值作为散列地址，这种方式通常用于关键字长度不同的场合。 除留取余法：取关键字被某个不大于散列表的表长 n 的数 m 除后所得的余数 p 为散列地址。这种方式也可以在用过其他方法后再使用。该函数对 m 的选择很重要，一般取素数或者直接用 n。 确定好散列函数之后，通过某个key值的确会得到一个唯一的value地址。但是却会出现一些特殊情况。即通过不同的key值可能会访问到同一个地址，这个现象称之为冲突。 冲突在发生之后，当在对不同的key值进行操作时会使得造成相同地址的数据发生覆盖或者丢失，是非常危险的。所以在设计散列表往往还需要采用冲突解决的办法。 常用的冲突处理方式有很多，常用的包括以下几种： 开放地址法（也叫开放寻址法）：实际上就是当需要存储值时，对 Key 哈希之后，发现这个地址已经有值了，这时该怎么办？不能放在这个地址，不然之前的映射会被覆盖。这时对计算出来的地址进行一个探测再哈希，比如往后移动一个地址，如果没人占用，就用这个地址。如果超过最大长度，则可以对总长度取余。这里移动的地址是产生冲突时的增列序量。 再哈希法：在产生冲突之后，使用关键字的其他部分继续计算地址，如果还是有冲突，则继续使用其他部分再计算地址。这种方式的缺点是时间增加了。 链地址法：链地址法其实就是对 Key 通过哈希之后落在同一个地址上的值，做一个链表。其实在很多高级语言的实现当中，也是使用这种方式处理冲突的。 公共溢出区：这种方式是建立一个公共溢出区，当地址存在冲突时，把新的地址放在公共溢出区里。 目前比较常用的冲突解决方法是链地址法，一般可以通过数组和链表的结合达到冲突数据缓存的目的。 左侧数组的每个成员包括一个指针，指向一个链表的头。每发生一个冲突的数据，就将该数据作为链表的节点链接到链表尾部。这样一来，就可以保证冲突的数据能够区分并顺利访问。 考虑到链表过长造成的问题，还可以使用红黑树替换链表进行冲突数据的处理操作，来提高散列表的查询稳定性。 9 图图相较于上文的几个结构可能接触的不多，但是在实际的应用场景中却经常出现。比方说交通中的线路图，常见的思维导图都可以看作是图的具体表现形式。 图结构一般包括顶点和边，顶点通常用圆圈来表示，边就是这些圆圈之间的连线。边还可以根据顶点之间的关系设置不同的权重，默认权重相同皆为 1。此外根据边的方向性，还可将图分为有向图和无向图。 图结构用抽象的图线来表示十分简单，顶点和边之间的关系非常清晰明了。但是在具体的代码实现中，为了将各个顶点和边的关系存储下来，却不是一件易事。 邻接矩阵目前常用的图存储方式为邻接矩阵，通过所有顶点的二维矩阵来存储两个顶点之间是否相连，或者存储两顶点间的边权重。 无向图的邻接矩阵是一个对称矩阵，是因为边不具有方向性，若能从此顶点能够到达彼顶点，那么彼顶点自然也能够达到此顶点。此外，由于顶点本身与本身相连没有意义，所以在邻接矩阵中对角线上皆为 0。 有向图由于边具有方向性，因此彼此顶点之间并不能相互达到，所以其邻接矩阵的对称性不再。 用邻接矩阵可以直接从二维关系中获得任意两个顶点的关系，可直接判断是否相连。但是在对矩阵进行存储时，却需要完整的一个二维数组。若图中顶点数过多，会导致二维数组的大小剧增，从而占用大量的内存空间。 而根据实际情况可以分析得，图中的顶点并不是任意两个顶点间都会相连，不是都需要对其边上权重进行存储。那么存储的邻接矩阵实际上会存在大量的 0。虽然可以通过稀疏表示等方式对稀疏性高的矩阵进行关键信息的存储，但是却增加了图存储的复杂性。 因此，为了解决上述问题，一种可以只存储相连顶点关系的邻接表应运而生。 邻接表在邻接表中，图的每一个顶点都是一个链表的头节点，其后连接着该顶点能够直接达到的相邻顶点。相较于无向图，有向图的情况更为复杂，因此这里采用有向图进行实例分析。 在邻接表中，每一个顶点都对应着一条链表，链表中存储的是顶点能够达到的相邻顶点。存储的顺序可以按照顶点的编号顺序进行。比如上图中对于顶点 B 来说，其通过有向边可以到达顶点 A 和顶点 E，那么其对应的邻接表中的顺序即 B-&gt;A-&gt;E，其它顶点亦如此。 通过邻接表可以获得从某个顶点出发能够到达的顶点，从而省去了对不相连顶点的存储空间。然而，这还不够。对于有向图而言，图中有效信息除了从顶点 “指出去” 的信息，还包括从别的顶点 “指进来” 的信息。这里的 “指出去” 和“指进来”可以用出度和入度来表示。 入度：有向图的某个顶点作为终点的次数和。 出度：有向图的某个顶点作为起点的次数和。 由此看出，在对有向图进行表示时，邻接表只能求出图的出度，而无法求出入度。这个问题很好解决，那就是增加一个表用来存储能够到达某个顶点的相邻顶点。这个表称作逆邻接表。 逆邻接表逆邻接表与邻接表结构类似，只不过图的顶点链接着能够到达该顶点的相邻顶点。也就是说，邻接表时顺着图中的箭头寻找相邻顶点，而逆邻接表时逆着图中的箭头寻找相邻顶点。 邻接表和逆邻接表的共同使用下，就能够把一个完整的有向图结构进行表示。可以发现，邻接表和逆邻接表实际上有一部分数据时重合的，因此可以将两个表合二为一，从而得到了所谓的十字链表。 十字链表十字链表似乎很简单，只需要通过相同的顶点分别链向以该顶点为终点和起点的相邻顶点即可。 但这并不是最优的表示方式。虽然这样的方式共用了中间的顶点存储空间，但是邻接表和逆邻接表的链表节点中重复出现的顶点并没有得到重复利用，反而是进行了再次存储。因此，上图的表示方式还可以进行进一步优化。 十字链表优化后，可通过扩展的顶点结构和边结构来进行正逆邻接表的存储：（下面的弧头可看作是边的箭头那端，弧尾可看作是边的圆点那端） data：用于存储该顶点中的数据； firstin 指针：用于连接以当前顶点为弧头的其他顶点构成的链表，即从别的顶点指进来的顶点； firstout 指针：用于连接以当前顶点为弧尾的其他顶点构成的链表，即从该顶点指出去的顶点； 边结构通过存储两个顶点来确定一条边，同时通过分别代表这两个顶点的指针来与相邻顶点进行链接： tailvex：用于存储作为弧尾的顶点的编号； headvex：用于存储作为弧头的顶点的编号； headlink 指针：用于链接下一个存储作为弧头的顶点的节点； taillink 指针：用于链接下一个存储作为弧尾的顶点的节点； 以上图为例子，对于顶点 A 而言，其作为起点能够到达顶点 E。因此在邻接表中顶点 A 要通过边AE（即边 04）指向顶点 E，顶点 A 的firstout指针需要指向边 04 的tailvex。同时，从 B 出发能够到达 A，所以在逆邻接表中顶点 A 要通过边AB（即边 10）指向 B，顶点 A 的firstin指针需要指向边 10 的弧头，即headlink指针。依次类推。 十字链表采用了一种看起来比较繁乱的方式对边的方向性进行了表示，能够在尽可能降低存储空间的情况下增加指针保留顶点之间的方向性。具体的操作可能一时间不好弄懂，建议多看几次上图，弄清指针指向的意义，明白正向和逆向邻接表的表示。 10 总结数据结构博大精深，没有高等数学的讳莫如深，也没有量子力学的玄乎其神，但是其在计算机科学的各个领域都具有强大的力量。本文试图采用图解的方式对九种数据结构进行理论上的介绍，但是其实这都是不够的。 即便是简单的数组、栈、队列等结构，在实际使用以及底层实现上都会有许多优化设计以及使用技巧，这意味着还需要真正把它们灵活的用起来，才能够算是真正意义上的熟悉和精通。但是本文可以作为常见数据结构的一个总结，当你对某些结构有些淡忘的时候，不妨重新回来看看。","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"二叉树的四种遍历方式","slug":"二叉树的四种遍历方式","date":"2020-07-10T09:07:59.381Z","updated":"2020-09-20T05:48:52.519Z","comments":true,"path":"2020/07/10/二叉树的四种遍历方式/","link":"","permalink":"https://topone233.github.io/2020/07/10/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%9B%9B%E7%A7%8D%E9%81%8D%E5%8E%86%E6%96%B9%E5%BC%8F/","excerpt":"","text":"原文地址 www.cnblogs.com 二叉树的四种遍历方式： 二叉树的遍历（traversing binary tree）是指从根结点出发，按照某种次序依次访问二叉树中所有的结点，使得每个结点被访问依次且仅被访问一次。四种遍历方式分别为：先序遍历、中序遍历、后序遍历、层序遍历。 树的相关术语：节点的度：一个节点含有的子树的个数称为该节点的度； 叶节点：度为0的节点； 树的度：一棵树中，最大的节点的度； 森林：由m（m&gt;=0）棵互不相交的树的集合 树的符号表现法：（1（2（4（5，6）），3） 解读：祖先1的子节点2（子节点4（叶节点5，6）），3。同层子树间用逗号隔开。 如何创建二叉树遍历之前，我们首先介绍一下，如何创建一个二叉树，在这里我们用的是先建左树在建右树的方法， 首先要声明结点 TreeNode 类，代码如下： public class TreeNode { public int data; public TreeNode leftChild; public TreeNode rightChild; public TreeNode(int data){ this.data = data; } } 再来创建一颗二叉树： /** * 构建二叉树 * @param list 输入序列 * @return */ public static TreeNode createBinaryTree(LinkedList&lt;Integer&gt; list){ TreeNode node = null; if(list == null || list.isEmpty()){ return null; } Integer data = list.removeFirst(); if(data!=null){ node = new TreeNode(data); node.leftChild = createBinaryTree(list); node.rightChild = createBinaryTree(list); } return node; } 接下来我们按照上面列的顺序一一讲解， 先序遍历首先来看先序遍历，所谓的先序遍历就是先访问根节点，在访问左节点，最后访问右节点， 如上图所示，前序遍历结果为：ABDFECGHI 实现代码如下： /** * 二叉树前序遍历 根-&gt; 左-&gt; 右 * @param node 二叉树节点 */ public static void preOrderTraveral(TreeNode node){ if(node == null){ return; } System.out.print(node.data+&quot; &quot;); preOrderTraveral(node.leftChild); preOrderTraveral(node.rightChild); } 中序遍历再者就是中序遍历，所谓的中序遍历就是先访问左节点，再访问根节点，最后访问右节点， 如上图所示，中序遍历结果为：DBEFAGHCI（G没有左子树，所以直接访问G，而不是访问H） 实现代码如下： /** * 二叉树中序遍历 左-&gt; 根-&gt; 右 * @param node 二叉树节点 */ public static void inOrderTraveral(TreeNode node){ if(node == null){ return; } inOrderTraveral(node.leftChild); System.out.print(node.data+&quot; &quot;); inOrderTraveral(node.rightChild); } 后序遍历最后就是后序遍历，所谓的后序遍历就是先访问左节点，再访问右节点，最后访问根节点。 如上图所示，前序遍历结果为：DEFBHGICA 实现代码如下： /** * 二叉树后序遍历 左-&gt; 右-&gt; 根 * @param node 二叉树节点 */ public static void postOrderTraveral(TreeNode node){ if(node == null){ return; } postOrderTraveral(node.leftChild); postOrderTraveral(node.rightChild); System.out.print(node.data+&quot; &quot;); } 非递归的前中后序遍历讲完上面三种非递归的方法，下面再给大家讲讲非递归是如何实现前中后序遍历的 非递归前序遍历还是一样，先看非递归前序遍历 首先申请一个新的栈，记为 stack； 声明一个结点 treeNode，让其指向 node 结点； 如果 treeNode 的不为空，将 treeNode 的值打印，并将 treeNode 入栈，然后让 treeNode 指向 treeNode 的右结点， 重复步骤 3，直到 treenode 为空； 然后出栈，让 treeNode 指向 treeNode 的右孩子 重复步骤 3，直到 stack 为空. 实现代码如下： public static void preOrderTraveralWithStack(TreeNode node){ Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode treeNode = node; while(treeNode!=null || !stack.isEmpty()){ //迭代访问节点的左孩子，并入栈 while(treeNode != null){ System.out.print(treeNode.data+&quot; &quot;); stack.push(treeNode); treeNode = treeNode.leftChild; } //如果节点没有左孩子，则弹出栈顶节点，访问节点右孩子 if(!stack.isEmpty()){ treeNode = stack.pop(); treeNode = treeNode.rightChild; } } } 非递归中序遍历中序遍历非递归，在此不过多叙述具体步骤了， 具体过程： 申请一个新栈，记为 stack，申请一个变量 cur，初始时令 treeNode 为头节点； 先把 treeNode 节点压入栈中，对以 treeNode 节点为头的整棵子树来说，依次把整棵树的左子树压入栈中，即不断令 treeNode=treeNode.leftChild，然后重复步骤 2； 不断重复步骤 2，直到发现 cur 为空，此时从 stack 中弹出一个节点记为 treeNode，打印 node 的值，并让 treeNode= treeNode.right，然后继续重复步骤 2； 当 stack 为空并且 cur 为空时结束。 public static void inOrderTraveralWithStack(TreeNode node){ Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode treeNode = node; while(treeNode!=null || !stack.isEmpty()){ while(treeNode != null){ stack.push(treeNode); treeNode = treeNode.leftChild; } if(!stack.isEmpty()){ treeNode = stack.pop(); System.out.print(treeNode.data+&quot; &quot;); treeNode = treeNode.rightChild; } } } 非递归后序遍历后序遍历这里较前两者实现复杂一点，我们需要一个标记为来记忆我们此时节点上一个节点，具体看代码注释 public static void postOrderTraveralWithStack(TreeNode node){ Stack&lt;TreeNode&gt; stack = new Stack&lt;TreeNode&gt;(); TreeNode treeNode = node; TreeNode lastVisit = null; //标记每次遍历最后一次访问的节点 while(treeNode!=null || !stack.isEmpty()){//节点不为空，结点入栈，并且指向下一个左孩子 while(treeNode!=null){ stack.push(treeNode); treeNode = treeNode.leftChild; } //栈不为空 if(!stack.isEmpty()){ //出栈 treeNode = stack.pop(); /** * 这块就是判断treeNode是否有右孩子， * 如果没有输出treeNode.data，让lastVisit指向treeNode，并让treeNode为空 * 如果有右孩子，将当前节点继续入栈，treeNode指向它的右孩子,继续重复循环 */ if(treeNode.rightChild == null || treeNode.rightChild == lastVisit) { System.out.print(treeNode.data + &quot; &quot;); lastVisit = treeNode; treeNode = null; }else{ stack.push(treeNode); treeNode = treeNode.rightChild; } } } } 层序遍历最后再介绍一下层序遍历 具体步骤如下： 首先申请一个新的队列，记为 queue； 将头结点 head 压入 queue 中； 每次从 queue 中出队，记为 node，然后打印 node 值，如果 node 左孩子不为空，则将左孩子入队；如果 node 的右孩子不为空，则将右孩子入队； 重复步骤 3，直到 queue 为空。 实现代码如下： public static void levelOrder(TreeNode root){ LinkedList&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()){ root = queue.pop(); System.out.print(root.data+&quot; &quot;); if(root.leftChild!=null) queue.add(root.leftChild); if(root.rightChild!=null) queue.add(root.rightChild); } }","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"二叉树","slug":"二叉树","permalink":"https://topone233.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"时间复杂度","slug":"时间复杂度","date":"2020-07-06T16:00:00.000Z","updated":"2020-09-17T02:16:57.815Z","comments":true,"path":"2020/07/07/时间复杂度/","link":"","permalink":"https://topone233.github.io/2020/07/07/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/","excerpt":"","text":"时间复杂度 若存在函数 f(n)，使得当n趋近于无穷大时，T(n) / f(n) 的极限值为不等于零的常数，则称 f(n) 是 T(n) 的同数量级函数。记作 T(n) = O( f(n) )，称 O( f(n) ) 为算法的渐进时间复杂度(asymptotic time complexity)，简称时间复杂度。 OK，我们先引出官方的定义先混个眼熟。接下来看几个通俗易懂的案例： 场景1：问：给李华一个长n寸的面包，每3天吃掉1寸，几天吃完？ 答：3 X n = 3n 天。 如果用函数表达这个相对时间：T(n) = 3n 场景2：问：给李华一个长16寸的面包，每5天吃掉面包剩余长度的一半。第一次吃掉8寸，第二次吃4寸，…… 几天可以吃得只剩1寸？ 答：5 X log16 = 20天。 T(n) = 5log n 场景3：问：给李华一个长10寸的面包和一个鸡腿，每2天吃掉1个鸡腿，几天吃完鸡腿？ 答：2天 （与面包一点关系没有）。 T(n) = 2 场景4：问：给李华一个长10寸的面包，吃掉第一个一寸需要1天时间，吃掉第二个一寸需要2天时间，吃掉第三个一寸需要3天时间…..每多吃一寸，所花的时间也多一天，几天吃完？ 答：1到10的和 = 55天 1+2+3+……+ n-1 + n = (1+n)*n/2 = 0.5n^2 + 0.5n 。 T(n) = 0.5n^2 + 0.5n 相信通过以上4个案例已经了解了： T(n) 基本操作执行次数的函数。 那么如何推导出时间复杂度呢？有如下几个原则： 如果运行时间是常数量级，用常数1表示； 只保留时间函数中的最高阶项； 如果最高阶项存在，则省去最高阶项前面的系数。 渐进分析（asymptotic analysis）：忽略掉那些依赖于机器的常量，不去检查实际的运行时间，而是关注运行时间的增长。 在看刚才的四个场景： 场景1： T(n) = 3n 最高阶项为3n，省去系数3，时间复杂度为：O(n) 场景2： T(n) = 5log n 最高阶项为5log n，省去系数5，时间复杂度为：O(log n) 场景3： T(n) = 2 只有常数量级，时间复杂度为：O(1) 场景4： T(n) = 0.5n^2 + 0.5n 最高阶项为0.5n^2，省去系数0.5，时间复杂度为：O(n^2) O（1）&lt; O（log n）&lt; O（n）&lt; O（n^2） 空间复杂度空间复杂度：即程序中变量的个数 位运算位运算在算法中很有用，速度可以比四则运算快很多。 左移 &lt;&lt;10 &lt;&lt; 1 结果： 20 左移就是将二进制全部往左移动。10的二进制：1010，左移一位：10100，十进制就是20。 基本可以把左移看作：a * (2 ^ b) 右移 &gt;&gt;10 &gt;&gt; 1 结果： 5 右移就是将二进制全部往右移动，并去除多余的右边。右移一位：101，十进制就是5 基本可以把右移看作：a / (2 ^ b) 右移很好用，比如可以用在二分算法中取中间值","categories":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"Hello World！（hexo配置记录）","slug":"Hello World！（hexo配置记录）","date":"2020-07-01T02:25:00.000Z","updated":"2020-09-17T02:15:20.172Z","comments":true,"path":"2020/07/01/Hello World！（hexo配置记录）/","link":"","permalink":"https://topone233.github.io/2020/07/01/Hello%20World%EF%BC%81%EF%BC%88hexo%E9%85%8D%E7%BD%AE%E8%AE%B0%E5%BD%95%EF%BC%89/","excerpt":"","text":"Hello World！（hexo配置记录）以前在网上冲浪的时候只是不经意间发现许多让人眼前一亮的blog，由此萌生了create my blog的想法。 目前的blog搭建： CSDN/博客园平台 但我个人不是很喜欢这种，首先是太丑了（没错，就是你，CSDN）。 其次依托于平台，虽然只需要创作就行，但是感觉不是属于自己的，没有归属感 独立blog 租云服务器、买域名，还要管理维护，个人感觉略微有些费事，精力有限，还是简单点好 hexo 依托于Github，也是我目前选择的，配置相对简单快捷，主题丰富 hexo配置步骤： 安装Git 安装Node.js 安装hexo 生成ssh并添加到GitHub 部署项目 上传到GitHub 修改主题 1.安装Git下载地址 安装步骤：双击下载的exe文件，一路next就行 2.安装Node.jsHexo是基于nodeJS环境的静态博客，npm是必备的 下载地址 安装步骤：下载好msi文件后，双击打开安装，也是一路next，不过在Custom Setup这一步记得选 Add to PATH ,这样你就不用自己去配置电脑上环境变量了 3.安装hexo 创建一个源文件夹，然后cd到该文件夹下 安装hexo： npm i -g hexo hexo -v 查看版本，检查是否安装成功 hexo init 初始化，初始化完成后可在文件夹下看到文件 这里要说下，npm install出现一直停留在”fetchMetadata: sill resolveWithNewModule find-cache-dir@”解决方法，更换成淘宝的源（反正我是解决了） //修改为淘宝源 npm config set registry https://registry.npm.taobao.org //配置后可通过下面方式来验证是否成功 npm config get registry //或 npm info express4.生成ssh并添加到GitHubSSH密钥可以防止其他人恶意部署文件到你的仓库 首先要有GitHub账号，没有的自行注册 创建一个仓库repository，名称为youname.github.io 在gitbash中，配置GitHub账号信息 //配置你的GitHub账号信息 git config --global user.name &quot;YourName&quot; git config --global user.email &quot;YourEmail&quot;创建ssh //创建ssh ssh-keygen -t rsa -C &quot;youremail@xx.com&quot;生成ssh，在gitbash中切换到文件目录cat读取 //读取ssh文件内容 cat id_rsa.pub 全部复制（包括开头的ssh-rsa，和尾部的email）到GitHub 配置ssh，title随便起 5.部署项目修改hexo的_config.yml文件配置信息（直接复制，只需要修改url即可） deploy: type: git repo: https://github.com/YourgithubName/YourgithubName.github.io.git branch: master回到gitbash，进入hexo目录 hexo clean hexo generate hexo server这时，在http://localhost:4000就可以看到默认页面 6.上传到GitHubnpm install hexo-deployer-git --save将写好的文章部署到GitHub服务器，执行命令 hexo clean hexo generate hexo deploy 第一次deploy要输入GitHubusername和password完成后https://yourgithubname.github.io，查看即可 7.修改主题hexo官网有推荐很多很多主题，自行选择","categories":[],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://topone233.github.io/tags/hexo/"}]}],"categories":[{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/categories/sql/"},{"name":"Java","slug":"java","permalink":"https://topone233.github.io/categories/java/"},{"name":"工具","slug":"工具","permalink":"https://topone233.github.io/categories/%E5%B7%A5%E5%85%B7/"},{"name":"安全","slug":"安全","permalink":"https://topone233.github.io/categories/%E5%AE%89%E5%85%A8/"},{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/categories/%E8%B5%84%E6%BA%90/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/categories/nosql/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/categories/spring/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"转载","slug":"转载","permalink":"https://topone233.github.io/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SQL","slug":"sql","permalink":"https://topone233.github.io/tags/sql/"},{"name":"Java","slug":"java","permalink":"https://topone233.github.io/tags/java/"},{"name":"工具","slug":"工具","permalink":"https://topone233.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"IDE","slug":"ide","permalink":"https://topone233.github.io/tags/ide/"},{"name":"Spring","slug":"spring","permalink":"https://topone233.github.io/tags/spring/"},{"name":"安全","slug":"安全","permalink":"https://topone233.github.io/tags/%E5%AE%89%E5%85%A8/"},{"name":"CROS","slug":"cros","permalink":"https://topone233.github.io/tags/cros/"},{"name":"网络协议","slug":"网络协议","permalink":"https://topone233.github.io/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"SSL","slug":"ssl","permalink":"https://topone233.github.io/tags/ssl/"},{"name":"TCP","slug":"tcp","permalink":"https://topone233.github.io/tags/tcp/"},{"name":"滑动窗口","slug":"滑动窗口","permalink":"https://topone233.github.io/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"},{"name":"集合","slug":"集合","permalink":"https://topone233.github.io/tags/%E9%9B%86%E5%90%88/"},{"name":"JVM","slug":"jvm","permalink":"https://topone233.github.io/tags/jvm/"},{"name":"资源","slug":"资源","permalink":"https://topone233.github.io/tags/%E8%B5%84%E6%BA%90/"},{"name":"配置","slug":"配置","permalink":"https://topone233.github.io/tags/%E9%85%8D%E7%BD%AE/"},{"name":"NoSQL","slug":"nosql","permalink":"https://topone233.github.io/tags/nosql/"},{"name":"CORS","slug":"cors","permalink":"https://topone233.github.io/tags/cors/"},{"name":"RESTful","slug":"restful","permalink":"https://topone233.github.io/tags/restful/"},{"name":"IO","slug":"io","permalink":"https://topone233.github.io/tags/io/"},{"name":"数据结构","slug":"数据结构","permalink":"https://topone233.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"排序算法","slug":"排序算法","permalink":"https://topone233.github.io/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"name":"HashMap","slug":"hashmap","permalink":"https://topone233.github.io/tags/hashmap/"},{"name":"二叉树","slug":"二叉树","permalink":"https://topone233.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"hexo","slug":"hexo","permalink":"https://topone233.github.io/tags/hexo/"}]}